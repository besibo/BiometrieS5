[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TP de Biométrie Semestre 5",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "01-EDA.html",
    "href": "01-EDA.html",
    "title": "1  Exploration statistique des données",
    "section": "",
    "text": "La première étape de toute analyse de données est l’exploration. Avant de se lancer dans des tests statistiques et des procédures complexes, et à supposer que les données dont vous disposez sont déjà dans un format approprié, il est toujours très utile de :\n\nexplorer visuellement les données dont on dispose en faisant des graphiques nombreux et variés, afin de comprendre, notamment quelle est la distribution des variables numériques, quelles sont les catégories les plus représentées pour les variables qualitatives (ou facteurs), quelles sont les relations les plus marquantes entre variables numériques et/ou catégorielles, etc. Vous avez déjà appris, au semestre 3, comment produire toutes sortes de graphiques avec le package ggplot2. Si vous avez besoin de revoir les bases, c’est là que ça se passe\nexplorer les données en calculant des indices de statistiques descriptives. Ces indices relèvent en général de 2 catégories : les indices de position (e.g. moyennes, médianes…) et les indices de dispersion (e.g. variance, écart-type, intervalle inter-quartiles…). Nous allons voir dans ce chapitre comment calculer ces indices dans plusieurs situations, notamment lorsque l’on souhaite les calculer pour plusieurs sous-groupes d’un jeu de données\n\nNous verrons également dans ce chapitre comment calculer des indices d’incertitude. Attention, il ne faut pas confondre indices de dispersion et indices d’incertitude. Nous y reviendrons plus loin.\nAfin d’explorer ces questions, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(palmerpenguins)\nlibrary(nycflights13)\n\nLes packages du tidyverse (Wickham 2022) permettent de manipuler facilement des tableaux de données et de réaliser des graphiques. Charger le tidyverse permet d’accéder, entre autres, aux packages readr (Wickham, Hester, et Bryan 2022), pour importer facilement des fichiers .csv au format tibble, dplyr (Wickham, François, et al. 2022) pour manipuler des tableaux de données ou encore ggplot2 (Wickham, Chang, et al. 2022) pour produire des graphiques. Le package skimr (Waring et al. 2022) permet de calculer des résumés de données très informatifs. Les packages palmerpenguins (Horst, Hill, et Gorman 2022) et nycflights13 (Wickham 2021) fournissent des jeux de données qui seront faciles à manipuler pour illustrer ce chapitre (et les suivants).\nAttention, pensez à installer ces packages avant de les charger en mémoire. Si vous ne savez plus comment faire, consultez d’urgence la section dédiée au package du livre en ligne de Biométrie du semestre 3.\nDe même, pour travailler dans de bonnes conditions, créez un nouveau dossier sur votre ordinateur, créez un Rproject et un script dans ce dossier et travaillez systématiquement dans votre script. Là encore, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire."
  },
  {
    "objectID": "01-EDA.html#créer-des-résumés-avec-les-fonctions-group_by-et-summarise",
    "href": "01-EDA.html#créer-des-résumés-avec-les-fonctions-group_by-et-summarise",
    "title": "1  Exploration statistique des données",
    "section": "1.2 Créer des résumés avec les fonctions group_by() et summarise()",
    "text": "1.2 Créer des résumés avec les fonctions group_by() et summarise()\nComme nous l’avons vu au semestre 3, le package dplyr fournit plusieurs fonctions qui portent le nom de verbes simples et qui permettent d’effectuer des manipulations simples mais qui peuvent devenir très puissantes lorsqu’on les combine. Nous avons ainsi vu les fonctions suivantes :\n\nselect() : pour sélectionner ou exclure certaines colonnes (variables) d’un tableau de données\nfilter() : pour trier des lignes d’un tableau de données selon des critères ou conditions choisis par l’utilisateur\nmutate() : pour transformer des variables existantes, ou pour créer de nouvelles colonnes dans un tableau de données\narrange() : pour trier des tableaux de données par ordre croissants ou décroissants\n\nSi vous ne savez plus comment utiliser ces fonctions, relisez le chapitre 4 du livre en ligne de Biométrie du semestre 3.\nÀ ces 4 verbes, on ajoute en général les 3 suivants :\n\nsummarise() : pour créer des résumés de données à partir des colonnes d’un tableau\ngroup_by() : pour effectuer des opérations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle)\ncount() : pour compter le nombre d’observations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle)\n\nVoyons comment on utilise ces fonctions pour calculer des indices de statistiques descriptives pour les variables du tableau penguins :\n\n# affichage du tableau\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\n1.2.1 Principe de la fonction summarise()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Schéma de la fonction summarise() tiré de la ‘cheatsheet’ de dplyr et tidyr\n\n\n\n\nLa Figure 1.1 ci-dessus indique comment travaille la fonction summarise() : elle prend plusieurs valeurs (potentiellement, un très grand nombre) et les réduit à une unique valeur qui les résume. Lorsque l’on applique cette démarche à plusieurs colonnes d’un tableau, on obtient un tableau qui ne contient plus qu’une unique ligne de résumé.\nLa valeur qui résume les données est choisie par l’utilisateur. Il peut s’agir par exemple d’un calcul moyenne ou de variance, il peut s’agir de calculer une somme, ou d’extraire la valeur maximale ou minimale, ou encore, il peut tout simplement s’agir de déterminer un nombre d’observations.\nAinsi, pour connaître la moyenne et l’écart-type de la longueur du bec des manchots de l’île de Palmer, il suffit d’utiliser le tableau penguins du package palmerpenguins et sa variable bill_length_mm que nous avons déjà utilisés au semestre 3 :\n\npenguins %>%\n  summarise(moyenne = mean(bill_length_mm),\n            ecart_type = sd(bill_length_mm))\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    <dbl>      <dbl>\n1      NA         NA\n\n\nLes fonctions mean() et sd() permettent de calculer une moyenne et un écart-type respectivement. Ici, les valeurs retournées sont NA car 2 individus n’ont pas été mesurés, et le tableau contient donc des valeurs manquantes :\n\npenguins %>%\n  filter(is.na(bill_length_mm))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen             NA            NA          NA      NA <NA>   2007\n2 Gentoo  Biscoe                NA            NA          NA      NA <NA>   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nPour obtenir les valeurs souhaitées, il faut indiquer à R d’exclure les valeurs manquantes lors des calculs de moyenne et écart-types :\n\npenguins %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    <dbl>      <dbl>\n1    43.9       5.46\n\n\nLa longueur moyenne du bec des manchots est donc de 43.9 millimètres et l’écart-type vaut 5.5 millimètres.\nLa fonction summarise() permet donc de calculer des indices statistiques variés, et sur plusieurs variables à la fois. Par exemple. pour calculer les moyennes, médianes, minima et maxima des longueurs de nageoires et de masses corporelles, on peut procéder ainsi :\n\npenguins %>% \n  summarise(moy_flip = mean(flipper_length_mm, na.rm = TRUE),\n            med_flip = median(flipper_length_mm, na.rm = TRUE),\n            min_flip = min(flipper_length_mm, na.rm = TRUE),\n            max_flip = max(flipper_length_mm, na.rm = TRUE),\n            moy_mass = mean(body_mass_g, na.rm = TRUE),\n            med_mass = median(body_mass_g, na.rm = TRUE),\n            min_mass = min(body_mass_g, na.rm = TRUE),\n            max_mass = max(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n  moy_flip med_flip min_flip max_flip moy_mass med_mass min_mass max_mass\n     <dbl>    <dbl>    <int>    <int>    <dbl>    <dbl>    <int>    <int>\n1     201.      197      172      231    4202.     4050     2700     6300\n\n\nLa fonction summarise() est donc très utile pour produire des résumés informatifs des données, mais nos exemples ne sont ici pas très pertinents puisque nous avons jusqu’ici calculé des indices sans distinguer les espèces. Si les 3 espèces de manchots ont des caractéristiques très différentes, calculer des moyennes toutes espèces confondues n’a pas de sens. Voyons maintenant comment obtenir ces même indices pour chaque espèce.\n\n\n1.2.2 Intérêt de la fonction group_by()\nLa fonction summarise() devient particulièrement puissante lorsqu’elle est combinée avec la fonction group_by() :\n\n\n\n\n\nFigure 1.2: Fonctionnement de group_by() travaillant de concert avec summarise(), tiré de la ‘cheatsheet’ de dplyr et tidyr\n\n\n\n\nComme son nom l’indique, la fonction group_by() permet de créer des sous-groupes dans un tableau, afin que le résumé des données soit calculé pour chacun des sous-groupes plutôt que sur l’ensemble du tableau. En ce sens, son fonctionnement est analogue à celui des facets de ggplot2 qui permettent de scinder les données d’un graphique en plusieurs sous-groupes.\nPour revenir à l’exemple de la longueur du bec des manchots, imaginons que nous souhaitions calculer les moyennes et les écart-types pour chacune des trois espèces. Voilà comment procéder :\n\npenguins %>%\n  group_by(species) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species   moyenne ecart_type\n  <fct>       <dbl>      <dbl>\n1 Adelie       38.8       2.66\n2 Chinstrap    48.8       3.34\n3 Gentoo       47.5       3.08\n\n\nIci, les étapes sont les suivantes :\n\nOn prend le tableau penguins, puis\nOn groupe les données selon la variable species, puis\nOn résume les données groupées sous la forme de moyennes et d’écart-types\n\nLà où nous avions auparavant une seule valeur de moyenne et d’écart-type pour l’ensemble des individus du tableau de données, nous avons maintenant une valeur de moyenne et d’écart-type pour chaque modalité de la variable espèce. Puisque le facteur species contient 3 modalités (Adelie, Chinstrap et Gentoo), le résumé des données contient maintenant 3 lignes.\nNous pouvons aller plus loin. Ajoutons à ce résumé 2 variables supplémentaires : le nombre de mesures et l’erreur standard (notée \\(se\\)), qui peut être calculée de la façon suivante :\n\\[se \\approx \\frac{s}{\\sqrt{n}}\\]\navec \\(s\\), l’écart-type de l’échantillon et \\(n\\), la taille de l’échantillon (plus d’informations sur cette statistique très importante dans la Section 1.4). Nous allons donc calculer ici ces résumés, et nous donnerons un nom au tableau créé pour pouvoir ré-utiliser ces statistiques descriptives :\n\nstats_esp <- penguins %>%\n  group_by(species) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\nstats_esp\n\n# A tibble: 3 × 5\n  species   moyenne ecart_type nb_obs erreur_std\n  <fct>       <dbl>      <dbl>  <int>      <dbl>\n1 Adelie       38.8       2.66    152      0.216\n2 Chinstrap    48.8       3.34     68      0.405\n3 Gentoo       47.5       3.08    124      0.277\n\n\nVous constatez ici que nous avons 4 statistiques descriptives pour chaque espèce. Deux choses sont importantes à retenir ici :\n\non peut obtenir le nombre d’observations dans chaque sous-groupe d’un tableau groupé en utilisant la fonction n(). Cette fonction n’a besoin d’aucun argument : elle détermine automatiquement la taille des groupes créés par group_by().\non peut créer de nouvelles variables en utilisant le nom de variables créées auparavant. Ainsi, nous avons créé la variable erreur_std en utilisant deux variables créées au préalable : ecart-type et nb_obs\n\n\n\n1.2.3 Grouper par plus d’une variable\nJusqu’ici, nous avons groupé les données par espèce. Il est tout à fait possible de grouper les données par plus d’une variable, par exemple, par espèce et par sexe :\n\nstats_esp_sex <- penguins %>%\n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nstats_esp_sex\n\n# A tibble: 8 × 6\n# Groups:   species [3]\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  <fct>     <fct>    <dbl>      <dbl>  <int>      <dbl>\n1 Adelie    female    37.3       2.03     73      0.237\n2 Adelie    male      40.4       2.28     73      0.267\n3 Adelie    <NA>      37.8       2.80      6      1.14 \n4 Chinstrap female    46.6       3.11     34      0.533\n5 Chinstrap male      51.1       1.56     34      0.268\n6 Gentoo    female    45.6       2.05     58      0.269\n7 Gentoo    male      49.5       2.72     61      0.348\n8 Gentoo    <NA>      45.6       1.37      5      0.615\n\n\nEn plus de la variable species, la tableau stats_esp_sex contient une variable sex. Les statistiques que nous avons calculées plus tôt sont maintenant disponibles pour chaque espèce et chaque sexe. D’ailleurs, puisque le sexe de certains individus est inconnu, nous avons également des lignes pour lesquelles le sexe affiché est NA. Pour les éliminer, il suffit de retirer les lignes du tableau pour lesquelles le sexe des individus est inconnu, puis de recalculer les mêmes indices :\n\nstats_esp_sex2 <- penguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nstats_esp_sex2\n\n# A tibble: 6 × 6\n# Groups:   species [3]\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  <fct>     <fct>    <dbl>      <dbl>  <int>      <dbl>\n1 Adelie    female    37.3       2.03     73      0.237\n2 Adelie    male      40.4       2.28     73      0.267\n3 Chinstrap female    46.6       3.11     34      0.533\n4 Chinstrap male      51.1       1.56     34      0.268\n5 Gentoo    female    45.6       2.05     58      0.269\n6 Gentoo    male      49.5       2.72     61      0.348\n\n\nSi vous ne comprenez pas la commande filter(!is.na(sex)), je vous encourage vivement à consulter cette section du livre en ligne de Biométrie du semestre 3.\nEnfin, lorsque nous groupons par plusieurs variables, il peut être utile de présenter les résultats sous la forme d’un tableau large (grâce à la fonction pivot_wider()) pour l’intégration dans un rapport par exemple. La fonction pivot_wider() permet de passer d’un tableau qui possède ce format :\n\npenguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    moyenne\n  <fct>     <fct>    <dbl>\n1 Adelie    female    37.3\n2 Adelie    male      40.4\n3 Chinstrap female    46.6\n4 Chinstrap male      51.1\n5 Gentoo    female    45.6\n6 Gentoo    male      49.5\n\n\nà un tableau sous ce format :\n\npenguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE)) %>% \n  pivot_wider(names_from = sex,\n              values_from = moyenne)\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  <fct>      <dbl> <dbl>\n1 Adelie      37.3  40.4\n2 Chinstrap   46.6  51.1\n3 Gentoo      45.6  49.5\n\n\nSous cette forme, les données ne sont plus “rangées”, c’est à dire que nous n’avons plus une observation par ligne et une variable par colonne. En effet ici, la variable sex est maintenant “étalée” dans 2 colonnes distinctes : chaque modalité modalité du facteur de départ (female et male) est utilisé en tant que titre de nouvelles colonnes, et la variable moyenne est répartie dans deux colonnes. Ce format de tableau n’est pas idéal pour les statistiques ou les représentations graphiques, mais il est plus synthétique, et donc plus facile à inclure dans un rapport ou un compte-rendu.\n\n\n1.2.4 Un raccourci pratique pour compter des effectifs\nIl est extrêmement fréquent d’avoir à grouper des données en fonction d’une variable catégorielle puis d’avoir à compter le nombre d’observations de chaque modalité avec n() :\n\npenguins %>% \n  group_by(species) %>% \n  summarise(effectif = n())\n\n# A tibble: 3 × 2\n  species   effectif\n  <fct>        <int>\n1 Adelie         152\n2 Chinstrap       68\n3 Gentoo         124\n\n\nCes deux opérations sont tellement fréquentes (regrouper puis compter) que le package dplyr nous fournit un raccourci : la fonction count().\nLe code ci-dessus est équivalent à celui-ci :\n\npenguins %>% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nNotez qu’avec la fonction count(), la colonne qui contient les comptages s’appelle toujours n par défaut. Comme avec group_by(), il est bien sûr possible d’utiliser count() avec plusieurs variables :\n\npenguins %>% \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  <fct>     <fct>  <int>\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    <NA>       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    <NA>       5\n\n\n\npenguins %>% \n  filter(!is.na(sex)) %>% \n  count(species, sex)\n\n# A tibble: 6 × 3\n  species   sex        n\n  <fct>     <fct>  <int>\n1 Adelie    female    73\n2 Adelie    male      73\n3 Chinstrap female    34\n4 Chinstrap male      34\n5 Gentoo    female    58\n6 Gentoo    male      61\n\n\nEt il est évidemment possible de présenter le résultats sous un format de tableau large :\n\npenguins %>% \n  filter(!is.na(sex)) %>% \n  count(species, sex) %>% \n  pivot_wider(names_from = sex,\n              values_from = n)\n\n# A tibble: 3 × 3\n  species   female  male\n  <fct>      <int> <int>\n1 Adelie        73    73\n2 Chinstrap     34    34\n3 Gentoo        58    61\n\n\n\n\n1.2.5 Exercices\n\nAvec le tableau diamonds du package ggplot2, faites un tableau indiquant combien de diamants de chaque couleur on dispose. Vous devriez obtenir le tableau suivant :\n\n\n\n# A tibble: 7 × 2\n  color     n\n  <ord> <int>\n1 D      6775\n2 E      9797\n3 F      9542\n4 G     11292\n5 H      8304\n6 I      5422\n7 J      2808\n\n\n\nExaminez le tableau weather du package nycflights13 et lisez son fichier d’aide pour comprendre à quoi correspondent les données et comment elles ont été acquises.\nÀ partir du tableau weather faites un tableau indiquant les vitesses de vents minimales, maximales et moyennes, enregistrées chaque mois dans chaque aéroport de New York. Indice : les 3 aéroports de New York sont Newark, LaGuardia Airport et John F. Kennedy, notés respectivement EWR, LGA et JFK dans la variable origin. Votre tableau devrait ressembler à ceci :\n\n\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month max_wind min_wind moy_wind\n   <chr>  <int>    <dbl>    <dbl>    <dbl>\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2   1048.         0    12.2 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# … with 26 more rows\n\n\n\nSachant que les vitesses du vent sont exprimées en miles par heure, certaines valeurs sont-elles surprenantes ? À l’aide de la fonction filter(), éliminez la ou les valeurs aberrantes. Vous devriez obtenir ce tableau :\n\n\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month max_wind min_wind moy_wind\n   <chr>  <int>    <dbl>    <dbl>    <dbl>\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2     31.1        0    10.7 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# … with 26 more rows\n\n\n\nEn utilisant les données de vitesse de vent du tableau weather, produisez le graphique suivant :\n\n\n\n\n\n\nIndications :\n\nles vitesses de vent aberrantes ont été éliminées grâce à la fonction filter()\nla fonction geom_jitter() a été utilisée avec l’argument height = 0\nla transparence des points est fixée à 0.2\n\n\nÀ votre avis :\n\n\npourquoi les points sont-ils organisés en bandes horizontales ?\npourquoi n’y a-t-il jamais de vent entre 0 et environ 3 miles à l’heure (mph) ?\nSachant qu’en divisant des mph par 1.151 on obtient des vitesses en nœuds, que nous apprend cette commande :\n\n\nsort(unique(weather$wind_speed)) / 1.151\n\n [1]   0.000000   2.999427   3.999235   4.999044   5.998853   6.998662\n [7]   7.998471   8.998280   9.998089  10.997897  11.997706  12.997515\n[13]  13.997324  14.997133  15.996942  16.996751  17.996560  18.996368\n[19]  19.996177  20.995986  21.995795  22.995604  23.995413  24.995222\n[25]  25.995030  26.994839  27.994648  28.994457  29.994266  30.994075\n[31]  31.993884  32.993692  33.993501  34.993310  36.992928 910.825873"
  },
  {
    "objectID": "01-EDA.html#créer-des-résumés-de-données-avec-des-fonctions-spécifiques",
    "href": "01-EDA.html#créer-des-résumés-de-données-avec-des-fonctions-spécifiques",
    "title": "1  Exploration statistique des données",
    "section": "1.3 Créer des résumés de données avec des fonctions spécifiques",
    "text": "1.3 Créer des résumés de données avec des fonctions spécifiques\nLes fonctions group_by() et summarise() permettent donc de calculer n’importe quel indice de statistique descriptive sur un tableau de donnée entier ou sur des modalités ou combinaisons de modalités de facteurs. Il existe par ailleurs de nombreuses fonctions, disponibles de base dans R ou dans certains packages spécifiques, qui permettent de fournir des résumés plus ou moins automatiques de tout ou partie des variables d’un jeu de données. Nous allons en décire 2 ici, mais il en existe beaucoup d’autres : à vous d’explorer les possibilités et d’utiliser les fonctions qui vous paraissent les plus pertinentes, les plus simples à utiliser ou les plus complètes.\n\n1.3.1 La fonction summary()\nLa fonction summary() permet d’obtenir des résumés de données pour tous types d’objets dans R. Selon la classe des objets que l’on transmets à summary(), la nature des résultats obtenus changera. Nous verrons ainsi au semestre 6 que cette fonction peut être utilisée pour examiner les résultats de modèles de régressions linéaires ou d’analyses de variances. Pour l’instant, nous nous intéressons à 3 situations :\n\nce que renvoie la fonction quand on lui fournit un vecteur\nce que renvoie la fonction quand on lui fournit un facteur\nce que renvoie la fonction quand on lui fournit un tableau\n\n\n1.3.1.1 Variable continue : vecteur numérique\nCommençons par fournir un vecteur numérique à la fonction summary(). Nous allons pour cela extraire les données de masses corporelles des manchots du tableau penguins :\n\npenguins$body_mass_g\n\n  [1] 3750 3800 3250   NA 3450 3650 3625 4675 3475 4250 3300 3700 3200 3800 4400\n [16] 3700 3450 4500 3325 4200 3400 3600 3800 3950 3800 3800 3550 3200 3150 3950\n [31] 3250 3900 3300 3900 3325 4150 3950 3550 3300 4650 3150 3900 3100 4400 3000\n [46] 4600 3425 2975 3450 4150 3500 4300 3450 4050 2900 3700 3550 3800 2850 3750\n [61] 3150 4400 3600 4050 2850 3950 3350 4100 3050 4450 3600 3900 3550 4150 3700\n [76] 4250 3700 3900 3550 4000 3200 4700 3800 4200 3350 3550 3800 3500 3950 3600\n [91] 3550 4300 3400 4450 3300 4300 3700 4350 2900 4100 3725 4725 3075 4250 2925\n[106] 3550 3750 3900 3175 4775 3825 4600 3200 4275 3900 4075 2900 3775 3350 3325\n[121] 3150 3500 3450 3875 3050 4000 3275 4300 3050 4000 3325 3500 3500 4475 3425\n[136] 3900 3175 3975 3400 4250 3400 3475 3050 3725 3000 3650 4250 3475 3450 3750\n[151] 3700 4000 4500 5700 4450 5700 5400 4550 4800 5200 4400 5150 4650 5550 4650\n[166] 5850 4200 5850 4150 6300 4800 5350 5700 5000 4400 5050 5000 5100 4100 5650\n[181] 4600 5550 5250 4700 5050 6050 5150 5400 4950 5250 4350 5350 3950 5700 4300\n[196] 4750 5550 4900 4200 5400 5100 5300 4850 5300 4400 5000 4900 5050 4300 5000\n[211] 4450 5550 4200 5300 4400 5650 4700 5700 4650 5800 4700 5550 4750 5000 5100\n[226] 5200 4700 5800 4600 6000 4750 5950 4625 5450 4725 5350 4750 5600 4600 5300\n[241] 4875 5550 4950 5400 4750 5650 4850 5200 4925 4875 4625 5250 4850 5600 4975\n[256] 5500 4725 5500 4700 5500 4575 5500 5000 5950 4650 5500 4375 5850 4875 6000\n[271] 4925   NA 4850 5750 5200 5400 3500 3900 3650 3525 3725 3950 3250 3750 4150\n[286] 3700 3800 3775 3700 4050 3575 4050 3300 3700 3450 4400 3600 3400 2900 3800\n[301] 3300 4150 3400 3800 3700 4550 3200 4300 3350 4100 3600 3900 3850 4800 2700\n[316] 4500 3950 3650 3550 3500 3675 4450 3400 4300 3250 3675 3325 3950 3600 4050\n[331] 3350 3450 3250 4050 3800 3525 3950 3650 3650 4000 3400 3775 4100 3775\n\n\nNous avons donc 344 valeurs de masses en grammes qui correspondent aux 344 manchots du jeu de données. La fonction summary() renvoie le résumé suivant lorsqu’on lui fournit ces valeurs :\n\nsummary(penguins$body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nNous obtenons ici 7 valeurs, qui correspondent respectivement à :\n\nla valeur minimale observée dans le vecteur. Ici, le manchot le plus léger de l’échantillon pèse donc 2700 grammes.\nla valeur du premier quartile du vecteur. Le premier quartile est la valeur qui coupe l’échantillon en 2 groupes : 25% des observations du vecteur sont inférieures au premier quartile, et 75% des observations du vecteur sont supérieures au premier quartile. Ici, 25% des manchots de l’échantillon (soit 86 individus) ont une masse inférieure à 3550 grammes, et 75% des individus de l’échantillon (soit 258 individus) ont une masse supérieure à 3550 grammes.\nla valeur de médiane du vecteur. La médiane est la valeur qui coupe l’échantillon en 2 groupes : 50% des observations du vecteur sont inférieures à la médiane, et 50% des observations du vecteur sont supérieures à la médiane. Ici, 50% des manchots de l’échantillon (soit 172 individus) ont une masse inférieure à 4050 grammes, et 50% des individus de l’échantillon (soit 172 individus) ont une masse supérieure à 4050 grammes.\nla moyenne du vecteur. Ici, les manchots des 3 espèces du jeu de données ont en moyenne une masse 4202 grammes.\nla valeur du troisième quartile du vecteur. Le troisième quartile est la valeur qui coupe l’échantillon en 2 groupes : 75% des observations du vecteur sont inférieures au troisième quartile, et 25% des observations du vecteur sont supérieures au troisième quartile. Ici, 75% des manchots de l’échantillon (soit 258 individus) ont une masse inférieure à 4700 grammes, et 25% des individus de l’échantillon (soit 86 individus) ont une masse supérieure à 4750 grammes.\nla valeur maximale observée dans le vecteur. Ici, le manchot le plus lourd de l’échantillon pèse donc 6300 grammes.\nle nombre de données manquantes. Ici, 2 manchots n’ont pas été pesés et présentent donc la mention NA (comme Not Available) pour la variable body_mass_g.\n\nCes différents indices statistiques nous renseignent donc à la fois sur la position de la distribution et sur la dispersion des données.\n\nLa position correspond à la tendance centrale et indique quelles sont les valeurs qui caractérisent le plus grand nombre d’individus. La moyenne et la médiane sont les deux indices de position les plus fréquemment utilisés. Lorsqu’une variable a une distribution parfaitement symétrique, la moyenne et la médiane sont strictement égales. Mais lorsqu’une distribution est asymétrique, la moyenne et la médiane diffèrent. En particulier, la moyenne est beaucoup plus sensible aux valeurs extrêmes que la médiane. Cela signifie que quand une distribution est très asymétrique, la médiane est souvent une meilleure indication des valeurs les plus fréquemment observées.\n\n\n\n\n\n\nFigure 1.3: Distribution des masses corporelles des manchots\n\n\n\n\nL’histogramme de la Figure 1.3 montre la distribution de la taille des manchots (toutes espèces confondues). Cette distribution présente une asymétrie à droite. Cela signifie que la distribution n’est pas symétrique et que la “queue de distribution” est plus longue à droite qu’à gauche. La plupart des individus ont une masse comprise entre 3500 et 3700 grammes, au niveau du pic principal du graphique. La médiane, en orange et qui vaut 4050 grammes est plus proche du pic que la moyenne, en rouge, qui vaut 4202 grammes. Ici, la différence entre moyenne et médiane n’est pas énorme, mais elle peut le devenir si la distribution est vraiment très asymétrique, par exemple, si quelques individus seulement avaient une masse supérieure à 7000 grammes, la moyenne serait tirée vers la droite du graphique alors que la médiane ne serait presque pas affectée. la moyenne représenterait alors encore moins fidèlement la tendance centrale.\nSi l’on revient à la fonction summary(), observer des valeurs proches pour la moyenne et la médiane nous indique donc le degré de symétrie de la distribution.\n\nLa dispersion des données nous renseigne sur la dispersion des points autour des indices de position. Les quartiles et les valeurs minimales et maximales renvoyées par la fonction summary() nous renseigne sur l’étalement des points. Les valeurs situées entre le premier et le troisième quartile correspondent aux 50% des valeurs de l’échantillon les plus centrales. Plus l’étendue entre ces quartiles (notée IQR pour “intervalle interquartile”) sera grande, plus la dispersion sera importante. D’ailleurs, lorsque la dispersion est très importante, les moyennes et médianes ne renseignent que très moyennement sur le tendance centrale. Les indices de position sont surtout pertinents lorsque la dispersion des points autour de cette tendance centrale n’est pas trop large. Par exemple, si la distribution des données ressemblait à ceci (Figure 1.4), la moyenne et la médiane seraient fort peu utiles car très éloignées de la plupart des observations :\n\n\n\n\n\n\nFigure 1.4: Distribution des masses corporelles (données fictives)\n\n\n\n\nOn comprend donc l’importance de considérer les indices de dispersion en plus des indices de position pour caractériser et comprendre une série de données numériques. L’intervalle interquartile est toujours utile pour connaître l’étendue des données qui correspond aux 50% des observations les plus centrales. Les autres indices de dispersion très fréquemment utilisés, mais qui ne sont pas proposés par défaut par la fonction summary(), sont la variance et l’écart-type. Il est possible de calculer tous les indices renvoyés par la fonction summary() et ceux qui nous manquent grâce à la fonction summarise() :\n\npenguins %>% \n  summarise(min = min(body_mass_g, na.rm = TRUE),\n            Q1 = quantile(body_mass_g, 0.25, na.rm = TRUE),\n            med = median(body_mass_g, na.rm = TRUE),\n            moy = mean(body_mass_g, na.rm = TRUE),\n            Q3 = quantile(body_mass_g, 0.75, na.rm = TRUE),\n            max = max(body_mass_g, na.rm = TRUE),\n            var = var(body_mass_g, na.rm = TRUE),\n            et = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n    min    Q1   med   moy    Q3   max     var    et\n  <int> <dbl> <dbl> <dbl> <dbl> <int>   <dbl> <dbl>\n1  2700  3550  4050 4202.  4750  6300 643131.  802.\n\n\nVous notez que le code est beaucoup plus long, et qu’utiliser summary() peut donc faire gagner beaucoup de temps, même si cette fonction ne nous fournit ni la variance ni l’écart-type. Mais comme souvent dans R, il est possible de calculer à la main toutes ces valeurs si besoin. Les fonctions suivantes pourront donc vous être utiles :\n\nmean() permet de calculer la moyenne.\nmedian() permet de calculer la médiane.\nmin() et max() permettent de calculer les valeurs minimales et maximales respectivement.\nquantile() permet de calculer les quartiles.\nsd() permet de calculer l’écart-type.\nvar() permet de calculer la variance.\n\nPour toutes ces fonctions l’argument na.rm = TRUE permet d’obtenir les résultats même si certaines valeurs sont manquantes. Enfin, la fonction IQR() permet de calculer l’intervalle inter-quartiles :\n\nIQR(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 1200\n\n\nIci, les 50% des valeurs les plus centrales de l’échantillon sont situées dans un intervalle de 1200 grammes autour de la médiane.\n\n\n1.3.1.2 Variable quantitative : facteur\nSi l’on fournit une variable catégorielle ou facteur à summary(), le résultat obtenu sera naturellement différent : calculer des moyennes, médianes ou quartiles n’aurait en effet pas de sens lorsque la variable fournie ne contient que des catégories :\n\nsummary(penguins$species)\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nPour les facteurs, summary() compte simplement le nombre d’observations pour chaque modalité. Ici, la variable species est un facteur qui compte 3 modalités. La fonction summary() nous indique donc le nombre d’individus pour chaque modalité : notre jeu de données se compose de 152 individus de l’espèce Adélie, 68 individus de l’espèce Chinstrap, et 124 individus de l’espèce Gentoo.\nComme pour les vecteurs numériques, si le facteur présente des données manquantes, la fonction summary() compte également leur nombre :\n\nsummary(penguins$sex)\n\nfemale   male   NA's \n   165    168     11 \n\n\nPour les facteurs, la fonction summary() est donc tout à fait équivalente à la fonction count() :\n\npenguins %>% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nL’avantage de la fonction count() est qu’il est possible d’utiliser plusieurs facteurs pour compter le nombre d’observations de toutes les combinaisons de modalités (par exemple, combien d’individus de chaque sexe pour chaque espèce), ce qui n’est pas possible avec la fonction summary().\n\n\n1.3.1.3 Les tableaux : data.frame ou tibble\nL’avantage de la fonction summary() par rapport à la fonction count() apparaît lorsque l’on souhaite obtenir des informations sur toutes les variables d’un tableau à la fois :\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nIci, on obtient un résumé pour chaque colonne du tableau. Les colonnes numériques sont traitées comme les vecteurs numériques (on obtient alors les minimas et maximas, les quartiles, les moyennes et médianes) et les colonnes contenant des variables catégorielles sont traitées comme des facteurs (et on obtient alors le nombre d’observations pour chaque modalité).\nOn constate au passage que la variable year est considérée ici comme une variable numérique, alors qu’elle devrait plutôt être considérée comme un facteur, ce qui nous permettrait de savoir combien d’individus ont été échantillonnés chaque année :\n\npenguins %>% \n  mutate(year = factor(year)) %>% \n  summary()\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex        year    \n Min.   :172.0     Min.   :2700   female:165   2007:110  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   2008:114  \n Median :197.0     Median :4050   NA's  : 11   2009:120  \n Mean   :200.9     Mean   :4202                          \n 3rd Qu.:213.0     3rd Qu.:4750                          \n Max.   :231.0     Max.   :6300                          \n NA's   :2         NA's   :2                             \n\n\nAu final, la fonction summary() est très utile dans certaines situations, notamment pour avoir rapidement accès à des statistiques descriptives simples sur toutes les colonnes d’un tableau. Elle reste cependant limitée car d’une part, elle ne fournit pas les variances ou les écarts-types pour les variables numériques, et il est impossible d’avoir des résumés plus fins, pour chaque modalité d’un facteur par exemple. Ici, il serait en effet intéressant d’avoir des informations synthétiques concernant les mesures biométriques des manchots, espèce par espèce, plutôt que toutes espèces confondues. C’est là que la fonction skim() intervient.\n\n\n\n1.3.2 La fonction skim()\nLa fonction skim() fait partie du package skimr. Avant de pouvoir l’utiliser, pensez donc à l’installer et à le charger en mémoire si ce n’est pas déjà fait. Comme pour la fonction summary(), on peut utiliser la fonction skim() sur plusieurs types d’objets. Nous nous contenterons d’examiner ici le cas le plus fréquent : celui des tableaux, groupés avec group_by() ou non.\n\n1.3.2.1 Tableau non groupé\nCommençons par examiner le résultat avec le tableau penguins non groupé :\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\nLes résultats obtenus grâce à cette fonction sont nombreux. La première section nous donne des informations sur le tableau :\n\nson nom, son nombre de lignes et de colonnes\nla nature des variables qu’il contient (ici 3 facteurs et 5 variables numériques)\nla présence de variables utilisées pour faire des regroupements (il n’y en a pas encore à ce stade)\n\nEnsuite, un bloc apporte des information sur chaque facteur présent dans le tableau :\n\nle nom de la variable catégorielle (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\ndes informations sur le nombre de modalités (n_unique)\nle nombre d’observations pour les modalités les plus représentées (top_counts)\n\nEn un coup d’œil, on sait donc que 3 espèces sont présentes (et on connait leurs effectifs), on sait que les manchots ont été échantillonnées sur 3 îles, et on sait que le sexe de 11 individu (sur 344) est inconnu. Pour le reste, il y a presque autant de femelles que de mâles.\nLe dernier bloc renseigne sur les variables numériques. Pour chaque d’elle, on a :\n\nle nom de la variable numérique (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\nla moyenne (mean) et l’écart-type (sd), ce qui est une nouveauté par rapport à la fonction summary()\nles valeurs minimales (p0), de premier quartile (p25), de second quartile (p50, c’est la médiane !), de troisième quartile (p75) et la valeur maximale (p100)\nun histogramme très simple qui donne un premier aperçu grossier de la forme de la distribution\n\nLà encore, en un coup d’œil, on dispose donc de toutes les informations pertinentes pour juger de la distribution, de la position et de la dispersion de chaque variable numérique du jeu de données.\n\n\n1.3.2.2 Tableau groupé\nLa fonction skim(), déjà très pratique, le devient encore plus lorsque l’on choisit de lui fournir seulement certaines variables, et qu’on fait certains regroupements. Par exemple, on peut sélectionner les variables relatives aux dimensions du bec (bill_length_mm et bill_depth_mm) avec la fonction select() que nous connaissons déjà, et demander un résumé des données pour chaque espèce grâce à la fonction group_by() que nous connaissons également :\n\npenguins %>%                     # Avec le tableau penguins...\n  select(species, \n         bill_length_mm,\n         bill_depth_mm) %>%      # Je sélectionne les variables d'intérêt...\n  group_by(species) %>%          # Je regroupe par espèce...\n  skim()                         # Et je produis un résumé des données\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n32.1\n36.75\n38.80\n40.75\n46.0\n▁▆▇▆▁\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n40.9\n46.35\n49.55\n51.08\n58.0\n▂▇▇▅▁\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n40.9\n45.30\n47.30\n49.55\n59.6\n▃▇▆▁▁\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n15.5\n17.50\n18.40\n19.00\n21.5\n▂▆▇▃▁\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n16.4\n17.50\n18.45\n19.40\n20.8\n▅▇▇▆▂\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n13.1\n14.20\n15.00\n15.70\n17.3\n▅▇▇▆▂\n\n\n\n\n\nOn constate ici que pour chaque variable numérique sélectionnée, des statistiques descriptives détaillées nous sont fournies pour chacune des 3 espèces. Ce premier examen semble montrer que :\n\nL’espèce Adélie est celle qui possède le bec le plus court (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces).\nL’espèce Gentoo est celle qui possède le bec le plus fin, ou le moins épais (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces)\nIl ne semble pas y avoir de fortes différences d’écarts-types (donc des dispersions des points autour des moyennes) entre les 3 espèces : pour chacune des 2 variables numériques, des valeurs d’écarts-types comparables sont en effet observées pour les 3 espèces\nLa distribution des 2 variables numériques semble approximativement suivre une distribution symétrique pour les 3 espèces, avec une forme de courbe en cloche. Les distributions devraient donc suivre à peu une distribution normale\n\n\n\n\n\n\n\nNote\n\n\n\nVous comprenez j’espère l’importance d’examiner ce genre de résumé des données avant de vous lancer dans des tests statistiques. Ils sont un complément indispensable aux explorations graphiques que vous devez également prendre l’habitude de réaliser pour mieux appréhender et comprendre la nature de vos données. Puisque chaque je de données est unique, vous devrez vous adapter à la situation et aux questions scientifiques qui vous sont posées (ou que vous vous posez !) : les choix qui seront pertinents pour une situation ne le seront pas nécessairement pour une autre. Mais dans tous les cas, pour savoir où vous allez et pour ne pas faire de bêtise au moment des tests statistiques et de leur interprétation, vous devrez toujours explorer vos données, avec des graphiques exploratoire et des statistiques descriptives.\n\n\n\n\n\n1.3.3 Exercice\nEn utilisant les fonctions de résumé abordées jusqu’ici et le tableau weather, répondez aux questions suivante :\n\nDans quel aéroport de New York les précipitations moyennes ont-elle été les plus fortes en 2013 ?\nDans quel aéroport de New York la vitesse du vent moyenne était-elle la plus forte en 2013 ? Quelle est cette vitesse ?\nDans quel aéroport de New York les rafales de vent étaient-elles les plus variables en 2013 ? Quel indice statistique vous donne cette information et quelle est sa valeur ?\nLes précipitation dans les 3 aéroports de New-York ont-elles une distribution symétrique ?\nQuelle est la température médiane observée en 2013 tous aéroports confondus ?\nTous aéroports confondus, quel est le mois de l’année où la température a été la plus variable en 2013 ? Quelles étaient les températures minimales et maximales observées ce mois-là ?"
  },
  {
    "objectID": "01-EDA.html#sec-disp",
    "href": "01-EDA.html#sec-disp",
    "title": "1  Exploration statistique des données",
    "section": "1.4 Dispersion et incertitude",
    "text": "1.4 Dispersion et incertitude\n\n1.4.1 La notion de dispersion\nComme expliqué plus haut, les indices de dispersion nous renseignent sur la variabilités des données autour de la valeur moyenne (ou médiane) d’une population ou d’un échantillon. L’écart-type, la variance et l’intervalle inter-quartiles sont 3 exemples d’indices de dispersion. Prenons l’exemple de l’écart-type. Un écart-type faible indique que la majorité des observations ont des valeurs proches de la moyenne. À l’inverse, un écart-type important indique que la plupart des points sont éloignés de la moyenne. L’écart-type est une caractéristique de la population que l’on étudie grâce à un échantillon, au même titre que la moyenne. En travaillant sur un échantillon, j’espère accéder aux vraies grandeurs de la population. Même si ces vraies grandeurs sont à jamais inaccessibles (on ne connaîtra jamais parfaitement quelle est la vraie valeur de moyenne \\(\\mu\\) ou d’écart-type \\(\\sigma\\) de la population), on espère qu’avec un échantillonnage réalisé correctement, la moyenne de l’échantillon (\\(\\bar{x}\\) ou \\(m\\)) et l’écart-type (\\(s\\)) de l’échantillon reflètent assez fidèlement les valeurs de la population générale. C’est la notion d’estimateur, intimement liée à la notion d’inférence statistique : la moyenne de l’échantillon est un estimateur de la moyenne \\(\\mu\\) de la population. C’est la raison pour laquelle on la note parfois \\(\\hat{\\mu}\\) (en plus de \\(\\bar{x}\\) ou \\(m\\)). De même, l’écart-type \\(s\\) et la variance \\(s^2\\) d’un échantillon sont des estimateurs de l’écart-type \\(\\sigma\\) et de la variance \\(\\sigma^2\\) de la population générale. C’est la raison pour laquelle on les note parfois \\(\\hat{\\sigma}\\) et \\(\\hat{\\sigma}^2\\) respectivement. L’accent circonflexe se prononce “chapeau”. On dit donc que \\(\\hat{\\sigma}\\) (sigma chapeau) est un estimateur de l´écart-type de la population générale. Comme nous l’avons vu, les indices de dispersion doivent accompagner les indices de position lorsque l’on décrit des données, car présenter une valeur de moyenne, ou de médiane seule n’a pas de sens : il faut savoir à quel point les données sont proches ou éloignées de la tendance centrale pour savoir si, dans la population générale, les indicateurs de position correspondent ou non, aux valeurs portées par la majorité des individus.\nNous avons vu plus haut comment calculer des indices de position et de dispersion. Tout ceci devrait donc être clair pour vous à ce stade.\n\n\n1.4.2 La notion d’incertitude\nPar ailleurs, puisqu’on ne sait jamais avec certitude si nos estimations (de moyennes ou d’écarts-types ou de tout autre paramètre) reflètent fidèlement ou non les vraies valeurs de la population, nous devons quantifier à quel point nos estimations s’écartent de celles de la population générale. C’est ce que permettent les indices d’incertitude. Les deux indices d’incertitude les plus connus (et les plus utilisés) sont l’intervalle de confiance à 95% (de la moyenne ou de tout autre estimateur ; les formules sont nombreuses et il n’est pas utile de les détailler ici : nous verrons comment les calculer plus bas) et l’erreur standard de la moyenne (\\(se_{\\bar{x}}\\)), dont la formule est la suivante :\n\\[se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\navec \\(s\\), l’écart-type de l’échantillon et \\(n\\) la taille de l’échantillon.\nComme pour la moyenne, on peut calculer l’erreur standard d’un écart-type, d’une médiane, d’une proportion, ou de tout autre estimateur calculé sur un échantillon. Cet indice d’incertitude ne nous renseigne pas sur une grandeur de la population générale qu’on chercherait à estimer, mais bien sur l’incertitude associée à une estimation que nous faisons en travaillant sur un échantillon de taille forcément limitée. Tout processus d’échantillonnage est forcément entaché d’incertitude, causée entre autre par le hasard de l’échantillonnage (ou fluctuation d’échantillonnage). Puisque nous travaillons sur des échantillons forcément imparfaits, les indices d’incertitude vont nous permettre de quantifier à quel point nos estimations s’écartent des vraies valeurs de la population. Ces “vraies valeurs”, faute de pouvoir collecter tous les individus de la population, resteront à jamais inconnues.\n\n\n\n\n\n\nAutrement dit…\n\n\n\nQuand on étudie des populations naturelles grâce à des échantillons on se trompe toujours. Les statistiques nous permettent de quantifier à quel point on se trompe grâce aux indices d’incertitude, et c’est déjà pas mal !\n\n\nEn examinant la formule de l’erreur standard de la moyenne présentée ci-dessus, on comprend intuitivement que plus la taille de l’échantillon (\\(n\\)) augmente, plus l’erreur standard (donc l’incertitude) associée à notre estimation de moyenne diminue. Autrement dit, plus les données sont abondantes dans l’échantillon, meilleure sera notre estimation de moyenne, et donc, moins le risque de raconter des bêtises sera grand.\nL’autre indice d’incertitude très fréquemment utilisé est l’intervalle de confiance à 95% (de la moyenne, de la médiane, de la variance, ou de toute autre estimateur calculé dans un échantillon). L’intervalle de confiance nous renseigne sur la gamme des valeurs les plus probables pour un paramètre de la population étudiée. Par exemple, si j’observe, dans un échantillon, une moyenne de 10, avec un intervalle de confiance calculé de [7 ; 15], cela signifie que, dans la population générale, la vraie valeur de moyenne a de bonnes chances de se trouver dans l’intervalle [7 ; 15]. Dans la population générale, toutes les valeurs comprises entre 7 et 15 sont vraisemblables pour la moyenne alors que les valeurs situées en dehors de cet intervalle sont moins probables. Une autre façon de comprendre l’intervalle de confiance est de dire que si je récupère un grand nombre d’échantillons dans la même population, en utilisant exactement le même protocole expérimental, 95% des échantillons que je vais récupérer auront une moyenne située à l’intérieur de l’intervalle de confiance à 95%, et 5% des échantillons auront une moyenne située à l’extérieur de l’intervalle de confiance à 95%. C’est une notion qui n’est pas si évidente que ça à comprendre, donc prenez bien le temps de relire cette section si besoin, et de poser des questions le cas échéant.\nConcrètement, plus l’intervalle de confiance est large, moins notre confiance est grande. Si, pour une moyenne d’échantillon de 10, l’intervalle de confiance à 95% vaut [9,5 ; 11], la gamme des valeurs probables pour la moyenne est étroite. Autrement dit, la moyenne de l’échantillon, qui vaut 10, a de bonne chances d’être très proche de la vraie valeur de moyenne de la population générale.\nLa notion d’intervalle de confiance à 95% est donc très proche de celle d’erreur standard. D’ailleurs, pour la plupart des grandeurs d’un échantillon, l’intervalle de confiance est très souvent calculé à partir de l’erreur standard.\n\n\n1.4.3 Calcul de l’erreur standard de la moyenne\nContrairement aux indices de position et de dispersion, il n’existe pas de fonction intégrée à R qui permette de calculer l’erreur standard de la moyenne. Toutefois, sa formule très simple nous permet de la calculer à la main quand on en a besoin grâce aux fonction group_by() et summarise().\nPar exemple, reprenons les données de température (tableau weather, colonne temp) dans les 3 aéroports de New York (colonne origin). Imaginons que nous souhaitions étudier les fluctuations de températures au fil des mois de l’année 2013 :\n\nJe vais commencer par transformer les températures (fournies en degrés Fahrenheit) en degrés Celsius :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8)\n\n# A tibble: 26,115 × 16\n   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed wind_g…¹\n   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>    <dbl>\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4        NA\n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06       NA\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5        NA\n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7        NA\n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7        NA\n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5        NA\n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0        NA\n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4        NA\n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0        NA\n10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8        NA\n# … with 26,105 more rows, 5 more variables: precip <dbl>, pressure <dbl>,\n#   visib <dbl>, time_hour <dttm>, temp_celsius <dbl>, and abbreviated variable\n#   name ¹​wind_gust\n\n\n\nEnsuite, je détermine, pour chaque jour de chaque mois de l’année, et pour chaque aéroport, quelle est la température maximale atteinte :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 1,092 × 4\n# Groups:   origin, month [36]\n   origin month   day temperature_max\n   <chr>  <int> <int>           <dbl>\n 1 EWR        1     1            5   \n 2 EWR        1     2            1.10\n 3 EWR        1     3            1.10\n 4 EWR        1     4            4.4 \n 5 EWR        1     5            6.7 \n 6 EWR        1     6            8.9 \n 7 EWR        1     7            8.3 \n 8 EWR        1     8            9.4 \n 9 EWR        1     9           10   \n10 EWR        1    10           10   \n# … with 1,082 more rows\n\n\n\nJe peux maintenant calculer la température moyenne mensuelle pour chaque aéroport :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 3\n# Groups:   origin [3]\n   origin month moyenne\n   <chr>  <int>   <dbl>\n 1 EWR        1    5.69\n 2 EWR        2    4.74\n 3 EWR        3    8.20\n 4 EWR        4   16.4 \n 5 EWR        5   22.2 \n 6 EWR        6   27.4 \n 7 EWR        7   31.0 \n 8 EWR        8   27.8 \n 9 EWR        9   24.6 \n10 EWR       10   20.0 \n# … with 26 more rows\n\n\nPour pouvoir réutiliser ce tableau, je lui donne un nom :\n\ntemperatures <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\nAu final, je peux faire un graphique de l’évolution de ces températures :\n\ntemperatures %>% \n  ggplot(aes(x = factor(month), y = moyenne)) +\n  geom_line(aes(group = 1)) +\n  geom_point() +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\") +\n  theme_bw()\n\n\n\n\nVous remarquerez que :\n\nj’associe factor(month), et non simplement month, à l’axe des x afin d’avoir, sur l’axe des abscisses, des chiffres cohérents allant de 1 à 12, et non des chiffres à virgule\nl’argument group = 1 doit être ajouté pour que la ligne reliant les points apparaisse. En effet, les lignes sont censées relier des points qui appartiennent à une même série temporelle. Or ici, nous avons transformé month en facteur. Préciser group = 1 permet d’indiquer à geom_line() que toutes les catégories du facteur month appartiennent au même groupe, que ce facteur peut être considéré comme une variable continue, et qu’il est donc correct de relier les points.\n\nPour les 3 aéroports, les profils de températures sont très proches. C’est tout à fait logique puisqu’ils sont situés dans un rayon de quelques kilomètres seulement. Le problème de ce graphique est que chaque point a été obtenu en calculant une moyenne. En janvier, nous avons fait la moyenne de 31 valeurs de températures quotidiennes pour chaque aéroport. En février, nous avons fait la moyenne de 28 valeurs de températures quotidiennes pour chaque aéroport. Et ainsi de suite pour tous les mois de l’année 2013. Puisque nous présentons des valeurs de moyennes, il nous faut présenter également l’incertitude associée à ces calculs de moyennes. Pour cela, nous devons calculer l’erreur standard des moyennes :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(temperature_max, na.rm = TRUE) / sqrt(N_obs))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne N_obs erreur_standard\n   <chr>  <int>   <dbl> <int>           <dbl>\n 1 EWR        1    5.69    31           1.14 \n 2 EWR        2    4.74    28           0.762\n 3 EWR        3    8.20    31           0.649\n 4 EWR        4   16.4     30           0.919\n 5 EWR        5   22.2     31           1.02 \n 6 EWR        6   27.4     30           0.769\n 7 EWR        7   31.0     31           0.700\n 8 EWR        8   27.8     31           0.385\n 9 EWR        9   24.6     30           0.716\n10 EWR       10   20.0     31           0.882\n# … with 26 more rows\n\n\nNotre tableau de statistiques descriptives possède maintenant 2 colonnes supplémentaires : le nombre d’observations (que j’ai nommé N_obs), et l’erreur standard associée à chaque moyenne, calculée grâce à la formule vue plus haut \\(se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\) (la fonction sqrt() permet de calculer la racine carrée). On constate que l’erreur standard, qui s’exprime dans la même unité que la moyenne, est variable selon les mois de l’année. Ainsi, pour l’aéroport de Newark, l’incertitude semble particulièrement faible pour le mois d’août (0.385 ºC) mais presque 3 fois plus forte pour le mois de janvier (1.14 ºC).\nUne fois de plus, je donne un nom à ce tableau de données pour pouvoir le réutiliser plus tard :\n\ntemperatures_se <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(temperature_max, na.rm = TRUE) / sqrt(N_obs))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\nNotez que le package ggplot2 contient une fonction permettant de calculer à la fois la moyenne et erreur standard de la moyenne d’un échantillon :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  4.55  6.82\n 2 EWR        2  4.74  3.97  5.50\n 3 EWR        3  8.20  7.55  8.85\n 4 EWR        4 16.4  15.5  17.4 \n 5 EWR        5 22.2  21.1  23.2 \n 6 EWR        6 27.4  26.6  28.1 \n 7 EWR        7 31.0  30.3  31.7 \n 8 EWR        8 27.8  27.4  28.2 \n 9 EWR        9 24.6  23.9  25.3 \n10 EWR       10 20.0  19.1  20.9 \n# … with 26 more rows\n\n\nLes résultats obtenus ne sont pas exactement au même format :\n\nla colonne y contient les valeurs de moyennes\nla colonne ymin contient la valeur de moyenne moins une fois l’erreur standard\nla colonne ymax contient la valeur de moyenne plus une fois l’erreur standard\n\nIl ne nous restera plus qu’à ajouter des barres d’erreur sur notre graphique pour visualiser l’incertitude associée à chaque valeur de moyenne.\n\n\n1.4.4 Calculs d’intervalles de confiance à 95%\nComme pour les erreurs standard, il est possible de calculer des intervalles de confiance de n’importe quel estimateur calculé à partir d’un échantillon, pour déterminer la gamme des valeurs les plus probables pour les paramètres équivalents dans la population générale. Nous nous concentrerons ici sur le calcul des intervalles de confiance à 95% de la moyenne, mais nous serons amenés à examiner également l’intervalle de confiance de la médiane, puis, dans la Chapitre 4, l’intervalle de confiance à 95% d’une différence de moyennes.\nContrairement à l’erreur standard, il n’y a pas qu’une bonne façon de calculer l’intervalle de confiance à 95% d’une moyenne. Plusieurs formules existent et le choix de la formule dépend en partie de la distribution des données (la distribution suit-elle une loi Normale ou non) et de la taille de l’échantillon dont nous disposons (\\(n\\) est-il supérieur à 30 ou non ?). Dans la situation idéale d’une variable qui suit la distribution Normale, les bornes inférieures et supérieures de l’intervalle de confiance à 95% sont obtenues grâce à cette formule\n\\[\\bar{x} - 1.96 \\cdot \\frac{s}{\\sqrt{n}} < \\mu < \\bar{x} + 1.96 \\cdot \\frac{s}{\\sqrt{n}}\\]\nAutrement dit, la vraie moyenne \\(\\mu\\) d’une population a de bonnes chances de se trouver dans un intervalle de plus ou moins 1.96 fois l’erreur standard de la moyenne. En première approximation, l’intervalle de confiance est donc la moyenne de l’échantillon \\(\\bar{x}\\) plus ou moins 2 fois l’erreur standard (que nous avons appris à calculer à la main un peu plus tôt). On peut donc calculer à la main les bornes inférieures et supérieures de l’intervalle de confiance ainsi :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max, mult = 1.96))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  3.46  7.92\n 2 EWR        2  4.74  3.24  6.23\n 3 EWR        3  8.20  6.93  9.47\n 4 EWR        4 16.4  14.6  18.2 \n 5 EWR        5 22.2  20.1  24.2 \n 6 EWR        6 27.4  25.8  28.9 \n 7 EWR        7 31.0  29.6  32.3 \n 8 EWR        8 27.8  27.0  28.5 \n 9 EWR        9 24.6  23.2  26.0 \n10 EWR       10 20.0  18.3  21.7 \n# … with 26 more rows\n\n\nIci, grâce à l’argument mult = 1.96 de la fonction mean_se() :\n\nla colonne ymin contient maintenant les valeurs de moyennes moins 1.96 fois l’erreur standard\nla colonne ymax contient maintenant les valeurs de moyennes plus 1.96 fois l’erreur standard\n\nDans la pratique, puisque cette méthode reste approximative et dépend de la nature des données dont on dispose, on utilisera plutôt des fonctions spécifiques qui calculeront pour nous les intervalles de confiance à 95% de nos estimateurs. C’est ce que permet en particulier la fonction mean_cl_normal() du package ggplot2. Il est toutefois important de bien comprendre qu’il y a un lien étroit entre l’erreur standard (l’incertitude associées à l’estimation d’un paramètre d’une population à partir des données d’un échantillon), et l’intervalle de confiance à 95% de ce paramètre.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_cl_normal(temperature_max))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  3.36  8.01\n 2 EWR        2  4.74  3.17  6.30\n 3 EWR        3  8.20  6.88  9.53\n 4 EWR        4 16.4  14.6  18.3 \n 5 EWR        5 22.2  20.1  24.2 \n 6 EWR        6 27.4  25.8  28.9 \n 7 EWR        7 31.0  29.5  32.4 \n 8 EWR        8 27.8  27.0  28.6 \n 9 EWR        9 24.6  23.1  26.0 \n10 EWR       10 20.0  18.2  21.8 \n# … with 26 more rows\n\n\nComme dans les tableaux précédents, 3 nouvelles colonnes ont été crées :\n\ny contient toujours la moyenne des températures mensuelles pour chaque aéroport\nymin contient maintenant les bornes inférieures de l’intervalle à 95% des moyennes\nymax contient maintenant les bornes supérieures de l’intervalle à 95% des moyennes\n\nPour que la suite soit plus claire, nous allons afficher et donner des noms à ces différents tableaux en prenant soin de renommer les colonnes pour plus de clarté.\nTout d’abord, nous disposons du tableau temperatures_se, qui contient, les moyennes des températures mensuelles de chaque aéroport de New York en 2013, et les erreurs standard de ces moyennes :\n\ntemperatures_se\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne N_obs erreur_standard\n   <chr>  <int>   <dbl> <int>           <dbl>\n 1 EWR        1    5.69    31           1.14 \n 2 EWR        2    4.74    28           0.762\n 3 EWR        3    8.20    31           0.649\n 4 EWR        4   16.4     30           0.919\n 5 EWR        5   22.2     31           1.02 \n 6 EWR        6   27.4     30           0.769\n 7 EWR        7   31.0     31           0.700\n 8 EWR        8   27.8     31           0.385\n 9 EWR        9   24.6     30           0.716\n10 EWR       10   20.0     31           0.882\n# … with 26 more rows\n\n\nEnsuite, nous avons produit un tableau presque équivalent que nous allons nommer temperature_se_bornes et pour lequel nous allons modifier le nom des colonnes y, ymin et ymax :\n\ntemperature_se_bornes <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max)) %>% \n  rename(moyenne = y,\n         moyenne_moins_se = ymin,\n         moyenne_plus_se = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_se_bornes\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne moyenne_moins_se moyenne_plus_se\n   <chr>  <int>   <dbl>            <dbl>           <dbl>\n 1 EWR        1    5.69             4.55            6.82\n 2 EWR        2    4.74             3.97            5.50\n 3 EWR        3    8.20             7.55            8.85\n 4 EWR        4   16.4             15.5            17.4 \n 5 EWR        5   22.2             21.1            23.2 \n 6 EWR        6   27.4             26.6            28.1 \n 7 EWR        7   31.0             30.3            31.7 \n 8 EWR        8   27.8             27.4            28.2 \n 9 EWR        9   24.6             23.9            25.3 \n10 EWR       10   20.0             19.1            20.9 \n# … with 26 more rows\n\n\nNous avons ensuite calculé manuellement des intervalles de confiance approximatifs, avec la fonction mean_se() et son argument mult = 1.96. Là encore, nous allons stocker cet objet dans un tableau nommé temperatures_ci_approx, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\ntemperature_ci_approx <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max, mult = 1.96)) %>% \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_ci_approx\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.46         7.92\n 2 EWR        2    4.74         3.24         6.23\n 3 EWR        3    8.20         6.93         9.47\n 4 EWR        4   16.4         14.6         18.2 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.6         32.3 \n 8 EWR        8   27.8         27.0         28.5 \n 9 EWR        9   24.6         23.2         26.0 \n10 EWR       10   20.0         18.3         21.7 \n# … with 26 more rows\n\n\nEnfin, nous avons calculé les intervalles de confiance avec une fonction spécialement dédiée à cette tâche : la fonction mean_cl_normal(). Nous allons stocker cet objet dans un tableau nommé temperatures_ci, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\ntemperature_ci <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_cl_normal(temperature_max)) %>% \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_ci\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.36         8.01\n 2 EWR        2    4.74         3.17         6.30\n 3 EWR        3    8.20         6.88         9.53\n 4 EWR        4   16.4         14.6         18.3 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.5         32.4 \n 8 EWR        8   27.8         27.0         28.6 \n 9 EWR        9   24.6         23.1         26.0 \n10 EWR       10   20.0         18.2         21.8 \n# … with 26 more rows\n\n\nMaintenant, si l’on compare les 2 tableaux contenant les calculs d’intervalles de confiance de la moyenne, on constate que les résultats sont très proches :\n\ntemperature_ci_approx\ntemperature_ci\n\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.46         7.92\n 2 EWR        2    4.74         3.24         6.23\n 3 EWR        3    8.20         6.93         9.47\n 4 EWR        4   16.4         14.6         18.2 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.6         32.3 \n 8 EWR        8   27.8         27.0         28.5 \n 9 EWR        9   24.6         23.2         26.0 \n10 EWR       10   20.0         18.3         21.7 \n# … with 26 more rows\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.36         8.01\n 2 EWR        2    4.74         3.17         6.30\n 3 EWR        3    8.20         6.88         9.53\n 4 EWR        4   16.4         14.6         18.3 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.5         32.4 \n 8 EWR        8   27.8         27.0         28.6 \n 9 EWR        9   24.6         23.1         26.0 \n10 EWR       10   20.0         18.2         21.8 \n# … with 26 more rows\n\n\n\n\nLes bornes inférieures et supérieures des intervalles de confiance à 95% des moyennes ne sont pas égales quand on les calcule manuellement de façon approchée et quand on les calcule de façon exacte, mais les différences sont minimes."
  },
  {
    "objectID": "01-EDA.html#visualiser-lincertitude-et-la-dispersion",
    "href": "01-EDA.html#visualiser-lincertitude-et-la-dispersion",
    "title": "1  Exploration statistique des données",
    "section": "1.5 Visualiser l’incertitude et la dispersion",
    "text": "1.5 Visualiser l’incertitude et la dispersion\nIl existe plusieurs façons de représenter visuellement les positions, les dispersions et les incertitudes. Concernant les positions et les dispersions tout d’abord, nous avons déjà vu plusieurs façons de faire au semestre 3, en particulier dans les parties consacrées aux histogrammes, aux stripcharts et aux boxplots. Nous reprenons ici brièvement chacun de ces 3 types de graphique afin de les remettre en contexte avec ce que nous avons appris ici.\nDans un dernier temps, nous verrons enfin comment visualiser l’incertitude associée à des calculs de moyennes ou de variance grâce aux barres d’erreurs ou aux encoches des boîtes à moustaches.\n\n1.5.1 Position et dispersion : les histogrammes\nJe vous renvoie à la partie sur les histogrammes du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire. Jetez aussi un œil la partie sur les histogrammes facettés.\nLes histogrammes permettent de déterminer à la fois où se trouvent les valeurs les plus fréquemment observées (la position du pic principal correspond à la tendance centrale), et la dispersion (ou variabilité) des valeurs autour de la tendance centrale. Par exemple, la fonction facet_grid() permet de faire des histogrammes des températures pour chaque aéroport de New York et chaque mois de l’année 2013 :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = temp_celsius, fill = factor(month))) +\n  geom_histogram(bins = 20, color = \"grey20\", show.legend = FALSE) +\n  facet_grid(factor(month) ~ origin, scales = \"free_y\") +\n  labs(x = \"Températures (ºC)\", y = \"Fréquence\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nIci, 36 histogrammes sont produits. Ils permettent de constater que :\n\nles températures évoluent à peu près de la même façon dans les 3 aéroports (les 3 colonnes de graphiques se ressemblent beaucoup)\nles températures moyennes sont plus faibles en hiver qu’en été, et qu’elles sont intermédiaires au printemps et à l’automne. C’est bien la position des pics sur l’axe des abscisses qui nous renseigne là-dessus. On sait aussi que les températures moyennes les plus fortes sont autour de 25 degrés ºC en juillet, alors que ces mêmes températures moyennes sont proches de 0 ºC en janvier, février et décembre.\nla variabilité des température est comparable pour la plupart des mois de l’année, avec une exception au mois d’août où la dispersion des valeurs semble plus limitée. Cette fois, c’est l’étalement de l’histogramme qui nous renseigne sur la dispersion.\n\n\n\n1.5.2 Position et dispersion : les stripcharts\nUne autre façon de visualiser à la fois les tendances centrales et les dispersion consiste à produire un nuage de points “stripchart”. Là encore, je vous renvoie à la partie sur les stripcharts du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_jitter(shape = 21, color = \"grey20\", show.legend = FALSE,\n              width = 0.15, height = 0,\n              alpha = 0.5) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCette fois, nous visualisons la totalité des données disponibles, et non les données regroupées dans des classes plus ou moins arbitraires. Mais là encore, on peut facilement comparer la position de chaque série de données : pour les mois d’été, les températures sont plus élevées que pour les mois d’hiver. Et la dispersion des données est aussi facile à comparer entre les mois. Par exemple, la variabilités des températures en janvier est nettement supérieure à celle du mois de février. C’est ici l’étendue du nuage de point sur l’axe des ordonnées qui nous permet de le dire.\n\n\n1.5.3 Position et dispersion : les boxplots\nLa dernière façon classique de visualiser à la fois les tendances centrales et les dispersion consiste à produire graphique boîte à moustaches, ou “boxplot”. Là encore, je vous renvoie à la partie sur les boxplots du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.5) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nVous voyez que le code est très proche pour produire un stripchart ou un boxplot. Comme indiqué au semestre 3, les différents éléments de chaque boîte nous renseignent sur la position et sur la dispersion des données pour chaque mois et chaque aéroport :\n\nLa limite inférieure de la boîte correspond au premier quartile : 25% des données de l’échantillon sont situées au-dessous de cette valeur.\nLa limite supérieure de la boîte correspond au troisième quartile : 25% des données de l’échantillon sont situées au-dessus de cette valeur.\nLe segment épais à l’intérieur de la boîte correspond au second quartile : c’est la médiane de l’échantillon, qui nous renseigne sur la position de la distribution. 50% des données de l’échantillon sont situées au-dessus de cette valeur, et 50% au-dessous.\nLa hauteur de la boîte correspond à l’étendue (ou intervalle) inter-quartile ou Inter Quartile Range (IQR) en anglais. On trouve dans cette boîte 50% des observations de l’échantillon. C’est une mesure de la dispersion des 50% des données les plus centrales. Une boîte plus allongée indique donc une plus grande dispersion.\nLes moustaches correspondent à des valeurs qui sont en dessous du premier quartile (pour la moustache du bas) et au-dessus du troisième quartile (pour la moustache du haut). La règle utilisée dans R est que ces moustaches s’étendent jusqu’aux valeurs minimales et maximales de l’échantillon, mais elles ne peuvent en aucun cas s’étendre au-delà de 1,5 fois la hauteur de la boîte (1,5 fois l’IQR) vers le haut et le bas. Si des points apparaissent au-delà des moustaches (vers le haut ou le bas), ces points sont appelés “outliers”. On peut en observer un pour l’espèce Adélie. Ce sont des points qui s’éloignent du centre de la distribution de façon importante puisqu’ils sont au-delà de 1,5 fois l’IQR de part et d’autre du premier ou du troisième quartile. Il peut s’agir d’anomalies de mesures, d’anomalies de saisie des données, ou tout simplement, d’enregistrements tout à fait valides mais atypiques ou extrêmes. J’attire votre attention sur le fait que la définition de ces outliers est relativement arbitraire. Nous pourrions faire le choix d’étendre les moustaches jusqu’à 1,8 fois l’IQR (ou 2, ou 2,5). Nous observerions alors beaucoup moins d’outliers. D’une façons générale, la longueur des moustaches renseigne sur la variabilité des données en dehors de la zone centrale. Plus elles sont longues, plus la variabilité est importante. Et dans tous les cas, l’examen attentif des outliers est utile car il nous permet d’en apprendre plus sur le comportement extrême de certaines observations.\n\nLorsque les boîtes ont une forme à peu près symétrique de part et d’autre de la médiane (c’est le cas pour cet exemple dans la plupart des catégories), cela signifie qu’un histogramme des mêmes données serait symétrique également\nLes stripcharts et les boxplots sont donc un bon moyen de comparer rapidement la position et la dispersion d’un grand nombre de séries de données : ici, en quelques lignes de code, nous en comparons 12 pour chacun des 3 aéroports de New York.\nLes histogrammes sont plus utiles lorsqu’il y a moins de catégories à comparer. Ils permettent en outre de mieux visualiser les distribution non symétriques, ou qui présentes plusieurs pics.\n\n\n1.5.4 Visualiser l’incertitude : les barres d’erreur\nComme évoqué plus haut, il est important de ne pas confondre dispersion et incertitude. Lorsque l’on visualise des moyennes calculées à partir des données d’un échantillon, il est important de faire apparaître des barres d’erreurs, qui correspondent en générale :\n\nsoit à l’erreur standard de la moyenne\nsoit à l’intervalle de confiance de la moyenne\n\nPuisque deux choix sont possibles, il sera important de préciser systématiquement dans la légende du graphique, la nature des barres représentées. Commençons par visualiser les températures mensuelles avec les erreurs standards. Pour cela, je reprends le tableau temperatures_se créé précédemment :\n\ntemperatures_se %>% \n  ggplot(aes(x = factor(month), y = moyenne)) +\n  geom_line(aes(group = 1)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = moyenne - erreur_standard,\n                    ymax = moyenne + erreur_standard),\n                width = 0.1) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\") +\n  theme_bw()\n\n\n\n\nFigure 1.5: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les erreurs standard\n\n\n\n\nVous remarquerez que :\n\nj’associe factor(month), et non simplement month, à l’axe des x afin d’avoir, sur l’axe des abscisses, des chiffres cohérents allant de 1 à 12, et non des chiffres à virgule\nl’argument group = 1 doit être ajouté pour que la ligne reliant les points apparaisse. En effet, les lignes sont censées relier des points qui appartiennent à une même série temporelle. Or ici, nous avons transformé month en facteur. Préciser group = 1 permet d’indiquer à geom_line() que toutes les catégories du facteur month appartiennent au même groupe, que ce facteur peut être considéré comme une variable continue, et qu’il est donc correct de relier les points.\nla fonction geom_errorbar() contient de nouvelles caractéristiques esthétiques qu’il nous faut obligatoirement renseigner : les extrémités inférieures et supérieures des barres d’erreur. Il nous faut donc associer 2 variables à ces caractéristiques esthétiques. Ici, nous utilisons moyenne - erreur_std pour la borne inférieure des barres d’erreur, et moyenne + erreur_std pour la borne supérieure. Les variables moyenne et erreur_standard faisant partie du tableau temperatures_se, geom_errorbar() les trouve sans difficulté.\nl’argument width de la fonction geom_errorbar() permet d’indiquer la longueur des segments horizontaux qui apparaissent à chaque extrémité des barres d’erreur.\n\nIci, bien que moins lisible, on peut aussi faire apparaître les trois courbes sur le même graphique, afin de mieux visualiser les similarités des fluctuations de températures entre les 3 aéroports :\n\ntemperatures_se %>% \n  ggplot(aes(x = factor(month), y = moyenne, color = origin, group = origin)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = moyenne - erreur_standard,\n                    ymax = moyenne + erreur_standard),\n                width = 0.1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\",\n       color = \"Aéroport\") +\n  theme_bw()\n\n\n\n\nFigure 1.6: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les erreurs standard\n\n\n\n\nDe la même façon, nous pouvons parfaitement faire apparaître, au lieu des erreurs standards, les intervalles de confiance à 95% de chaque valeur de température moyenne. Il nous suffit pour cela d’utiliser le tableau temperatures_ci qui contient les valeurs de moyennes et des bornes supérieures et inférieures de ces intervalles :\n\ntemperature_ci %>% \n  ggplot(aes(x = factor(month), y = moyenne, group = 1)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = ci_borne_inf, ymax = ci_borne_sup), width = 0.1) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\",\n       color = \"Aéroport\") +\n  theme_bw()\n\n\n\n\nFigure 1.7: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les intervalels de confiance à 95% des moyenes mensuelles.\n\n\n\n\nComme vous voyez, les barres d’erreurs sont maintenant plus longues que sur la Figure 1.5. C’est normal car rappelez-vous que les intervalles de confiance sont à peu près équivalents à 2 fois les erreurs standards. L’intérêt de représenter les intervalles de confiance est qu’ils sont directement liés aux tests statistiques que nous aborderons dans les chapitres suivants. Globalement, quand 2 séries de données ont des intervalles de confiance qui se chevauchent largement (comme les mois de janvier et février par exemple), alors, un test d’hypothèse conclurait presque toujours à l’absence de différence significative entre les 2 groupes. À l’inverse, quand 2 séries de données ont des intervalles de confiance qui ne se chevauchent pas du tout (comme les mois de mars et d’avril par exemple), alors, un test d’hypothèse conclurait presque toujours à l’existence d’une différence significative entre les 2 groupes. Lorsque les intervalles de confiance entre 2 catégorie se chevauchent faiblement ou partiellement (comme entre les mois de juin et juillet pour l’aéroport LGA), la situation est moins tranchée, et nous devrons nous en remettre aux résultats du test pour savoir si une différence devrait être considérée comme significative ou non.\n\n\n1.5.5 Visualiser l’incertitude : les boîtes à moustaches\nOutre les informations de position et de dispersion, les boîtes à moustaches permettent également de visualiser l’incertitude associée aux médianes. Il suffit pour cela d’ajouter l’argument notch = TRUE dans la fonction geom_boxplot() :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.5, notch = TRUE) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nDes encoches ont été ajoutées autour de la médiane de chaque boîte à moustache. Ces encoches sont des encoches d’incertitudes. Les limites inférieures et supérieures de ces encoches correspondent aux bornes inférieures et supérieures de l’intervalle de confiance à 95% des médianes. Comme pour les moyennes, le chevauchement ou l’absence de chevauchement entre les encoches de 2 séries de données nous renseigneront sur l’issue probable des futurs tests statistiques que nous devrons réaliser. Il sera donc important de bien examiner ces encoches en amont des tests statistiques pour éviter de faire/dire des bêtises…\n\n\n1.5.6 Exercice\n\nAvec le tableau penguins, calculez les grandeurs suivantes pour chaque espèce de manchot et chaque sexe :\n\n\nla moyenne de la longueur des nageoires\nla variance de la longueur des nageoires\nl’écart-type de la longueur des nageoires\nl’erreur standard de la longueur moyenne des nageoires\nla moyenne de la masse corporelle\nla variance de la masse corporelle\nl’écart-type de la masse corporelle\nl’erreur standard de la masse corporelle moyenne\n\nAttention : pensez à retirer les individus dont le sexe est inconnu.\n\nVérifiez avec la fonction skim() que les moyennes et écart-types calculés ci-dessus sont corrects.\nAvec ces données synthétiques faites le graphique suivant :\n\n\n\n\n\n\n\n\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. nycflights13: Flights that Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\n———. 2022. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, et Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, et Kirill Müller. 2022. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "02-PropTests.html",
    "href": "02-PropTests.html",
    "title": "2  Principes des tests statistiques et comparaison de proportions",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "03-OneSampleTests.html",
    "href": "03-OneSampleTests.html",
    "title": "3  Comparaison de moyennes : un échantillon et deux échantillons appariés",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "04-TwoSampleTests.html",
    "href": "04-TwoSampleTests.html",
    "title": "4  Comparaison de moyennes : deux échantillons indépendants",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "05-Cohortes.html",
    "href": "05-Cohortes.html",
    "title": "5  Analyse de cohortes",
    "section": "",
    "text": "Cette section doit permettre d’illustrer la partie du cours de Population Dynamics de l’EC “Fonctionnement des écosystèmes” consacrée à l’analyse des cohortes. Vous utiliserez les tailes corporelles d’individus échantillonnés sur le terrain à plusieurs dates pour :\n\nProduire la structure démographique instantanée de la population pour chaque date d’échantillonnage\nRéaliser la décomposition polymodale pour identifier les cohortes à chaque date d’échantillonnage\nCréer une courbe de croissance et une courbe de mortalité\nUtiliser une relation allométrique pour passer de la taille des individus à leur masse\nProduire la courbe d’Allen\n\nVous devrez en premier lieu importer les données fournies dans un fichier Excel et vous assurer qu’elles sont dans un format permettant les analyses et représentations graphiques."
  },
  {
    "objectID": "05-Cohortes.html#sec-pres",
    "href": "05-Cohortes.html#sec-pres",
    "title": "5  Analyse de cohortes",
    "section": "5.2 Présentation de l’étude",
    "text": "5.2 Présentation de l’étude\nLe suivi démographique des populations animales est extrêmement fréquent dans le domaine de l’écologie. Le suivi des populations naturelles est particulièrement pertinent dans un contexte de conservation, pour réaliser des études d’impact ou pour mesurer l’évolution de la biodiversité.\nIci, une population de gastéropodes Nassarius reticulatus (la nasse réticulée) a été suivie pendant 5 ans. Deux sessions d’échantillonnage ont été organisées chaque année depuis 2010 : la première a été réalisée en mars, juste après le recrutement des juvéniles, et la seconde a été réalisée 6 mois plus tard, en septembre. Tous les échantillons ont été collectés au même endroit, selon la même méthode (collecte systématique de tous les individus présents à l’intérieur de 10 quadrats positionnés aléatoirement dans la zone d’étude), lors d’une marée basse de fort coefficient (marées de printemps et d’automne). Les individus échantillonnés ont été mesurés sur place, de la base, à l’apex de la coquille (voir Figure 5.1) à l’aide d’un pied à coulisse (précision : centième de millimètre) et relâchés sur place.\n\n\n\n\n\nFigure 5.1: Coquille de nasse réticulée Nassarius reticulatus (Linnaeus, 1758)\n\n\nL’espèce étudiée présente les caractéristiques suivantes :\n\nLes individus ont une durée de vie de 5 ans en moyenne.\nLes plus grand individus peuvent atteindre une taille de plus de 40 millimètres.\nIl n’y a qu’une seule période de reproduction chaque année. Il n’y a donc qu’un unique recrutement chaque année, au tout début du mois de mars.\n\nDes travaux antérieurs réalisés au laboratoire ont montré que la taille et la masse des individus étaient liées par la relation allométrique suivante :\n\\[w = 0.0013 \\cdot l^{2.3}\\]\navec \\(w\\), la masse en grammes et \\(l\\) la longueur des coquilles en millimètres.\n\n\n\n\n\n\nObjectif principal\n\n\n\nL’objectif principal de cette étude est de produire la courbe d’Allen d’une cohorte de cette populations. Cette courbe sera utilisée pour déterminer des gains et des pertes de biomasses au sein de l’écosystème étudié. Les étapes nécessaires à la production de cette courbe sont présentées ci-dessous."
  },
  {
    "objectID": "05-Cohortes.html#avant-de-vous-lancer",
    "href": "05-Cohortes.html#avant-de-vous-lancer",
    "title": "5  Analyse de cohortes",
    "section": "5.3 Avant de vous lancer…",
    "text": "5.3 Avant de vous lancer…\nPour travailler dans de bonnes conditions, vous aurez absolument besoin de travailler dans un script et à l’intérieur d’un Rproject. Si vous ne savez plus comment faire, reportez-vous aux chapitrex correspondants du livre en ligne de Biométrie du semestre 3 : au sujet des scripts et au sujet des Rprojects. Vous devrez notamment (liste non exhaustive !) :\n\nCréer un nouveau script (nommez-le Nasses.R)\nTélécharger (si besoin) et charger quelques packages (voir plus bas)\nTélécharger dans votre répertoire de travail le fichier Nassarius.csv\nImporter dans RStudio les données du fichier Nassarius.csv\n\nPour cette section, vous aurez besoin des packages suivants :\n\ntidyverse : pour manipuler les données et faire des graphiques (Wickham et al. 2019)\nmixdist : pour effectuer les décompositions polymodales (Macdonald et Juan Du 2018)\n\nSi vous ne savez plus comment installer et charger des packages en mémoire, reportez-vous au chapitre correspondant du livre en ligne de Biométrie du semestre 3\n\nlibrary(tidyverse)\nlibrary(mixdist)\n\nPour importer les données, utilisez l’assistant d’importation de RStudio. Notez que :\n\nLes colonnes sont séparées par des tabulations\nLe symbole utilisé pour les décimales dans le fichier Nassarius.csv est la virgule\n\nVous devrez donc spécifier correctement ces éléments pour pouvoir importer le fichier dans le logiciel. Si vous ne savez plus comment faire, reportez-vous au chapitre correspondant du livre en ligne de Biométrie du semestre 3.\nSi l’importation s’est déroulée normalement, vous devriez maintenant disposer de l’objet nommé Nassarius suivant :\n\nNassarius\n\n# A tibble: 3,710 × 2\n    size date    \n   <dbl> <chr>   \n 1  4.8  march-10\n 2  3    march-10\n 3  5.56 march-10\n 4  3.74 march-10\n 5  4.1  march-10\n 6  2.21 march-10\n 7  2.75 march-10\n 8  3.18 march-10\n 9  2.56 march-10\n10  4.36 march-10\n# … with 3,700 more rows\n\n\nEt la commande suivante devrait produire exactement ces résultats :\n\nNassarius %>%\n  count(date)\n\n# A tibble: 10 × 2\n   date         n\n   <chr>    <int>\n 1 march-10   468\n 2 march-11   481\n 3 march-12   460\n 4 march-13   468\n 5 march-14   487\n 6 sept-10    268\n 7 sept-11    276\n 8 sept-12    265\n 9 sept-13    258\n10 sept-14    279\n\n\n\n\n%>% : le pipe est un opérateur spécial qui prend l’objet situé à gauche et le transmets à la fonction placée à droite, en guise de premier argument. P. ex. : Nassarius %>% count(date) est équivalent à count(Nassarius, date)\n count() : compte le nombre d’occurrences de chaque valeur possible (ou niveau/modalité) d’une variable catégorielle (ou facteur)."
  },
  {
    "objectID": "05-Cohortes.html#les-étapes-de-lanalyse",
    "href": "05-Cohortes.html#les-étapes-de-lanalyse",
    "title": "5  Analyse de cohortes",
    "section": "5.4 Les étapes de l’analyse",
    "text": "5.4 Les étapes de l’analyse\nPour produire une courbe d’Allen, de nombreuses étapes sont nécessaires. Vous devrez :\n\nProduire la structure démographique instantanée de la population à chaque date d’échantillonnage\nRéaliser la décomposition polymodale pour identifier les cohortes présentes dans la population à chaque date d’échantillonnage.\nDéterminer la taille moyenne de la coquille des individus de la cohorte d’intérêt, à chaque date d’échantillonnage, pour produire la courbe de croissance\nDéterminer l’abondance des individus de la cohorte d’intérêt, à chaque date d’échantillonnage, pour produire la courbe de survie\n\nJe vais présenter ci-dessous les étapes de cette procédure pour une unique date d’échantillonnage : march 2010. Vous devrez reproduire ces étapes pour les 9 autres dates d’échantillonnage."
  },
  {
    "objectID": "05-Cohortes.html#sélection-des-données-dune-date-spécifique",
    "href": "05-Cohortes.html#sélection-des-données-dune-date-spécifique",
    "title": "5  Analyse de cohortes",
    "section": "5.5 Sélection des données d’une date spécifique",
    "text": "5.5 Sélection des données d’une date spécifique\nNotre jeu de données Nassrisu contient 2 colonnes : la première contient les dates d’échantillonnage et la seconde les tailles individuelles en millimètres. La première étape consiste à créer un nouvel objet (nous le nommerons Nas_01) qui contiendra uniquement les données collectées lors de la toute première session d’échantillonnage de mars 2010. Vous devriez déjà savoir comment utiliser la fonction filter() pour le faire :\n\nNas_01 <- Nassarius %>%\n  filter(date == \"march-10\")\n\nNas_01\n\n# A tibble: 468 × 2\n    size date    \n   <dbl> <chr>   \n 1  4.8  march-10\n 2  3    march-10\n 3  5.56 march-10\n 4  3.74 march-10\n 5  4.1  march-10\n 6  2.21 march-10\n 7  2.75 march-10\n 8  3.18 march-10\n 9  2.56 march-10\n10  4.36 march-10\n# … with 458 more rows\n\n\n\n\nfilter() : permet de filtrer les lignes d’un tableau (ici Nassarius) pour ne conserver que celles qui remplissent une condition spécifiée par l’utilisateur (ici date == \"march_10\")\n\ndim(Nassarius)\n\n[1] 3710    2\n\ndim(Nas_01)\n\n[1] 468   2\n\n\n\n\ndim() : Affiche le nombre de lignes et de colonnes d’un tableau.\nComme nous pouvons le constater, on passe du tableau original Nasarius contenant 3710 lignes à un nouveau tableau Nas_01 qui en contient seulement 468."
  },
  {
    "objectID": "05-Cohortes.html#structure-démographique-instantanée",
    "href": "05-Cohortes.html#structure-démographique-instantanée",
    "title": "5  Analyse de cohortes",
    "section": "5.6 Structure démographique instantanée",
    "text": "5.6 Structure démographique instantanée\nMaintenant que nous disposons d’une table Nas_01 qui contient uniquement la taille des individus collectés en mars 2010, il nous faut visualiser la structure démographique instantanée afin de déterminer combien de cohortes étaient présentes dans la population à cette date. La structure démographique instantanée est ici simplement un histogramme présentant la distribution des tailles individuelles. Nous allons donc placer la taille des individus sur l’axe des x en guise de descripteur individuel, et sur l’axe des y, RStudio placera automatiquement l’abondance pour chaque classe de taille en guise de descripteur populationel.\n\nNas_01 %>%\n  ggplot(aes(x = size)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5.2: Structure démographique instantanée\n\n\n\n\n\n\nggplot() : crée un graphique\n aes() : associe une variable d’un jeu de données à une caractéristique esthétique d’un graphique (p. ex. la position le long de l’axe des x ou des y, la couleur, la forme, la taille, etc.)\n geom_histogram() : ajoute un objet géométrique de type histogramme à une graphique produit par ggplot()\nNotez le message d’avertissement qui s’affiche quand vous produisez cet histogramme. Il indique que R a choisi pour nous le nombre de classes de tailles. Par défaut, il crée 30 classes, mais nous indique que ce n’est certainement pas le meilleur choix. Nous allons donc devoir créer nous même manuellement les classes de tailles pour (i) identifier les cohortes présentes dans la population et (ii) fixer des classes identiques que nous utiliserons pour toutes les autres dates d’échantillonnage et qui rendront les comparaisons plus aisées.\nPuisque les individus de cette espèce peuvent atteindre une taille de 40 millimètres environ, nous allons définir des classes de tailles tous les millimètres. On peut faire ça simplement en créant un vecteur qui contient les limites des classes de tailles que l’on souhaite :\n\n# Calcul de la taille maximale observée\ntaille_max <- max(Nassarius$size, na.rm = TRUE)\ntaille_max\n\n[1] 41.33\n\n\n\n\nmax() : affiche la valeur maximale contenue dans un vecteur\n\n# Définition des limites des classes de tailles\nlimites <- 0:(taille_max + 1)\nlimites\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\n\n\n\n: : l’opérateur “deux points” permet de créer des suites d’entiers\nL’objet limites contient les limites des classes de tailles que nous utiliserons pour produire les structures démographiques instantanées, pour chaque date d’échantillonnage, et pas seulement pour mars 2010. C’est la raison pour nous avons utilisé max(Nassarius$size) +1 : cette syntaxe nous assure que nos classes de tailles recouvrent bien la taille de tous les individus échantillonnés au cours des 5 années d’études. Ainsi, nous pourrons utiliser les mêmes classes de tailles pour toutes les dates.\nMaintenant, on peut utiliser le vecteur limites comme argument de la fonction geom_histogram(). On modifie aussi les couleurs des barres de l’histogramme pour mieux visualiser les classes :\n\nNas_01 %>%\n  ggplot(aes(x = size)) +\n  geom_histogram(breaks = limites, color = \"black\", fill = \"steelblue\")\n\n\n\n\nFigure 5.3: Structure démographique instantanée utilisable pour la décomposition polymodale\n\n\n\n\nCette structure démographique instantanée (Figure 5.3) fait apparaître 5 cohortes, bien que la plus âgée soit à peine visible. Les pics, ou modes, sont situés autour de 4, 9, 17, 30 et 34 millimètres. Ces valeurs approximatives sont importantes car nous les utiliserons pour réaliser la décomposition polymodale."
  },
  {
    "objectID": "05-Cohortes.html#décomposition-polymodale",
    "href": "05-Cohortes.html#décomposition-polymodale",
    "title": "5  Analyse de cohortes",
    "section": "5.7 Décomposition polymodale",
    "text": "5.7 Décomposition polymodale\n\n5.7.1 Principe et étapes\nEffectuer une décomposition polymodale revient à tenter d’ajuster une distribution Normale à chaque cohorte de la population. Puisque nous avons identifié 5 cohortes dans la population en mars 2010, il nous faut identifier 5 distribution Normales distinctes dont la somme reproduira le plus fidèlement possible la structure démographique instantanée observée. Plusieurs étapes sont requises pour y parvenir.\nNous allons utiliser le package mixdist, que vous devez avoir déjà installé et chargé en mémoire. Ce package fournit plusieurs fonctions relativement simples d’utilisation et qui permettent d’ajuster des distributions Normales à une distribution observée (notre structure démographique instantanée).\nLa fonction du package mixdist qui nous permettra de réaliser la décomposition polymodale est la fonction mix(). Son fichier d’aide nous indique que pour fonctionner correctement, il faut lui fournir plusieurs choses :\n\n?mix()\n\n\n\n? : l’opérateur “point d’interrogation” suivi du nom d’une fonction, permet d’ouvrir le fichier d’aide de cette fonction.\nLes deux premiers arguments de la fonction mix() ne possèdent pas de valeur par défaut. Nous devons donc spécifier nous même ces deux arguments :\n\nLe premier s’appelle mixdat. Il doit obligatoirement s’agir d’un tableau de données (data.frame ou tibble) contenant 2 colonnes. La première colonne doit contenir les limites supérieures des classes de tailles de notre structure démographique instantanée. Le dernier élément de cette colonne doit être “ouvert” : il doit être fixé à \\(+\\infty\\). La seconde colonne doit contenir les abondances pour chaque classe de taille de la structure démographique instantanée. Nous allons voir juste après comment créer cet objet mixdat.\nLe second argument s’appelle mixpar. Là encore, il s’agit d’un tableau de données. Il doit contenir des valeurs approchées pour les paramètres des distributions Normales que nous souhaitons ajuster à chacune de nos 5 cohortes. Chaque distribution Normale possède 2 paramètres : la moyenne (qui correspond à la position du pic sur l’axe des x d’un histogramme) et l’écart-type (qui correspond à l’étalement de la courbe en cloche, c’est-à-dire à la dispersion des données de part et d’autre de la moyenne). Là encore, nous verrons plus bas comment créer cet objet.\n\n\n\n5.7.2 Création du premier objet\nPour créer le premier tableau qui sera utilisé comme argument mixdat de la fonction mix(), nous allons nous servir de la fonction mixgroup() du package mixdist. Elle est très simple `a utiliser puisqu’elle n’a besoin que de 2 arguments :\n\nUn vecteur de données (ici, la taille de tous les individus échantillonnés en mars 2010)\nLa liste des valeurs correspondant aux limites des classes de tailles de la structure démographique instantanée. Nous disposons déjà de cet objet puisque nous l’avons créé plus tôt : c’est le vecteur limites\n\n\nmix_01 <- mixgroup(Nas_01$size, breaks = limites)\nclass(mix_01)\n\n[1] \"mixdata\"    \"data.frame\"\n\n\nL’objet mix_01 que nous venons de créer est donc un data.frame, mais c’est aussi un objet de classe mixdata que nous utiliserons en guise de premier argument de la fonction mix(). Pour afficher son contenu, il suffit comme toujours de taper son nom :\n\nmix_01\n\n     X count\n1    1     1\n2    2     6\n3    3    39\n4    4    70\n5    5    69\n6    6    30\n7    7    19\n8    8    23\n9    9    26\n10  10    33\n11  11    19\n12  12     7\n13  13     2\n14  14     1\n15  15     4\n16  16     5\n17  17    10\n18  18    15\n19  19     7\n20  20    13\n21  21     6\n22  22     5\n23  23     2\n24  24     1\n25  25     4\n26  26     4\n27  27     6\n28  28     2\n29  29     7\n30  30     8\n31  31     3\n32  32     4\n33  33     3\n34  34     4\n35  35     3\n36  36     1\n37  37     2\n38  38     2\n39  39     2\n40  40     0\n41  41     0\n42 Inf     0\n\n\nNotez que la dernière ligne de mix_01 correspond à la catégorie [42 mm ; \\(+\\infty\\)[. Pour vérifier que nous n’avons pas fait d’erreur, on peut faire une représentation graphique de cet objet particulier avec la fonction plot() :\n\nplot(mix_01, xlab = \"Size (mm)\", ylab = \"Probability\")\n\n\n\n\nFigure 5.4: Le contenu de mix_01 correspond exactement à la structure démographique instantanée de mars 2010. La ligne bleue est le contour de la structure démographique instantanée, qui devra être approchée par la superposition de 5 courbes Normales correspondant au 5 cohortes qui composent la population en mars 2010\n\n\n\n\n\n\nplot() : fonction générique permettant de produire des graphiques sans ggplot2. La forme du résultat dépendra de la classe de l’objet utilisé comme argument.\nAu final, l’objet mix_01 est simplement la structure démographique instantanée de mars 2010, présentée dans un format qui sera compris par la fonction mix() que nous utiliserons pour effectuer la décomposition polymodale.\n\n\n5.7.3 Création du deuxième objet\nPour créer le second data.frame() dont la fonction mix() aura besoin, nous allons utiliser une autre fonction du package mixdist : la fonction mixparam(). Cette fonction prend au minimum 2 arguments :\n\nmu : un vecteur qui contient la position approximative des modes (ou pics) de chaque cohorte. Si nous avons 5 cohortes à identifier, il faudra fournir 5 valeurs pour mu. Pour mars 2010, nous utiliserons les valeurs présentées plus au, au niveau de la Figure 5.3 (4, 9, 17, 30 et 34 millimètres). Ces valeurs changeront pour chaque date d’échantillonnage et il faudra donc examiner attentivement les structures démographiques instantanées de chaque échantillonnage pour les déterminer. J’insiste sur le fait que la position des pics peut être approximative, mais qu’elle ne doit malgré tout pas être trop éloignée des vraies valeurs.\nsigma : un vecteur qui contient la valeur approchée de l’écart-type de chaque cohorte. L’écart-type d’une cohorte correspond à l’étalement des tailles de part et d’autres de la moyenne de la cohorte. C’est ce que nous avons appelé “polymorphisme” dans le cours de dynamique des populations, puisque cet étalement représente des performances de croissance variables pour des individus qui sont tous nés approximativement en même temps. Là encore, il faut fournir autant de valeurs que nous avons de cohortes (soit 5 pour mars 2010). Puisque nous n’avons pas d’idée précise d’ordre de grandeur pour ces écarts-types, nous utiliserons la valeur 1 pour chaque cohorte.\n\nAinsi, on obtient le second data.frame ainsi :\n\nparam_01 <- mixparam(mu = c(4, 9, 17, 30, 34),\n                     sigma = c(1, 1, 1, 1, 1))\nparam_01\n\n   pi mu sigma\n1 0.2  4     1\n2 0.2  9     1\n3 0.2 17     1\n4 0.2 30     1\n5 0.2 34     1\n\n\n\n\nmixparam() : fonction qui crée un data.frame dans lequel chaque ligne contient les caractéristiques approchées d’une cohorte de la population échantillonnée\n c() : fonction qui permet de créer un vecteur, donc une collection d’éléments qui sont tous du même type (ici, des valeurs numériques).\nLa première colonne de ce nouveau data.frame, nommée pi, correspond à la proportion de l’effectif total échantillonné, contenu dans chaque cohorte. Puisque nous n’avons rien spécifié, mixparam() suppose que chaque cohorte contient une proportion identiques des individus de la population, et fixe donc la proportion à 0.2 (soit 20% de l’abondance totale dans chacune des 5 cohortes supposées). Nous savons pertinemment que ces proportions sont fausses puisqu’en réalité, l’abondance au sein des cohortes décroit avec l’âge des individus en raison de la mortalité. On sait donc que la cohorte la plus jeune sera la plus abondante, et que les cohortes plus âgées seront de moins en moins abondantes. Ces proportions seront estimées automatiquement par la fonction mix() lorsque nous réaliserons la décomposition polymodale. Les autres colonnes de param_01, qui contiennent les valeurs que nous avons fournies manuellement, seront également ajustées lors de la décomposition polymodale\n\n\n5.7.4 Ajustement des lois Normales aux données\nMaintenant que nous disposons des deux objets nécessaires, nous pouvons réaliser la décomposition polymodale qui consiste, grâce à la fonction mix(), à ajuster une distribution Normale à chaque cohorte supposée de la population échantillonnés.\n\nres_01 <- mix(mix_01, param_01)\n\nWarning in mix(mix_01, param_01): The optimization process terminated because\niteration limit exceeded\n\nres_01\n\n\nParameters:\n       pi     mu sigma\n1 0.46418  3.903 1.086\n2 0.27062  8.795 1.555\n3 0.14355 18.115 2.102\n4 0.09100 28.231 2.861\n5 0.03066 35.021 2.344\n\nDistribution:\n[1] \"norm\"\n\nConstraints:\n   conpi    conmu consigma \n  \"NONE\"   \"NONE\"   \"NONE\" \n\n\n\n\nmix() : réalise la décomposition polymodale. La fonction identifie une combinaison de distributions Normales qui s’ajuste le mieux possible aux données observées compte tenu des données brutes et des caractéristiques approximatives de chaque distribution Normale fournie par l’utilisateur.\nLa fonction mix() peut produire quelques avertissements. Ignorez-les à ce stade : seuls les messages d’erreurs sont problématiques et nous y reviendrons plus tard. L’objet res_01 contient donc les résultats de la décomposition polymodale. C’est une liste de 3 éléments dont seul le premier nous intéresse.1 Il contient les valeurs ajustées pour les 3 paramètres des distributions Normales (pi, mu et sigma) pour chacune des 5 cohortes identifiées en mars 2010.1 Il s’appelle parameters, sans majuscule, même si R affiche son nom avec une majuscule. Vous pouvez le vérifier en tapant str(res_01). \nAinsi, par exemple, pour l’échantillon de mars 2010, la première ligne du tableau parameters contenu dans res_01 nous apprend les choses suivantes :\n\nres_01$parameters\n\n          pi        mu    sigma\n1 0.46417551  3.903201 1.085989\n2 0.27061611  8.795297 1.555004\n3 0.14354832 18.114948 2.101807\n4 0.09099874 28.230516 2.861214\n5 0.03066132 35.020985 2.343520\n\n\n\nLa première cohorte représente 46.42% de l’abondance totale de la population\nLa taille moyenne des individus composant cette cohorte vaut 3.9 millimètres\nL’écart-type (ou polymorphisme de taille) de cette cohorte vaut 1.1 millimètres\n\nLes distributions Normales qui ont été ajustées peuvent être visualisées grâce à la fonction plot() :\n\nplot(res_01)\n\n\n\n\nFigure 5.5: Distribution observée (histogramme bleu) et cohortes ajustées (courbes rouges). Les triangles indiquent la position du mode des cohortes et la courbe verte est la somme de toutes les distributions Normales. Idéallement, cette courbe devrait être étroitement ajustée aux données observées\n\n\n\n\nOn observe sur la Figure 5.5 et dans l’objet res_01$parameters que la qualité de l’ajustement (et donc de la décomposition polymodale) est bonne :\n\nla courbe verte représente bien les données observées : elle est bien ajustée aux contours de l’histogramme.\nl’abondance des cohortes décroit avec l’âge de la cohorte : pi décroit de la cohorte la plus jeune à la cohorte la plus âgée.\nla taille moyenne des individus augmente avec l’âge de la cohorte : mu augmente de la cohorte la plus jeune à la cohorte la plus âgée.\nle polymorphisme (ou dispersion des tailles autour de la moyenne) augmente avec l’âge de la cohorte : sigma augmente de la cohorte la plus jeune à la cohorte la plus âgée.\n\n\n\n\n\n\n\nDe l’importance du choix des valeurs initiales\n\n\n\nSi nous avions choisi d’autres valeurs approchée lors de la création de l’objet param_01 avec la fonction mixparam(), l’ajustement obtenu aurait pu être différent. La décomposition polymodale n’est pas une science exacte et il est souvent nécessaire de procéder par tâtonnements pour trouver des valeurs satisfaisantes. En particulier, les résultats que nous obtenons dépendent :\n\ndu choix des valeurs pour la fonction mixparam()\ndu choix des classes de tailles pour la structure démographique instantanée. Ici, nous avons des classes de tailles de un millimètre de large, mais nous aurions pu faire un autre choix (1,5 millimètre de large, ou 2) et nous aurions alors obtenu des résultats différents.\n\nIl est donc important de retenir que les résultats obtenus ne sont pas “justes” ou “faux” en tant que tel et qu’il n’y a pas qu’une seule “bonne réponse”. La qualité des résultats obtenus s’apprécie au regard de ce qu’on connait de l’espèce étudiée (traits d’histoire de vie, période de reproduction et de recrutement, nombre de cohorte supposées selon la date, etc.), et de ce qu’on connait du comportement “normal” des cohortes (baisse de l’abondance avec l’âge, augmentation de la taille moyenne avec l’âge, augmentation du polymorphisme avec l’âge).\n\n\n\n\n5.7.5 Et en cas de message d’erreur ?\nLorsque vous utiliserez la fonction mix(), il se peut que des messages d’erreurs et/ou des messages d’avertissement apparaissent.\n\n\n Les messages d’information et les avertissements commencent en général par le mot Warning et sont presque toujours sans conséquence. La commande a été comprise par RStudio et un résultat a été produit. Dans le cas de la fonction mix(), les avertissements indiquent que la solution trouvée (l’ajustement des loi Normales aux cohortes observées) n’est peut-être pas optimale, mais une solution a néanmoins été obtenue.\n\n\n Les messages d’erreurs commencent par Erreur ou Error et indiquent que la commande n’a pas abouti. Dans le cas de la fonction mix(), cela peut être lié à 2 choses :\n\nsoit les valeurs choisies pour la fonction mixparam() sont trop éloignées de la position des pics réels, et la fonction mix() ne parvient donc pas à trouver de solution satisfaisante\nsoit le nombre de valeurs choisies pour la fonction mixparam() n’est pas le bon. Par exemple, s’il y a 5 cohortes et qu’on ne fournit que 4 valeurs pour mu et sigma, un message d’erreur apparaîtra. De même, s’il y a 4 cohortes et qu’on fournit 5 valeurs pour mu et sigma, un message d’erreur apparaîtra. Enfin, si on ne fournit pas le même nombre de valeurs pour mu et pour sigma, un message d’erreur apparaîtra.\n\n\n\nDans les deux cas, vous devez\n\nrevenir à votre structure démographique instantanée et l’observer plus attentivement\ndéterminer des valeurs plus appropriées2 pour la fonction mixparam()\nré-exécuter le code depuis la création de l’objet param_01 et jusqu’à la décomposition polymodale avec la fonction mix()\n\n2 ajoutez ou retirez une cohorte si besoin, et choisissez des valeurs plus proches des pics observés"
  },
  {
    "objectID": "05-Cohortes.html#taille-moyenne-et-abondance-des-cohortes",
    "href": "05-Cohortes.html#taille-moyenne-et-abondance-des-cohortes",
    "title": "5  Analyse de cohortes",
    "section": "5.8 Taille moyenne et abondance des cohortes",
    "text": "5.8 Taille moyenne et abondance des cohortes\nÀ partir des résultats de la décomposition polymodale, nous devons maintenant calculer l’abondance (i.e. le nombre d’individus) de chaque cohorte. Nous devrons ensuite stocker dans un nouveau tableau :\n\nla date d’échantillonnage (mars 2010)\nla valeur d’abondance de la cohorte dont on souhaite réaliser le suivi (il s’agit de la cohorte la plus jeune en mars 2010)\nla taille moyenne des individus de la cohorte dont on souhaite réaliser le suivi3\n\n3 on sait déjà que cette taille vaut 3.9 millimètres en mars 2010.L’abondance d’une cohorte est obtenue en multipliant la valeur de pi de la cohorte d’intérêt par le nombre total d’individus échantillonnés en mars 2010. En effet, sur la Figure 5.5, la surface totale comprise entre l’axe des abscisses et la courbe verte vaut 1. Cette surface correspond au nombre total d’individus échantillonnés en mars 2010 :\n\nnrow(Nas_01)\n\n[1] 468\n\n\n\n\nnrow() : affiche le nombre de lignes d’une matrice ou d’un data.frame\nPuisque la première cohorte représente 46.42% de l’abondance totale4, tout ce dont on a besoin pour connaitre l’abondance de la cohorte la plus jeune est :4 c’est ce que nous dit la valeur de pi de la première cohorte dans le tableau res_01$parameters :\n\n\n\n\n\n\npi\nmu\nsigma\n\n\n\n\n0.4642\n3.9032\n1.0860\n\n\n0.2706\n8.7953\n1.5550\n\n\n0.1435\n18.1149\n2.1018\n\n\n0.0910\n28.2305\n2.8612\n\n\n0.0307\n35.0210\n2.3435\n\n\n\n\n\nres_01$parameters[1, 1] * nrow(Nas_01)\n\n[1] 217.2341\n\n\nEn arrondissant à l’entier le plus proche, on obtient :\n\nround(res_01$parameters[1, 1] * nrow(Nas_01), 0)\n\n[1] 217\n\n\n\n\nround() : arrondit des valeurs numériques. Le deuxième argument permet d’indiquer le nombre de décimales.\nNous savons donc maintenant que la première cohorte, la plus jeune de la population échantillonnée en mars 2010, a donc une taille moyenne de 3.9 millimètre et une abondance de 217 individus. Nous allons maintenant utiliser ces valeurs pour créer un tableau et placer le premier point des courbes de croissance, de survie et d’Allen."
  },
  {
    "objectID": "05-Cohortes.html#tableau-et-mise-en-forme-des-données",
    "href": "05-Cohortes.html#tableau-et-mise-en-forme-des-données",
    "title": "5  Analyse de cohortes",
    "section": "5.9 Tableau et mise en forme des données",
    "text": "5.9 Tableau et mise en forme des données\nNous venons de décrire ci-dessus la méthode que vous devrez appliquer pour chaque date d’échantillonnage. En suivant les mêmes étapes, vous devriez être en mesure d’obtenir la taille et l’abondance moyennes de notre cohorte d’intérêt pour les 9 dates restantes. Lorsque vous effectuerez la décomposition polymodale pour ces 9 dates, n’oubliez jamais que vous voulez suivre systématiquement la même cohorte dans le temps. Cette cohorte va progressivement évoluer vers des tailles plus importantes puisque les individus grandissent chaque mois et chaque année. Notre cohorte d’intérêt va donc progressivement se décaler vers la droite des structures démographiques instantanées, et les informations de cette cohortes ne seront donc pas systématiquement situées sur la première ligne des résultats de la décomposition polymodale.\nPour chaque date, on s’attend donc à ce que la taille moyenne des individus soit plus grande que la taille obtenue pour la date d’échantillonnage précédente. De même, l’abondance devrait diminuer au fil du temps en raison de la mortalité naturelle qui affecte tous les individus de la population.\nPour produire les 3 courbes (croissance, survie et Allen) dont nous avons besoin, nous allons créer un tibble contenant 3 colonnes :\n\nla date d’échantillonnage\nla taille moyenne des individus de la cohorte en millimètres\nl’abondance de la cohorte (nombre d’individus de la cohorte)\n\nNous pouvons utiliser la fonction tribble() pour le faire :\n\ncohort <- tribble(\n  ~date,        ~size,                  ~abundance,\n  \"2010-03-01\", res_01$parameters[1,2], res_01$parameters[1,1] * nrow(Nas_01)\n)\n\n\n\ntribble() : permet de créer un tibble ligne par ligne. Le “r” de tribble est l’abréviation de “row”.\nPour chaque nouvelle date d’échantillonnage, vous devrez compléter ce tableau en ajoutant une nouvelle ligne à l’intérieur de cette fonction tribble().\n\n\n\n\n\n\nAttention !\n\n\n\nUne erreur fréquente est de saisir les données obtenues à chaque date d’échantillonnage dans un tableau différent à chaque fois. Ça n’est pas ce qu’il faut faire ! Il faut au contraire compléter le tableau cohorte en ajoutant une nouvelle ligne au tableau existant de la façon suivante :\n\ncohort <- tribble(\n  ~date,        ~size,                  ~abundance,\n  \"2010-03-01\", res_01$parameters[1,2], res_01$parameters[1,1] * nrow(Nas_01),\n  \"2010-09-01\", ...                   , ...\n)\n\n\n\nNous devons spécifier une dernière chose afin de produire les graphiques : à ce stade, la colonne date du nouveau tableau cohort est considéré comme une variable de type character (type <chr>) :\n\ncohort\n\n# A tibble: 1 × 3\n  date        size abundance\n  <chr>      <dbl>     <dbl>\n1 2010-03-01  3.90      217.\n\n\nIl nous faut donc la transformer pour que R la reconnaisse comme étant une variable temporelle afin que les données apparaissent dans l’ordre chronologique (et non alphabétique) sur l’axe des abscisses de nos graphiques. Voilà comment procéder :\n\n# On charge le package lubridate pour travailler avec des dates\n# Ce package fait partie du tidyverse\n# Si vous l'avez installé, lubridate est disponible sur votre ordinateur\nlibrary(lubridate)\ncohort <- cohort %>%\n  mutate(date = date(date))\n\n\n\nmutate() : crée de nouvelles variables dans un tibble, ou modifie des variables existantes.\n date() : transforme des variables de type <chr> (caractères) en variable de type <date> (dates).\nOn vérifie que la variable date possède maintenant le type <date> :\n\ncohort\n\n# A tibble: 1 × 3\n  date        size abundance\n  <date>     <dbl>     <dbl>\n1 2010-03-01  3.90      217.\n\n\nÀ ce stade, nous avons tout ce qu’il nous faut pour produire les courbes de croissance, de survie et d’Allen, à l’aide de ggplot2. Évidemment, chaque courbe ne contiendra ici qu’un seul point puisque nous avons examiné pour l’instant qu’une seule date d’échantillonnage. Elles ne seront complètes que lorsque que vous aurez répété ce travail pour l’ensemble des 10 dates d’échantillonnage."
  },
  {
    "objectID": "05-Cohortes.html#courbe-de-croissance",
    "href": "05-Cohortes.html#courbe-de-croissance",
    "title": "5  Analyse de cohortes",
    "section": "5.10 Courbe de croissance",
    "text": "5.10 Courbe de croissance\nOn place les dates d’échantillonnage sur l’axe des abscisses, et la taille moyennes des individus de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = date, y = size, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Date d'échantillonnage\",\n       y = \"Longueur moyenne de la coquille (mm)\",\n       title = \"Courbe de croissance\") +\n  theme_bw()\n\n\n\ngeom_point() et geom_line() : permettent d’ajouter des points et des lignes (respectivement) sur un graphique\n labs() : permet de spécifier le titre d’un graphique et de ses axes\n theme_bw() : change l’apparence générale du graphique\n\n\n\n\n\nFigure 5.6: Courbe de croissance d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage\n\n\n\n\nL’argument group = 1 est utilisé pour indiquer que toutes les dates d’échantillonnage appartiennent à la même série temporelle, et qu’on souhaite donc relier les dates par une ligne. Le résultat sera nettement plus parlant quand vous aurez ajouté des données d’autres dates d’échantillonnage sur le graphique, afin de visualiser l’évolution de la taille moyenne des individus de la cohorte d’intérêt au fil du temps."
  },
  {
    "objectID": "05-Cohortes.html#courbe-de-survie",
    "href": "05-Cohortes.html#courbe-de-survie",
    "title": "5  Analyse de cohortes",
    "section": "5.11 Courbe de survie",
    "text": "5.11 Courbe de survie\nPour produire la courbe de survie, on procède de la même façon, mais on place les dates d’échantillonnage sur l’axe des abscisses, et l’abondance de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = date, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Date d'échantillonnage\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe de survie\") +\n  theme_bw()\n\n\n\n\nFigure 5.7: Courbe de survie d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#courbe-dallen",
    "href": "05-Cohortes.html#courbe-dallen",
    "title": "5  Analyse de cohortes",
    "section": "5.12 Courbe d’Allen",
    "text": "5.12 Courbe d’Allen\nPour produire la courbe de survie, on place la taille moyenne des individus de la cohorte sur l’axe des abscisses, et l’abondance de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = size, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Longueur moyenne de la coquille (mm)\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe d'Allen\") +\n  theme_bw()\n\n\n\n\nFigure 5.8: Courbe d’Allen d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#relation-allométrique",
    "href": "05-Cohortes.html#relation-allométrique",
    "title": "5  Analyse de cohortes",
    "section": "5.13 Relation allométrique",
    "text": "5.13 Relation allométrique\nL’un des objectifs de ce travail était de produire une courbe d’Allen pour étudier les variations de biomasses. Pour y parvenir, nous devons faire une courbe d’Allen sur laquelle figure la masse moyenne des individus de la cohorte d’intérêt (en grammes), plutôt que la taille moyenne des individus de cette cohorte (en millimètres). Pour passer des tailles en millimètres aux masses en grammes, il nous suffit d’appliquer la relation allométrique fournie dans la Section 5.2, et d’ajouter une nouvelle colonne à notre tableau grâce à la fonction mutate() :\n\ncohort_2 <- cohort %>%\n  mutate(weight = 0.0013 * size ^ 2.3)\n\ncohort_2\n\n# A tibble: 1 × 4\n  date        size abundance weight\n  <date>     <dbl>     <dbl>  <dbl>\n1 2010-03-01  3.90      217. 0.0298\n\n\nIl n’y a plus qu’à utiliser cette nouvelle variable sur l’axe des abscisses d’une courbe d’Allen :\n\ncohort_2 %>%\n  ggplot(aes(x = weight, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Masse moyenne des individus (g)\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe d'Allen\") +\n  theme_bw()\n\n\n\n\nFigure 5.9: Courbe d’Allen d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#à-vous-de-jouer",
    "href": "05-Cohortes.html#à-vous-de-jouer",
    "title": "5  Analyse de cohortes",
    "section": "5.14 À vous de jouer !",
    "text": "5.14 À vous de jouer !\nAu final, voilà comment on peut résumer les différentes étapes de ce travail.\n\n\n\nAssurez vous que vous avez bien compris chaque étape de la méthode décrite pour l’échantillonnage de mars 2010, et pourquoi on fait les choses présentées ici dans l’ordre où on les fait. À ce stade, vous devriez déjà avoir un script assez long qui devrait contenir la plupart des commandes évoquées plus haut. Vous devriez donc pouvoir reproduire ces étapes pour toutes les autres dates d’échantillonnage en faisant des copier-coller et en modifiant quelques éléments précis (dates, nom des objets, valeurs de tailles moyennes pour les cohortes, etc.). Attention aussi à adopter une structure de script la plus clair possible, afin de pouvoir corriger les éventuels bugs ou problèmes plus facilement.\nIl ne vous reste donc plus qu’à reproduire ce travail pour les 9 autres dates d’échantillonnage afin (i) de récupérer les valeurs d’abondance, de taille et de masse moyenne des individus de la cohorte d’intérêt (celle qui a été recrutée en mars 2010) et (ii) de compléter les 3 courbes que nous avons commencées plus haut.\nBon courage !\n\n\n\n\nMacdonald, Peter, et with contributions from Juan Du. 2018. mixdist: Finite Mixture Distribution Models. https://CRAN.R-project.org/package=mixdist.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. « Welcome to the tidyverse ». Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "06-DynamicSystems.html",
    "href": "06-DynamicSystems.html",
    "title": "6  Systèmes dynamiques",
    "section": "",
    "text": "Ne sera pas traité cette année…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Horst, Allison, Alison Hill, and Kristen Gorman. 2022.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://CRAN.R-project.org/package=palmerpenguins.\n\n\nMacdonald, Peter, and with contributions from Juan Du. 2018.\nMixdist: Finite Mixture Distribution Models. https://CRAN.R-project.org/package=mixdist.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in\n2013. https://github.com/hadley/nycflights13.\n\n\n———. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, and Kirill Müller. 2022.\nDplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "index.html#objectifs",
    "href": "index.html#objectifs",
    "title": "TP de Biométrie Semestre 5",
    "section": "Objectifs",
    "text": "Objectifs\nCe livre contient l’ensemble du matériel (contenus, exemples, exercices…) nécessaire à la réalisation des travaux pratiques de Biométrie de l’EC ‘Outils pour l’étude et la compréhension du vivant 4’ du semestre 5 de la licence Sciences de la Vie de La Rochelle Université.\nÀ la fin du semestre, vous devriez être capables de faire les choses suivantes dans le logiciel RStudio :\n\nExplorer des jeux de données en produisant des résumés statistiques de variables de différentes nature (numériques continues ou catégorielles) et en produisant des graphiques appropriés\nCalculer des statistiques descriptives (moyennes, médianes, quartiles, écart-types, variances, erreurs standard, intervalles de confiance, etc.) pour plusieurs sous-groupes de vos jeux de données, et les représenter sur des graphiques adaptés\nChoisir et formuler des hypothèses adaptées à la question scientifique posée (hypothèses bilatérales ou unilatérales)\nChoisir les tests statistiques permettant de répondre à une question scientifique précise selon la nature de la question posée et la nature des variables à disposition\nRéaliser les tests usuels de comparaison de proportions et de moyennes (\\(\\chi^2\\), \\(t\\) de Student à 1 ou 2 échantillons, appariés ou indépendants, etc.)\nVérifier les conditions d’application des tests, et le cas échéant, réaliser des tests non paramétriques équivalents\nInterpréter correctement les résultats des tests pour répondre aux questions scientifiques posées\nIdentifier des cohortes dans une population et en étudier les caractéristiques et l’évolution temporelle\nSimuler le comportement de populations théoriques simples suivant des modèles démographiques précis (mortalité exponentielle, croissance exponentielle, croissance logistique, système prédateur-proies de Lotka et Volterra, et systèmes de compétition à 2 ou 3 espèces…)\nSimuler, par chaînes de Markov, les successions écologiques dans un écosystème théorique"
  },
  {
    "objectID": "index.html#pré-requis",
    "href": "index.html#pré-requis",
    "title": "TP de Biométrie Semestre 5",
    "section": "Pré-requis",
    "text": "Pré-requis\nPour atteindre les objectifs fixés ici, et compte tenu du volume horaire restreint qui est consacré aux TP et TEA de Biométrie au S5, je suppose que vous possédez un certain nombre de pré-requis. En particulier, vous devriez avoir à ce stade une bonne connaissance de l’interface des logiciels R et RStudio, et vous devriez être capables :\n\nde créer un Rproject et un script d’analyse dans RStudio\nd’importer des jeux de données issus de tableurs dans RStudio\nd’effectuer des manipulations de données simples (sélectionner des variables, trier des colonnes, filtrer des lignes, créer de nouvelles variables, etc.)\nde produire des graphiques de qualité, adaptés à la fois aux variables dont vous disposez et aux questions auxquelles vous souhaitez répondre.\n\n\n\n\n\n\n\nSi ces pré-requis ne sont pas maîtrisés\n\n\n\n\nmettez-vous à niveau de toute urgence en lisant attentivement le livre en ligne de Biométrie du semestre 3\nmettez-vous en binôme avec un·e collègue qui a suivi l’EC Immersion R et RStudio en début de semestre. Ça ne vous dispensera pas de lire le livre en ligne de Biométrie S3, mais ça vous fera certainement gagner pas mal de temps."
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "TP de Biométrie Semestre 5",
    "section": "Organisation",
    "text": "Organisation\n\nVolume de travail\nLes travaux pratiques et TEA de biométrie auront lieu entre le 17 octobre et le 02 décembre 2022 :\n\nSemaine 42 (du 17 au 21 octobre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 43 (du 24 au 28 octobre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 45 (du 07 au 10 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 46 (du 14 au 18 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 47 (du 21 au 25 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 48 (du 28 novembre au 02 décembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\n\n\nTous les TP ont lieu en salle MSI 217. Tous les TEA sont à distance.\nAu total, chaque groupe aura donc 6 séances de TP et 6 séances de TEA, soit un total de 18 heures prévues dans vos emplois du temps. C’est peu pour atteindre les objectifs fixés et il y aura donc évidemment du travail personnel à fournir en dehors de ces séances. J’estime que vous devrez fournir à peu près une vingtaine d’heures de travail personnel en plus des séances prévues dans votre emploi du temps. Attention donc : pensez bien à prévoir du temps dans vos plannings car le travail personnel est essentiel pour progresser dans cette matière. J’insiste sur l’importance de faire l’effort dès maintenant : vous allez en effet avoir des enseignements qui reposent sur l’utilisation de ces logiciels jusqu’à la fin du S6 (y compris pendant vos stage et, très vraisemblablement, dans vos futurs masters également). C’est donc maintenant qu’il faut acquérir des automatismes, cela vous fera gagner énormément de temps ensuite.\n\n\nModalités d’enseignement\nPour suivre cet enseignement vous pourrez utiliser les ordinateurs de l’université, mais je ne peux que vous encourager à utiliser vos propres ordinateurs, sous Windows, Linux ou MacOS. Lors de vos futurs stages et pour rédiger vos comptes-rendus de TP, vous utiliserez le plus souvent vos propres ordinateurs, autant prendre dès maintenant de bonnes habitudes en installant les logiciels dont vous aurez besoin tout au long de votre licence. Si vous n’avez pas suivi l’EC immersion et que les logiciels R et RStudio ne sont pas encore installés sur vos ordinateurs, suivez la procédure décrite ici. Si vous ne possédez pas d’ordinateur, manifestez vous rapidement auprès de moi car des solutions existent (prêt par l’université, travail sur tablette via RStudio cloud…).\n\n\n\n\n\n\nImportant\n\n\n\nL’essentiel du contenu de cet enseignement peut être abordé en autonomie, à distance, grâce à ce livre en ligne, aux ressources mises à disposition sur Moodle et à votre ordinateur personnel. Cela signifie que la présence physique lors de ces séances de TP n’est pas obligatoire.\n\n\nPlus que des séances de TP classiques, considérez plutôt qu’il s’agit de permanences non-obligatoires : si vous pensez avoir besoin d’aide, si vous avez des points de blocage ou des questions sur le contenu de ce document ou sur les exercices demandés, alors venez poser vos questions lors des séances de TP. Vous ne serez d’ailleurs pas tenus de rester pendant 1h30 : si vous obtenez une réponse en 10 minutes et que vous préférez travailler ailleurs, vous serez libres de repartir !\nDe même, si vous n’avez pas de difficulté de compréhension, que vous n’avez pas de problème avec les exercices de ce livre en ligne ni avec les quizz Moodle, votre présence n’est pas requise. Si vous souhaitez malgré tout venir en salle de TP, pas de problème, vous y serez toujours les bienvenus.\nCe fonctionnement très souple a de nombreux avantages :\n\nvous vous organisez comme vous le souhaitez\nvous ne venez que lorsque vous en avez vraiment besoin\ncelles et ceux qui se déplacent reçoivent une aide personnalisée\nvous travaillez sur vos ordinateurs\nles effectifs étant réduits, c’est aussi plus confortable pour moi !\n\nToutefois, pour que cette organisation fonctionne, cela demande de la rigueur de votre part, en particulier sur la régularité du travail que vous devez fournir. Si la présence en salle de TP n’est pas requise, le travail demandé est bel et bien obligatoire ! Si vous venez en salle de TP sans avoir travaillé en amont, votre venue sera totalement inutile puisque vous n’aurez pas de question à poser et que vous passerez votre séance à lire et suivre ce livre en ligne, choses que vous pouvez très bien faire chez vous. Vous perdrez donc votre temps, celui de vos collègues, et le mien. De même, si vous attendez la 4e semaine pour vous y mettre, vous irez droit dans le mur. Je le répète, outre les heures de TP/TEA prévus dans vos emplois du temps, vous devez prévoir au moins 20 heures de travail personnel supplémentaire.\nJe vous laisse donc une grande liberté d’organisation. À vous d’en tirer le maximum et de faire preuve du sérieux nécessaire. Le rythme auquel vous devriez avancer est présenté dans la partie suivante intitulée “Progression conseillée”.\n\n\nUtilisation de Slack\nOutre les séances de permanence non-obligatoires, nous échangerons aussi sur l’application Slack, qui fonctionne un peu comme un “twitter privé”. Slack facilite la communication des équipes et permet de travailler ensemble. Créez-vous un compte en ligne et installez le logiciel sur votre ordinateur (il existe aussi des versions pour tablettes et smartphones). Lorsque vous aurez installé le logiciel, cliquez sur ce lien pour vous connecter à notre espace de travail commun intitulé L3 SV 22-23 / EC outils (ce lien expire régulièrement : faites moi signe s’il n’est plus valide).\nVous verrez que 3 “chaînes” sont disponibles :\n\n#général : c’est là que les questions liées à l’organisation générale du cours, des TP et TEA, des évaluations, etc. doivent être posées. Si vous ne savez pas si une séance de permanence a lieu, posez la question ici.\n#questions-rstudio : c’est ici que toutes les questions pratiques liées à l’utilisation de R et RStudio devront êtres posées. Problèmes de syntaxe, problèmes liés à l’interface, à l’installation des packages ou à l’utilisation des fonctions, à la création des graphiques, à la manipulation des tableaux… Tout ce qui concerne directement les logiciels sera traité ici. Vous êtes libres de poser des questions, de poster des captures d’écran, des morceaux de code, des messages d’erreur. Et vous êtes bien entendus vivement encouragés à vous entraider et à répondre aux questions de vos collègues. Je n’interviendrai ici que pour répondre aux questions laissées sans réponse ou si les réponses apportées sont inexactes. Le fonctionnement est celui d’un forum de discussion instantané. Vous en tirerez le plus grand bénéfice en participant et en n’ayant pas peur de poser des questions, même si elles vous paraissent idiotes. Rappelez-vous toujours que si vous vous posez une question, d’autres se la posent aussi probablement.\n#questions-stats : C’est ici que vous pourrez poser vos questions liées aux méthodes statistiques ou aux choix des modèles de dynamique des populations. Tout ce qui ne concerne pas directement l’utilisation du logiciel (comme par exemple le choix d’un test ou des hypothèses nulles et alternatives, la démarche d’analyse, la signification de tel paramètre ou estimateur, le principe de telle ou telle méthode…) peut être discuté ici. Comme pour le canal #questions-rstudio, vous êtes encouragés à vous entraider et à répondre aux questions de vos collègues.\n\nAinsi, quand vous travaillerez à vos TP ou TEA, que vous soyez installés chez vous ou en salle de TP, prenez l’habitude de garder Slack ouvert sur votre ordinateur. Même si vous n’avez pas de question à poser, votre participation active pour répondre à vos collègues est souhaitable et souhaitée. Je vous incite donc fortement à vous entraider : c’est très formateur pour celui qui explique, et celui qui rencontre une difficulté a plus de chances de comprendre si c’est quelqu’un d’autre qui lui explique plutôt que la personne qui a rédigé les instructions mal comprises.\nCe document est fait pour vous permettre d’avancer en autonomie et vous ne devriez normalement pas avoir beaucoup besoin de moi si votre lecture est attentive. L’expérience montre en effet que la plupart du temps, il suffit de lire correctement les paragraphes précédents et/ou suivants pour obtenir la réponse à ses questions. J’essaie néanmoins de rester disponible sur Slack pendant les séances de TP et de TEA de tous les groupes. Cela veut donc dire que même si votre groupe n’est pas en TP, vos questions ont des chances d’être lues et de recevoir des réponses dès que d’autres groupes sont en TP ou TEA. Vous êtes d’ailleurs encouragés à échanger sur Slack aussi pendant vos phases de travail personnel."
  },
  {
    "objectID": "index.html#progession-conseillée",
    "href": "index.html#progession-conseillée",
    "title": "TP de Biométrie Semestre 5",
    "section": "Progession conseillée",
    "text": "Progession conseillée\nSi vous avez suivi le document de prise en main de R et RStudio (lors de l’immersion ou lors d’une remise à niveau en autonomie), vous savez que pour apprendre à utiliser ces logiciels, il faut faire les choses soi-même, ne pas avoir peur des messages d’erreurs (il faut d’ailleurs apprendre à les déchiffrer pour comprendre d’où viennent les problèmes), essayer maintes fois, se tromper beaucoup, recommencer, et surtout, ne pas se décourager. J’utilise ce logiciel presque quotidiennement depuis plus de 15 ans et à chaque session de travail, je rencontre des messages d’erreur. Avec suffisamment d’habitude, on apprend à les déchiffrer, et on corrige les problèmes en quelques secondes. Ce livre est conçu pour vous faciliter la tâche, mais ne vous y trompez pas, vous rencontrerez des difficultés, et c’est normal. C’est le prix à payer pour profiter de la puissance du meilleur logiciel permettant d’analyser des données, de produire des graphiques de qualité et de réaliser toutes les statistiques dont vous aurez besoin d’ici la fin de vos études et au-delà.\nPour que cet apprentissage soit le moins problématique possible, il convient de prendre les choses dans l’ordre. C’est la raison pour laquelle les chapitres de ce livre doivent être lus dans l’ordre, et les exercices d’application faits au fur et à mesure de la lecture.\nIdéalement, voilà les étapes que vous devriez avoir franchi chaque semaine :\n\nLa première semaine (42) est consacrée l’exploration statistique des jeux de données. Avant votre seconde séance de TP, vous devriez avoir compris comment calculer et interpréter des résumés statistiques de vos jeux de données. Vous devriez en particulier être capable de calculer des estimateurs de position (moyennes, médianes, quartiles…) et de dispersion (variances, écart-types, intervalles inter-quartiles…) sur des variables numériques, et ce, pour plusieurs modalités d’une variable catégorielle ou pour chaque combinaison de modalités de plusieurs variables catégorielles (par exemple, quelles sont les moyennes et variances des longueurs de becs pour chaque espèce de manchots et chaque sexe). Vous devrez être capables de distinguer la notion de dispersion de celle de précision, et vous devrez être capables de calculer l’erreur standard de la moyenne (ou erreur type). Vous devrez en outre être capables de produire des graphiques sur lesquels apparaissent des barres d’incertitude (erreurs standards ou intervalles de confiance).\nLa deuxième semaine (43) est consacrée aux tests statistiques. Avant votre troisième séance de TP, vous devriez être capable de formuler des hypothèses nulles et alternatives pertinentes, et vous devriez connaître le concept de \\(p-\\)value. Vous devriez en outre être capables, avec des données de comptages, de réaliser des tests de comparaison de proportions, et d’en interpréter correctement les résultats.\nLa troisième semaine (45) est également consacrée aux tests d’hypothèses. Avant votre quatrième séance de TP, vous devriez être capable de comparer la moyenne d’une population à une valeur théorique, et de comparer la moyenne de 2 populations, dans le cas où vous disposez d’échantillons appariés. Dans les deux cas, vous devrez être capable de vérifier les conditions d’application des tests paramétriques, et de choisir des tests non-paramétriques équivalents si les conditions d’application ne sont pas vérifiées.\nLa quatrième semaine (46) est consacrée aux derniers tests de comparaison de moyennes. Avant votre cinquième séance de TP, vous devrez donc être capable de comparer la moyenne de deux populations lorsque les échantillons sont indépendants. Comme pour la semaine précédente, vous devrez être capable de vérifier les conditions d’application du test paramétrique, et de réaliser le tests non paramétrique équivalent le cas échéant. Enfin, vous devrez aussi être en mesure de spécifier les hypothèses alternatives unilatérales ou bilatérales pertinentes selon la question scientifique posée. Pour chaque semaine consacrée aux tests, vous devrez aussi toujours penser à examiner les données graphiquement, et par le biais des statistiques descriptives décrites lors de la première semaine\nLa cinquième semaine (47) est consacrée à la mise en pratique des notions vues dans le cours magistral de Population Dynamics (EC “Fonctionnement des Écosystèmes). Nous aborderons ici les analyses de cohorte. Avant votre dernière séance de TP, vous devriez être en mesure de réaliser l’analyse de cohorte d’une population étudiée pendant plusieurs années afin de produire les courbes de croissance, de survie et d’Allen d’une cohorte d’intérêt. Vous devrez en particulier importer et mettre en forme des données issues d’un suivi de terrain, produire les structures démographiques instantanées à chaque date d’échantillonnage, faire les décompositions polymodales afin de récupérer les informations utiles au sujet de la cohorte dont on souhaite assurer le suivi.\nLa sixième semaine (48) est consacrée à la mise en pratique des notions vues dans le cours magistral de Population Dynamics (EC “Fonctionnement des Écosystèmes). Nous aborderons ici l’étude des systèmes dynamiques. Vous devrez coder différents modèles d’évolution d’une populations (mortalité exponentielle, croissance exponentielle, croissance logistique) ou de plusieurs populations ou espèces (modèle prédateurs-proies, modèle de compétition). Ces modèles généreront des données que vous devrez représenter graphiquement. Vous devrez enfin modifier la valeur de certains paramètres de ces modèles afin de comprendre leur influence sur le comportement des systèmes dynamiques étudiés."
  },
  {
    "objectID": "index.html#évaluations",
    "href": "index.html#évaluations",
    "title": "TP de Biométrie Semestre 5",
    "section": "Évaluation(s)",
    "text": "Évaluation(s)\nL’évaluation de la partie “Biométrie” de l’EC “Outils pour l’étude et la compréhension du vivant” aura lieu dans le cadre du travail de stratégie d’échantillonnage que vous mettez en œuvre avec Pierrick Bocher. Le compte-rendu de stratégie d’échantillonnage servira donc à évaluer 3 choses :\n\nles grands principes de stratégie d’échantillonnage abordés par Pierrick\nla mise en œuvre de méthodes statistiques adaptées pour répondre aux questions scientifiques posées, telles que nous les traitons en Biométrie\nla maîtrise du logiciel RStudio pour réaliser les analyses de données pertinentes (de l’importation des données et leur mise en forme dans le logiciel, à la réalisation et l’interprétation correcte des tests statistiques appropriés, en passant par l’exploration des statistiques descriptives et la création de graphiques informatifs). Pour ce dernier volet, vous devrez rendre, en plus de votre compte-rendu, votre script d’analyse. C’est ce script qui me permettra d’évaluer votre niveau de compétence et de maîtrise de l’outil, tant sur la forme du script (lisibilité, structure, reproductibilité, etc.) que sur le fond (pertinence des analyses réalisées).\n\nPour vous aider à comprendre ce qui est attendu, je vous fournis ci-dessous la grille critériée dont je me servirai pour évaluer la forme de votre script. Je ne peux que vous encourager à lire attentivement les critères d’évaluation ci-dessous et à tenter de vous les approprier. Les séances de TP et de TEA qui viennent doivent vous permettre de vous entraîner à produire des scripts de qualité.\n\nPour ce qui est du fond (pertinence des analyses statistiques réalisées et de leurs interprétations), une autre grille critériée sera fournie ici avant la fin du semestre."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "TP de Biométrie Semestre 5",
    "section": "Licence",
    "text": "Licence\nCe livre est ligne est sous licence Creative Commons (CC BY-NC-ND 4.0)\n\n\n\n\n\nVous êtes autorisé à partager, copier, distribuer et communiquer ce matériel par tous moyens et sous tous formats, tant que les conditions suivantes sont respectées :\n\n\n Attribution : vous devez créditer ce travail (donc citer son auteur), fournir un lien vers ce livre en ligne, intégrer un lien vers la licence Creative Commons et indiquer si des modifications du contenu original ont été effectuées. Vous devez indiquer ces informations par tous les moyens raisonnables, sans toutefois suggérer que l’auteur vous soutient ou soutient la façon dont vous avez utilisé son travail.\n\n\n Pas d’Utilisation Commerciale : vous n’êtes pas autorisé à faire un usage commercial de cet ouvrage, ni de tout ou partie du matériel le composant. Cela comprend évidemment la diffusion sur des plateformes de partage telles que studocu.com qui tirent profit d’œuvres dont elles ne sont pas propriétaires, souvent à l’insu des auteurs.\n\n\n Pas de modifications : dans le cas où vous effectuez un remix, que vous transformez, ou créez à partir du matériel composant l’ouvrage original, vous n’êtes pas autorisé à distribuer ou mettre à disposition l’ouvrage modifié.\n\n\n Pas de restrictions complémentaires : vous n’êtes pas autorisé à appliquer des conditions légales ou des mesures techniques qui restreindraient légalement autrui à utiliser cet ouvrage dans les conditions décrites par la licence."
  }
]