[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "TP de Biométrie Semestre 5",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "01-EDA.html",
    "href": "01-EDA.html",
    "title": "1  Exploration statistique des données",
    "section": "",
    "text": "La première étape de toute analyse de données est l’exploration. Avant de se lancer dans des tests statistiques et des procédures complexes, et à supposer que les données dont vous disposez sont déjà dans un format approprié, il est toujours très utile de :\n\nexplorer visuellement les données dont on dispose en faisant des graphiques nombreux et variés, afin de comprendre, notamment quelle est la distribution des variables numériques, quelles sont les catégories les plus représentées pour les variables qualitatives (ou facteurs), quelles sont les relations les plus marquantes entre variables numériques et/ou catégorielles, etc. Vous avez déjà appris, au semestre 3, comment produire toutes sortes de graphiques avec le package ggplot2. Si vous avez besoin de revoir les bases, c’est là que ça se passe\nexplorer les données en calculant des indices de statistiques descriptives. Ces indices relèvent en général de 2 catégories : les indices de position (e.g. moyennes, médianes…) et les indices de dispersion (e.g. variance, écart-type, intervalle inter-quartiles…). Nous allons voir dans ce chapitre comment calculer ces indices dans plusieurs situations, notamment lorsque l’on souhaite les calculer pour plusieurs sous-groupes d’un jeu de données\n\nNous verrons également dans ce chapitre comment calculer des indices d’incertitude. Attention, il ne faut pas confondre indices de dispersion et indices d’incertitude. Nous y reviendrons plus loin.\nAfin d’explorer ces questions, nous aurons besoin des packages suivants :\n\nlibrary(tidyverse)\nlibrary(skimr)\nlibrary(palmerpenguins)\nlibrary(nycflights13)\n\nLes packages du tidyverse (Wickham 2022) permettent de manipuler facilement des tableaux de données et de réaliser des graphiques. Charger le tidyverse permet d’accéder, entre autres, aux packages readr (Wickham, Hester, et Bryan 2022), pour importer facilement des fichiers .csv au format tibble, dplyr (Wickham et al. 2023) pour manipuler des tableaux de données ou encore ggplot2 (Wickham et al. 2022) pour produire des graphiques. Le package skimr (Waring et al. 2022) permet de calculer des résumés de données très informatifs. Les packages palmerpenguins (Horst, Hill, et Gorman 2022) et nycflights13 (Wickham 2021) fournissent des jeux de données qui seront faciles à manipuler pour illustrer ce chapitre (et les suivants).\nAttention, pensez à installer ces packages avant de les charger en mémoire. Si vous ne savez plus comment faire, consultez d’urgence la section dédiée au package du livre en ligne de Biométrie du semestre 3.\nDe même, pour travailler dans de bonnes conditions, créez un nouveau dossier sur votre ordinateur, créez un Rproject et un script dans ce dossier et travaillez systématiquement dans votre script. Là encore, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire."
  },
  {
    "objectID": "01-EDA.html#créer-des-résumés-avec-les-fonctions-group_by-et-summarise",
    "href": "01-EDA.html#créer-des-résumés-avec-les-fonctions-group_by-et-summarise",
    "title": "1  Exploration statistique des données",
    "section": "1.2 Créer des résumés avec les fonctions group_by() et summarise()",
    "text": "1.2 Créer des résumés avec les fonctions group_by() et summarise()\nComme nous l’avons vu au semestre 3, le package dplyr fournit plusieurs fonctions qui portent le nom de verbes simples et qui permettent d’effectuer des manipulations simples mais qui peuvent devenir très puissantes lorsqu’on les combine. Nous avons ainsi vu les fonctions suivantes :\n\nselect() : pour sélectionner ou exclure certaines colonnes (variables) d’un tableau de données\nfilter() : pour trier des lignes d’un tableau de données selon des critères ou conditions choisis par l’utilisateur\nmutate() : pour transformer des variables existantes, ou pour créer de nouvelles colonnes dans un tableau de données\narrange() : pour trier des tableaux de données par ordre croissants ou décroissants\n\nSi vous ne savez plus comment utiliser ces fonctions, relisez le chapitre 4 du livre en ligne de Biométrie du semestre 3.\nÀ ces 4 verbes, on ajoute en général les 3 suivants :\n\nsummarise() : pour créer des résumés de données à partir des colonnes d’un tableau\ngroup_by() : pour effectuer des opérations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle)\ncount() : pour compter le nombre d’observations pour chaque niveau d’un facteur (ou modalité d’une variable catégorielle)\n\nVoyons comment on utilise ces fonctions pour calculer des indices de statistiques descriptives pour les variables du tableau penguins :\n\n# affichage du tableau\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_…¹ body_…² sex    year\n   <fct>   <fct>              <dbl>         <dbl>      <int>   <int> <fct> <int>\n 1 Adelie  Torgersen           39.1          18.7        181    3750 male   2007\n 2 Adelie  Torgersen           39.5          17.4        186    3800 fema…  2007\n 3 Adelie  Torgersen           40.3          18          195    3250 fema…  2007\n 4 Adelie  Torgersen           NA            NA           NA      NA <NA>   2007\n 5 Adelie  Torgersen           36.7          19.3        193    3450 fema…  2007\n 6 Adelie  Torgersen           39.3          20.6        190    3650 male   2007\n 7 Adelie  Torgersen           38.9          17.8        181    3625 fema…  2007\n 8 Adelie  Torgersen           39.2          19.6        195    4675 male   2007\n 9 Adelie  Torgersen           34.1          18.1        193    3475 <NA>   2007\n10 Adelie  Torgersen           42            20.2        190    4250 <NA>   2007\n# … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#   ²​body_mass_g\n\n\n\n1.2.1 Principe de la fonction summarise()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Schéma de la fonction summarise() tiré de la ‘cheatsheet’ de dplyr et tidyr\n\n\n\n\nLa Figure 1.1 ci-dessus indique comment travaille la fonction summarise() : elle prend plusieurs valeurs (potentiellement, un très grand nombre) et les réduit à une unique valeur qui les résume. Lorsque l’on applique cette démarche à plusieurs colonnes d’un tableau, on obtient un tableau qui ne contient plus qu’une unique ligne de résumé.\nLa valeur qui résume les données est choisie par l’utilisateur. Il peut s’agir par exemple d’un calcul moyenne ou de variance, il peut s’agir de calculer une somme, ou d’extraire la valeur maximale ou minimale, ou encore, il peut tout simplement s’agir de déterminer un nombre d’observations.\nAinsi, pour connaître la moyenne et l’écart-type de la longueur du bec des manchots de l’île de Palmer, il suffit d’utiliser le tableau penguins du package palmerpenguins et sa variable bill_length_mm que nous avons déjà utilisés au semestre 3 :\n\npenguins %>%\n  summarise(moyenne = mean(bill_length_mm),\n            ecart_type = sd(bill_length_mm))\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    <dbl>      <dbl>\n1      NA         NA\n\n\nLes fonctions mean() et sd() permettent de calculer une moyenne et un écart-type respectivement. Ici, les valeurs retournées sont NA car 2 individus n’ont pas été mesurés, et le tableau contient donc des valeurs manquantes :\n\npenguins %>%\n  filter(is.na(bill_length_mm))\n\n# A tibble: 2 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen             NA            NA          NA      NA <NA>   2007\n2 Gentoo  Biscoe                NA            NA          NA      NA <NA>   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\nPour obtenir les valeurs souhaitées, il faut indiquer à R d’exclure les valeurs manquantes lors des calculs de moyenne et écart-types :\n\npenguins %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 2\n  moyenne ecart_type\n    <dbl>      <dbl>\n1    43.9       5.46\n\n\nLa longueur moyenne du bec des manchots est donc de 43.9 millimètres et l’écart-type vaut 5.5 millimètres.\nLa fonction summarise() permet donc de calculer des indices statistiques variés, et sur plusieurs variables à la fois. Par exemple. pour calculer les moyennes, médianes, minima et maxima des longueurs de nageoires et de masses corporelles, on peut procéder ainsi :\n\npenguins %>% \n  summarise(moy_flip = mean(flipper_length_mm, na.rm = TRUE),\n            med_flip = median(flipper_length_mm, na.rm = TRUE),\n            min_flip = min(flipper_length_mm, na.rm = TRUE),\n            max_flip = max(flipper_length_mm, na.rm = TRUE),\n            moy_mass = mean(body_mass_g, na.rm = TRUE),\n            med_mass = median(body_mass_g, na.rm = TRUE),\n            min_mass = min(body_mass_g, na.rm = TRUE),\n            max_mass = max(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n  moy_flip med_flip min_flip max_flip moy_mass med_mass min_mass max_mass\n     <dbl>    <dbl>    <int>    <int>    <dbl>    <dbl>    <int>    <int>\n1     201.      197      172      231    4202.     4050     2700     6300\n\n\nLa fonction summarise() est donc très utile pour produire des résumés informatifs des données, mais nos exemples ne sont ici pas très pertinents puisque nous avons jusqu’ici calculé des indices sans distinguer les espèces. Si les 3 espèces de manchots ont des caractéristiques très différentes, calculer des moyennes toutes espèces confondues n’a pas de sens. Voyons maintenant comment obtenir ces même indices pour chaque espèce.\n\n\n1.2.2 Intérêt de la fonction group_by()\nLa fonction summarise() devient particulièrement puissante lorsqu’elle est combinée avec la fonction group_by() :\n\n\n\n\n\nFigure 1.2: Fonctionnement de group_by() travaillant de concert avec summarise(), tiré de la ‘cheatsheet’ de dplyr et tidyr\n\n\n\n\nComme son nom l’indique, la fonction group_by() permet de créer des sous-groupes dans un tableau, afin que le résumé des données soit calculé pour chacun des sous-groupes plutôt que sur l’ensemble du tableau. En ce sens, son fonctionnement est analogue à celui des facets de ggplot2 qui permettent de scinder les données d’un graphique en plusieurs sous-groupes.\nPour revenir à l’exemple de la longueur du bec des manchots, imaginons que nous souhaitions calculer les moyennes et les écart-types pour chacune des trois espèces. Voilà comment procéder :\n\npenguins %>%\n  group_by(species) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE))\n\n# A tibble: 3 × 3\n  species   moyenne ecart_type\n  <fct>       <dbl>      <dbl>\n1 Adelie       38.8       2.66\n2 Chinstrap    48.8       3.34\n3 Gentoo       47.5       3.08\n\n\nIci, les étapes sont les suivantes :\n\nOn prend le tableau penguins, puis\nOn groupe les données selon la variable species, puis\nOn résume les données groupées sous la forme de moyennes et d’écart-types\n\nLà où nous avions auparavant une seule valeur de moyenne et d’écart-type pour l’ensemble des individus du tableau de données, nous avons maintenant une valeur de moyenne et d’écart-type pour chaque modalité de la variable espèce. Puisque le facteur species contient 3 modalités (Adelie, Chinstrap et Gentoo), le résumé des données contient maintenant 3 lignes.\nNous pouvons aller plus loin. Ajoutons à ce résumé 2 variables supplémentaires : le nombre de mesures et l’erreur standard (notée \\(se\\)), qui peut être calculée de la façon suivante :\n\\[se \\approx \\frac{s}{\\sqrt{n}}\\]\navec \\(s\\), l’écart-type de l’échantillon et \\(n\\), la taille de l’échantillon (plus d’informations sur cette statistique très importante dans la Section 1.4). Nous allons donc calculer ici ces résumés, et nous donnerons un nom au tableau créé pour pouvoir ré-utiliser ces statistiques descriptives :\n\nstats_esp <- penguins %>%\n  group_by(species) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\nstats_esp\n\n# A tibble: 3 × 5\n  species   moyenne ecart_type nb_obs erreur_std\n  <fct>       <dbl>      <dbl>  <int>      <dbl>\n1 Adelie       38.8       2.66    152      0.216\n2 Chinstrap    48.8       3.34     68      0.405\n3 Gentoo       47.5       3.08    124      0.277\n\n\nVous constatez ici que nous avons 4 statistiques descriptives pour chaque espèce. Deux choses sont importantes à retenir ici :\n\non peut obtenir le nombre d’observations dans chaque sous-groupe d’un tableau groupé en utilisant la fonction n(). Cette fonction n’a besoin d’aucun argument : elle détermine automatiquement la taille des groupes créés par group_by().\non peut créer de nouvelles variables en utilisant le nom de variables créées auparavant. Ainsi, nous avons créé la variable erreur_std en utilisant deux variables créées au préalable : ecart-type et nb_obs\n\n\n\n1.2.3 Grouper par plus d’une variable\nJusqu’ici, nous avons groupé les données par espèce. Il est tout à fait possible de grouper les données par plus d’une variable, par exemple, par espèce et par sexe :\n\nstats_esp_sex <- penguins %>%\n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nstats_esp_sex\n\n# A tibble: 8 × 6\n# Groups:   species [3]\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  <fct>     <fct>    <dbl>      <dbl>  <int>      <dbl>\n1 Adelie    female    37.3       2.03     73      0.237\n2 Adelie    male      40.4       2.28     73      0.267\n3 Adelie    <NA>      37.8       2.80      6      1.14 \n4 Chinstrap female    46.6       3.11     34      0.533\n5 Chinstrap male      51.1       1.56     34      0.268\n6 Gentoo    female    45.6       2.05     58      0.269\n7 Gentoo    male      49.5       2.72     61      0.348\n8 Gentoo    <NA>      45.6       1.37      5      0.615\n\n\nEn plus de la variable species, la tableau stats_esp_sex contient une variable sex. Les statistiques que nous avons calculées plus tôt sont maintenant disponibles pour chaque espèce et chaque sexe. D’ailleurs, puisque le sexe de certains individus est inconnu, nous avons également des lignes pour lesquelles le sexe affiché est NA. Pour les éliminer, il suffit de retirer les lignes du tableau pour lesquelles le sexe des individus est inconnu, puis de recalculer les mêmes indices :\n\nstats_esp_sex2 <- penguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE),\n            ecart_type = sd(bill_length_mm, na.rm = TRUE),\n            nb_obs = n(),\n            erreur_std = ecart_type / sqrt(nb_obs))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\nstats_esp_sex2\n\n# A tibble: 6 × 6\n# Groups:   species [3]\n  species   sex    moyenne ecart_type nb_obs erreur_std\n  <fct>     <fct>    <dbl>      <dbl>  <int>      <dbl>\n1 Adelie    female    37.3       2.03     73      0.237\n2 Adelie    male      40.4       2.28     73      0.267\n3 Chinstrap female    46.6       3.11     34      0.533\n4 Chinstrap male      51.1       1.56     34      0.268\n5 Gentoo    female    45.6       2.05     58      0.269\n6 Gentoo    male      49.5       2.72     61      0.348\n\n\nSi vous ne comprenez pas la commande filter(!is.na(sex)), je vous encourage vivement à consulter cette section du livre en ligne de Biométrie du semestre 3.\nEnfin, lorsque nous groupons par plusieurs variables, il peut être utile de présenter les résultats sous la forme d’un tableau large (grâce à la fonction pivot_wider()) pour l’intégration dans un rapport par exemple. La fonction pivot_wider() permet de passer d’un tableau qui possède ce format :\n\npenguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE))\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 6 × 3\n# Groups:   species [3]\n  species   sex    moyenne\n  <fct>     <fct>    <dbl>\n1 Adelie    female    37.3\n2 Adelie    male      40.4\n3 Chinstrap female    46.6\n4 Chinstrap male      51.1\n5 Gentoo    female    45.6\n6 Gentoo    male      49.5\n\n\nà un tableau sous ce format :\n\npenguins %>%\n  filter(!is.na(sex)) %>% \n  group_by(species, sex) %>%\n  summarise(moyenne = mean(bill_length_mm, na.rm = TRUE)) %>% \n  pivot_wider(names_from = sex,\n              values_from = moyenne)\n\n`summarise()` has grouped output by 'species'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3 × 3\n# Groups:   species [3]\n  species   female  male\n  <fct>      <dbl> <dbl>\n1 Adelie      37.3  40.4\n2 Chinstrap   46.6  51.1\n3 Gentoo      45.6  49.5\n\n\nSous cette forme, les données ne sont plus “rangées”, c’est à dire que nous n’avons plus une observation par ligne et une variable par colonne. En effet ici, la variable sex est maintenant “étalée” dans 2 colonnes distinctes : chaque modalité modalité du facteur de départ (female et male) est utilisé en tant que titre de nouvelles colonnes, et la variable moyenne est répartie dans deux colonnes. Ce format de tableau n’est pas idéal pour les statistiques ou les représentations graphiques, mais il est plus synthétique, et donc plus facile à inclure dans un rapport ou un compte-rendu.\n\n\n1.2.4 Un raccourci pratique pour compter des effectifs\nIl est extrêmement fréquent d’avoir à grouper des données en fonction d’une variable catégorielle puis d’avoir à compter le nombre d’observations de chaque modalité avec n() :\n\npenguins %>% \n  group_by(species) %>% \n  summarise(effectif = n())\n\n# A tibble: 3 × 2\n  species   effectif\n  <fct>        <int>\n1 Adelie         152\n2 Chinstrap       68\n3 Gentoo         124\n\n\nCes deux opérations sont tellement fréquentes (regrouper puis compter) que le package dplyr nous fournit un raccourci : la fonction count().\nLe code ci-dessus est équivalent à celui-ci :\n\npenguins %>% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nNotez qu’avec la fonction count(), la colonne qui contient les comptages s’appelle toujours n par défaut. Comme avec group_by(), il est bien sûr possible d’utiliser count() avec plusieurs variables :\n\npenguins %>% \n  count(species, sex)\n\n# A tibble: 8 × 3\n  species   sex        n\n  <fct>     <fct>  <int>\n1 Adelie    female    73\n2 Adelie    male      73\n3 Adelie    <NA>       6\n4 Chinstrap female    34\n5 Chinstrap male      34\n6 Gentoo    female    58\n7 Gentoo    male      61\n8 Gentoo    <NA>       5\n\n\n\npenguins %>% \n  filter(!is.na(sex)) %>% \n  count(species, sex)\n\n# A tibble: 6 × 3\n  species   sex        n\n  <fct>     <fct>  <int>\n1 Adelie    female    73\n2 Adelie    male      73\n3 Chinstrap female    34\n4 Chinstrap male      34\n5 Gentoo    female    58\n6 Gentoo    male      61\n\n\nEt il est évidemment possible de présenter le résultats sous un format de tableau large :\n\npenguins %>% \n  filter(!is.na(sex)) %>% \n  count(species, sex) %>% \n  pivot_wider(names_from = sex,\n              values_from = n)\n\n# A tibble: 3 × 3\n  species   female  male\n  <fct>      <int> <int>\n1 Adelie        73    73\n2 Chinstrap     34    34\n3 Gentoo        58    61\n\n\n\n\n1.2.5 Exercices\n\nAvec le tableau diamonds du package ggplot2, faites un tableau indiquant combien de diamants de chaque couleur on dispose. Vous devriez obtenir le tableau suivant :\n\n\n\n# A tibble: 7 × 2\n  color     n\n  <ord> <int>\n1 D      6775\n2 E      9797\n3 F      9542\n4 G     11292\n5 H      8304\n6 I      5422\n7 J      2808\n\n\n\nExaminez le tableau weather du package nycflights13 et lisez son fichier d’aide pour comprendre à quoi correspondent les données et comment elles ont été acquises.\nÀ partir du tableau weather faites un tableau indiquant les vitesses de vents minimales, maximales et moyennes, enregistrées chaque mois dans chaque aéroport de New York. Indice : les 3 aéroports de New York sont Newark, LaGuardia Airport et John F. Kennedy, notés respectivement EWR, LGA et JFK dans la variable origin. Votre tableau devrait ressembler à ceci :\n\n\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month max_wind min_wind moy_wind\n   <chr>  <int>    <dbl>    <dbl>    <dbl>\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2   1048.         0    12.2 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# … with 26 more rows\n\n\n\nSachant que les vitesses du vent sont exprimées en miles par heure, certaines valeurs sont-elles surprenantes ? À l’aide de la fonction filter(), éliminez la ou les valeurs aberrantes. Vous devriez obtenir ce tableau :\n\n\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month max_wind min_wind moy_wind\n   <chr>  <int>    <dbl>    <dbl>    <dbl>\n 1 EWR        1     42.6        0     9.87\n 2 EWR        2     31.1        0    10.7 \n 3 EWR        3     29.9        0    11.6 \n 4 EWR        4     25.3        0     9.63\n 5 EWR        5     33.4        0     8.49\n 6 EWR        6     34.5        0     9.55\n 7 EWR        7     20.7        0     9.15\n 8 EWR        8     21.9        0     7.62\n 9 EWR        9     23.0        0     8.03\n10 EWR       10     26.5        0     8.32\n# … with 26 more rows\n\n\n\nEn utilisant les données de vitesse de vent du tableau weather, produisez le graphique suivant :\n\n\n\n\n\n\nIndications :\n\nles vitesses de vent aberrantes ont été éliminées grâce à la fonction filter()\nla fonction geom_jitter() a été utilisée avec l’argument height = 0\nla transparence des points est fixée à 0.2\n\n\nÀ votre avis :\n\n\npourquoi les points sont-ils organisés en bandes horizontales ?\npourquoi n’y a-t-il jamais de vent entre 0 et environ 3 miles à l’heure (mph) ?\nSachant qu’en divisant des mph par 1.151 on obtient des vitesses en nœuds, que nous apprend cette commande :\n\n\nsort(unique(weather$wind_speed)) / 1.151\n\n [1]   0.000000   2.999427   3.999235   4.999044   5.998853   6.998662\n [7]   7.998471   8.998280   9.998089  10.997897  11.997706  12.997515\n[13]  13.997324  14.997133  15.996942  16.996751  17.996560  18.996368\n[19]  19.996177  20.995986  21.995795  22.995604  23.995413  24.995222\n[25]  25.995030  26.994839  27.994648  28.994457  29.994266  30.994075\n[31]  31.993884  32.993692  33.993501  34.993310  36.992928 910.825873"
  },
  {
    "objectID": "01-EDA.html#créer-des-résumés-de-données-avec-des-fonctions-spécifiques",
    "href": "01-EDA.html#créer-des-résumés-de-données-avec-des-fonctions-spécifiques",
    "title": "1  Exploration statistique des données",
    "section": "1.3 Créer des résumés de données avec des fonctions spécifiques",
    "text": "1.3 Créer des résumés de données avec des fonctions spécifiques\nLes fonctions group_by() et summarise() permettent donc de calculer n’importe quel indice de statistique descriptive sur un tableau de donnée entier ou sur des modalités ou combinaisons de modalités de facteurs. Il existe par ailleurs de nombreuses fonctions, disponibles de base dans R ou dans certains packages spécifiques, qui permettent de fournir des résumés plus ou moins automatiques de tout ou partie des variables d’un jeu de données. Nous allons en décire 2 ici, mais il en existe beaucoup d’autres : à vous d’explorer les possibilités et d’utiliser les fonctions qui vous paraissent les plus pertinentes, les plus simples à utiliser ou les plus complètes.\n\n1.3.1 La fonction summary()\nLa fonction summary() permet d’obtenir des résumés de données pour tous types d’objets dans R. Selon la classe des objets que l’on transmets à summary(), la nature des résultats obtenus changera. Nous verrons ainsi au semestre 6 que cette fonction peut être utilisée pour examiner les résultats de modèles de régressions linéaires ou d’analyses de variances. Pour l’instant, nous nous intéressons à 3 situations :\n\nce que renvoie la fonction quand on lui fournit un vecteur\nce que renvoie la fonction quand on lui fournit un facteur\nce que renvoie la fonction quand on lui fournit un tableau\n\n\n1.3.1.1 Variable continue : vecteur numérique\nCommençons par fournir un vecteur numérique à la fonction summary(). Nous allons pour cela extraire les données de masses corporelles des manchots du tableau penguins :\n\npenguins$body_mass_g\n\n  [1] 3750 3800 3250   NA 3450 3650 3625 4675 3475 4250 3300 3700 3200 3800 4400\n [16] 3700 3450 4500 3325 4200 3400 3600 3800 3950 3800 3800 3550 3200 3150 3950\n [31] 3250 3900 3300 3900 3325 4150 3950 3550 3300 4650 3150 3900 3100 4400 3000\n [46] 4600 3425 2975 3450 4150 3500 4300 3450 4050 2900 3700 3550 3800 2850 3750\n [61] 3150 4400 3600 4050 2850 3950 3350 4100 3050 4450 3600 3900 3550 4150 3700\n [76] 4250 3700 3900 3550 4000 3200 4700 3800 4200 3350 3550 3800 3500 3950 3600\n [91] 3550 4300 3400 4450 3300 4300 3700 4350 2900 4100 3725 4725 3075 4250 2925\n[106] 3550 3750 3900 3175 4775 3825 4600 3200 4275 3900 4075 2900 3775 3350 3325\n[121] 3150 3500 3450 3875 3050 4000 3275 4300 3050 4000 3325 3500 3500 4475 3425\n[136] 3900 3175 3975 3400 4250 3400 3475 3050 3725 3000 3650 4250 3475 3450 3750\n[151] 3700 4000 4500 5700 4450 5700 5400 4550 4800 5200 4400 5150 4650 5550 4650\n[166] 5850 4200 5850 4150 6300 4800 5350 5700 5000 4400 5050 5000 5100 4100 5650\n[181] 4600 5550 5250 4700 5050 6050 5150 5400 4950 5250 4350 5350 3950 5700 4300\n[196] 4750 5550 4900 4200 5400 5100 5300 4850 5300 4400 5000 4900 5050 4300 5000\n[211] 4450 5550 4200 5300 4400 5650 4700 5700 4650 5800 4700 5550 4750 5000 5100\n[226] 5200 4700 5800 4600 6000 4750 5950 4625 5450 4725 5350 4750 5600 4600 5300\n[241] 4875 5550 4950 5400 4750 5650 4850 5200 4925 4875 4625 5250 4850 5600 4975\n[256] 5500 4725 5500 4700 5500 4575 5500 5000 5950 4650 5500 4375 5850 4875 6000\n[271] 4925   NA 4850 5750 5200 5400 3500 3900 3650 3525 3725 3950 3250 3750 4150\n[286] 3700 3800 3775 3700 4050 3575 4050 3300 3700 3450 4400 3600 3400 2900 3800\n[301] 3300 4150 3400 3800 3700 4550 3200 4300 3350 4100 3600 3900 3850 4800 2700\n[316] 4500 3950 3650 3550 3500 3675 4450 3400 4300 3250 3675 3325 3950 3600 4050\n[331] 3350 3450 3250 4050 3800 3525 3950 3650 3650 4000 3400 3775 4100 3775\n\n\nNous avons donc 344 valeurs de masses en grammes qui correspondent aux 344 manchots du jeu de données. La fonction summary() renvoie le résumé suivant lorsqu’on lui fournit ces valeurs :\n\nsummary(penguins$body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nNous obtenons ici 7 valeurs, qui correspondent respectivement à :\n\nla valeur minimale observée dans le vecteur. Ici, le manchot le plus léger de l’échantillon pèse donc 2700 grammes.\nla valeur du premier quartile du vecteur. Le premier quartile est la valeur qui coupe l’échantillon en 2 groupes : 25% des observations du vecteur sont inférieures au premier quartile, et 75% des observations du vecteur sont supérieures au premier quartile. Ici, 25% des manchots de l’échantillon (soit 86 individus) ont une masse inférieure à 3550 grammes, et 75% des individus de l’échantillon (soit 258 individus) ont une masse supérieure à 3550 grammes.\nla valeur de médiane du vecteur. La médiane est la valeur qui coupe l’échantillon en 2 groupes : 50% des observations du vecteur sont inférieures à la médiane, et 50% des observations du vecteur sont supérieures à la médiane. Ici, 50% des manchots de l’échantillon (soit 172 individus) ont une masse inférieure à 4050 grammes, et 50% des individus de l’échantillon (soit 172 individus) ont une masse supérieure à 4050 grammes.\nla moyenne du vecteur. Ici, les manchots des 3 espèces du jeu de données ont en moyenne une masse 4202 grammes.\nla valeur du troisième quartile du vecteur. Le troisième quartile est la valeur qui coupe l’échantillon en 2 groupes : 75% des observations du vecteur sont inférieures au troisième quartile, et 25% des observations du vecteur sont supérieures au troisième quartile. Ici, 75% des manchots de l’échantillon (soit 258 individus) ont une masse inférieure à 4700 grammes, et 25% des individus de l’échantillon (soit 86 individus) ont une masse supérieure à 4750 grammes.\nla valeur maximale observée dans le vecteur. Ici, le manchot le plus lourd de l’échantillon pèse donc 6300 grammes.\nle nombre de données manquantes. Ici, 2 manchots n’ont pas été pesés et présentent donc la mention NA (comme Not Available) pour la variable body_mass_g.\n\nCes différents indices statistiques nous renseignent donc à la fois sur la position de la distribution et sur la dispersion des données.\n\nLa position correspond à la tendance centrale et indique quelles sont les valeurs qui caractérisent le plus grand nombre d’individus. La moyenne et la médiane sont les deux indices de position les plus fréquemment utilisés. Lorsqu’une variable a une distribution parfaitement symétrique, la moyenne et la médiane sont strictement égales. Mais lorsqu’une distribution est asymétrique, la moyenne et la médiane diffèrent. En particulier, la moyenne est beaucoup plus sensible aux valeurs extrêmes que la médiane. Cela signifie que quand une distribution est très asymétrique, la médiane est souvent une meilleure indication des valeurs les plus fréquemment observées.\n\n\n\n\n\n\nFigure 1.3: Distribution des masses corporelles des manchots\n\n\n\n\nL’histogramme de la Figure 1.3 montre la distribution de la taille des manchots (toutes espèces confondues). Cette distribution présente une asymétrie à droite. Cela signifie que la distribution n’est pas symétrique et que la “queue de distribution” est plus longue à droite qu’à gauche. La plupart des individus ont une masse comprise entre 3500 et 3700 grammes, au niveau du pic principal du graphique. La médiane, en orange et qui vaut 4050 grammes est plus proche du pic que la moyenne, en rouge, qui vaut 4202 grammes. Ici, la différence entre moyenne et médiane n’est pas énorme, mais elle peut le devenir si la distribution est vraiment très asymétrique, par exemple, si quelques individus seulement avaient une masse supérieure à 7000 grammes, la moyenne serait tirée vers la droite du graphique alors que la médiane ne serait presque pas affectée. la moyenne représenterait alors encore moins fidèlement la tendance centrale.\nSi l’on revient à la fonction summary(), observer des valeurs proches pour la moyenne et la médiane nous indique donc le degré de symétrie de la distribution.\n\nLa dispersion des données nous renseigne sur la dispersion des points autour des indices de position. Les quartiles et les valeurs minimales et maximales renvoyées par la fonction summary() nous renseigne sur l’étalement des points. Les valeurs situées entre le premier et le troisième quartile correspondent aux 50% des valeurs de l’échantillon les plus centrales. Plus l’étendue entre ces quartiles (notée IQR pour “intervalle interquartile”) sera grande, plus la dispersion sera importante. D’ailleurs, lorsque la dispersion est très importante, les moyennes et médianes ne renseignent que très moyennement sur le tendance centrale. Les indices de position sont surtout pertinents lorsque la dispersion des points autour de cette tendance centrale n’est pas trop large. Par exemple, si la distribution des données ressemblait à ceci (Figure 1.4), la moyenne et la médiane seraient fort peu utiles car très éloignées de la plupart des observations :\n\n\n\n\n\n\nFigure 1.4: Distribution des masses corporelles (données fictives)\n\n\n\n\nOn comprend donc l’importance de considérer les indices de dispersion en plus des indices de position pour caractériser et comprendre une série de données numériques. L’intervalle interquartile est toujours utile pour connaître l’étendue des données qui correspond aux 50% des observations les plus centrales. Les autres indices de dispersion très fréquemment utilisés, mais qui ne sont pas proposés par défaut par la fonction summary(), sont la variance et l’écart-type. Il est possible de calculer tous les indices renvoyés par la fonction summary() et ceux qui nous manquent grâce à la fonction summarise() :\n\npenguins %>% \n  summarise(min = min(body_mass_g, na.rm = TRUE),\n            Q1 = quantile(body_mass_g, 0.25, na.rm = TRUE),\n            med = median(body_mass_g, na.rm = TRUE),\n            moy = mean(body_mass_g, na.rm = TRUE),\n            Q3 = quantile(body_mass_g, 0.75, na.rm = TRUE),\n            max = max(body_mass_g, na.rm = TRUE),\n            var = var(body_mass_g, na.rm = TRUE),\n            et = sd(body_mass_g, na.rm = TRUE))\n\n# A tibble: 1 × 8\n    min    Q1   med   moy    Q3   max     var    et\n  <int> <dbl> <dbl> <dbl> <dbl> <int>   <dbl> <dbl>\n1  2700  3550  4050 4202.  4750  6300 643131.  802.\n\n\nVous notez que le code est beaucoup plus long, et qu’utiliser summary() peut donc faire gagner beaucoup de temps, même si cette fonction ne nous fournit ni la variance ni l’écart-type. Mais comme souvent dans R, il est possible de calculer à la main toutes ces valeurs si besoin. Les fonctions suivantes pourront donc vous être utiles :\n\nmean() permet de calculer la moyenne.\nmedian() permet de calculer la médiane.\nmin() et max() permettent de calculer les valeurs minimales et maximales respectivement.\nquantile() permet de calculer les quartiles.\nsd() permet de calculer l’écart-type.\nvar() permet de calculer la variance.\n\nPour toutes ces fonctions l’argument na.rm = TRUE permet d’obtenir les résultats même si certaines valeurs sont manquantes. Enfin, la fonction IQR() permet de calculer l’intervalle inter-quartiles :\n\nIQR(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 1200\n\n\nIci, les 50% des valeurs les plus centrales de l’échantillon sont situées dans un intervalle de 1200 grammes autour de la médiane.\n\n\n1.3.1.2 Variable quantitative : facteur\nSi l’on fournit une variable catégorielle ou facteur à summary(), le résultat obtenu sera naturellement différent : calculer des moyennes, médianes ou quartiles n’aurait en effet pas de sens lorsque la variable fournie ne contient que des catégories :\n\nsummary(penguins$species)\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nPour les facteurs, summary() compte simplement le nombre d’observations pour chaque modalité. Ici, la variable species est un facteur qui compte 3 modalités. La fonction summary() nous indique donc le nombre d’individus pour chaque modalité : notre jeu de données se compose de 152 individus de l’espèce Adélie, 68 individus de l’espèce Chinstrap, et 124 individus de l’espèce Gentoo.\nComme pour les vecteurs numériques, si le facteur présente des données manquantes, la fonction summary() compte également leur nombre :\n\nsummary(penguins$sex)\n\nfemale   male   NA's \n   165    168     11 \n\n\nPour les facteurs, la fonction summary() est donc tout à fait équivalente à la fonction count() :\n\npenguins %>% \n  count(species)\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nL’avantage de la fonction count() est qu’il est possible d’utiliser plusieurs facteurs pour compter le nombre d’observations de toutes les combinaisons de modalités (par exemple, combien d’individus de chaque sexe pour chaque espèce), ce qui n’est pas possible avec la fonction summary().\n\n\n1.3.1.3 Les tableaux : data.frame ou tibble\nL’avantage de la fonction summary() par rapport à la fonction count() apparaît lorsque l’on souhaite obtenir des informations sur toutes les variables d’un tableau à la fois :\n\nsummary(penguins)\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n\n\nIci, on obtient un résumé pour chaque colonne du tableau. Les colonnes numériques sont traitées comme les vecteurs numériques (on obtient alors les minimas et maximas, les quartiles, les moyennes et médianes) et les colonnes contenant des variables catégorielles sont traitées comme des facteurs (et on obtient alors le nombre d’observations pour chaque modalité).\nOn constate au passage que la variable year est considérée ici comme une variable numérique, alors qu’elle devrait plutôt être considérée comme un facteur, ce qui nous permettrait de savoir combien d’individus ont été échantillonnés chaque année :\n\npenguins %>% \n  mutate(year = factor(year)) %>% \n  summary()\n\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex        year    \n Min.   :172.0     Min.   :2700   female:165   2007:110  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   2008:114  \n Median :197.0     Median :4050   NA's  : 11   2009:120  \n Mean   :200.9     Mean   :4202                          \n 3rd Qu.:213.0     3rd Qu.:4750                          \n Max.   :231.0     Max.   :6300                          \n NA's   :2         NA's   :2                             \n\n\nAu final, la fonction summary() est très utile dans certaines situations, notamment pour avoir rapidement accès à des statistiques descriptives simples sur toutes les colonnes d’un tableau. Elle reste cependant limitée car d’une part, elle ne fournit pas les variances ou les écarts-types pour les variables numériques, et il est impossible d’avoir des résumés plus fins, pour chaque modalité d’un facteur par exemple. Ici, il serait en effet intéressant d’avoir des informations synthétiques concernant les mesures biométriques des manchots, espèce par espèce, plutôt que toutes espèces confondues. C’est là que la fonction skim() intervient.\n\n\n\n1.3.2 La fonction skim()\nLa fonction skim() fait partie du package skimr. Avant de pouvoir l’utiliser, pensez donc à l’installer et à le charger en mémoire si ce n’est pas déjà fait. Comme pour la fonction summary(), on peut utiliser la fonction skim() sur plusieurs types d’objets. Nous nous contenterons d’examiner ici le cas le plus fréquent : celui des tableaux, groupés avec group_by() ou non.\n\n1.3.2.1 Tableau non groupé\nCommençons par examiner le résultat avec le tableau penguins non groupé :\n\nskim(penguins)\n\n\nData summary\n\n\nName\npenguins\n\n\nNumber of rows\n344\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n3\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nspecies\n0\n1.00\nFALSE\n3\nAde: 152, Gen: 124, Chi: 68\n\n\nisland\n0\n1.00\nFALSE\n3\nBis: 168, Dre: 124, Tor: 52\n\n\nsex\n11\n0.97\nFALSE\n2\nmal: 168, fem: 165\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\n2\n0.99\n43.92\n5.46\n32.1\n39.23\n44.45\n48.5\n59.6\n▃▇▇▆▁\n\n\nbill_depth_mm\n2\n0.99\n17.15\n1.97\n13.1\n15.60\n17.30\n18.7\n21.5\n▅▅▇▇▂\n\n\nflipper_length_mm\n2\n0.99\n200.92\n14.06\n172.0\n190.00\n197.00\n213.0\n231.0\n▂▇▃▅▂\n\n\nbody_mass_g\n2\n0.99\n4201.75\n801.95\n2700.0\n3550.00\n4050.00\n4750.0\n6300.0\n▃▇▆▃▂\n\n\nyear\n0\n1.00\n2008.03\n0.82\n2007.0\n2007.00\n2008.00\n2009.0\n2009.0\n▇▁▇▁▇\n\n\n\n\n\nLes résultats obtenus grâce à cette fonction sont nombreux. La première section nous donne des informations sur le tableau :\n\nson nom, son nombre de lignes et de colonnes\nla nature des variables qu’il contient (ici 3 facteurs et 5 variables numériques)\nla présence de variables utilisées pour faire des regroupements (il n’y en a pas encore à ce stade)\n\nEnsuite, un bloc apporte des information sur chaque facteur présent dans le tableau :\n\nle nom de la variable catégorielle (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\ndes informations sur le nombre de modalités (n_unique)\nle nombre d’observations pour les modalités les plus représentées (top_counts)\n\nEn un coup d’œil, on sait donc que 3 espèces sont présentes (et on connait leurs effectifs), on sait que les manchots ont été échantillonnées sur 3 îles, et on sait que le sexe de 11 individu (sur 344) est inconnu. Pour le reste, il y a presque autant de femelles que de mâles.\nLe dernier bloc renseigne sur les variables numériques. Pour chaque d’elle, on a :\n\nle nom de la variable numérique (skim_variable)\nle nombre de données manquantes (n_missing) et le taux de “données complètes” (complete_rate)\nla moyenne (mean) et l’écart-type (sd), ce qui est une nouveauté par rapport à la fonction summary()\nles valeurs minimales (p0), de premier quartile (p25), de second quartile (p50, c’est la médiane !), de troisième quartile (p75) et la valeur maximale (p100)\nun histogramme très simple qui donne un premier aperçu grossier de la forme de la distribution\n\nLà encore, en un coup d’œil, on dispose donc de toutes les informations pertinentes pour juger de la distribution, de la position et de la dispersion de chaque variable numérique du jeu de données.\n\n\n1.3.2.2 Tableau groupé\nLa fonction skim(), déjà très pratique, le devient encore plus lorsque l’on choisit de lui fournir seulement certaines variables, et qu’on fait certains regroupements. Par exemple, on peut sélectionner les variables relatives aux dimensions du bec (bill_length_mm et bill_depth_mm) avec la fonction select() que nous connaissons déjà, et demander un résumé des données pour chaque espèce grâce à la fonction group_by() que nous connaissons également :\n\npenguins %>%                     # Avec le tableau penguins...\n  select(species, \n         bill_length_mm,\n         bill_depth_mm) %>%      # Je sélectionne les variables d'intérêt...\n  group_by(species) %>%          # Je regroupe par espèce...\n  skim()                         # Et je produis un résumé des données\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n344\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nspecies\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nspecies\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nbill_length_mm\nAdelie\n1\n0.99\n38.79\n2.66\n32.1\n36.75\n38.80\n40.75\n46.0\n▁▆▇▆▁\n\n\nbill_length_mm\nChinstrap\n0\n1.00\n48.83\n3.34\n40.9\n46.35\n49.55\n51.08\n58.0\n▂▇▇▅▁\n\n\nbill_length_mm\nGentoo\n1\n0.99\n47.50\n3.08\n40.9\n45.30\n47.30\n49.55\n59.6\n▃▇▆▁▁\n\n\nbill_depth_mm\nAdelie\n1\n0.99\n18.35\n1.22\n15.5\n17.50\n18.40\n19.00\n21.5\n▂▆▇▃▁\n\n\nbill_depth_mm\nChinstrap\n0\n1.00\n18.42\n1.14\n16.4\n17.50\n18.45\n19.40\n20.8\n▅▇▇▆▂\n\n\nbill_depth_mm\nGentoo\n1\n0.99\n14.98\n0.98\n13.1\n14.20\n15.00\n15.70\n17.3\n▅▇▇▆▂\n\n\n\n\n\nOn constate ici que pour chaque variable numérique sélectionnée, des statistiques descriptives détaillées nous sont fournies pour chacune des 3 espèces. Ce premier examen semble montrer que :\n\nL’espèce Adélie est celle qui possède le bec le plus court (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces).\nL’espèce Gentoo est celle qui possède le bec le plus fin, ou le moins épais (ses valeurs de moyennes, médianes et quartiles sont plus faibles que celles des 2 autres espèces)\nIl ne semble pas y avoir de fortes différences d’écarts-types (donc des dispersions des points autour des moyennes) entre les 3 espèces : pour chacune des 2 variables numériques, des valeurs d’écarts-types comparables sont en effet observées pour les 3 espèces\nLa distribution des 2 variables numériques semble approximativement suivre une distribution symétrique pour les 3 espèces, avec une forme de courbe en cloche. Les distributions devraient donc suivre à peu une distribution normale\n\n\n\n\n\n\n\nNote\n\n\n\nVous comprenez j’espère l’importance d’examiner ce genre de résumé des données avant de vous lancer dans des tests statistiques. Ils sont un complément indispensable aux explorations graphiques que vous devez également prendre l’habitude de réaliser pour mieux appréhender et comprendre la nature de vos données. Puisque chaque je de données est unique, vous devrez vous adapter à la situation et aux questions scientifiques qui vous sont posées (ou que vous vous posez !) : les choix qui seront pertinents pour une situation ne le seront pas nécessairement pour une autre. Mais dans tous les cas, pour savoir où vous allez et pour ne pas faire de bêtise au moment des tests statistiques et de leur interprétation, vous devrez toujours explorer vos données, avec des graphiques exploratoire et des statistiques descriptives.\n\n\n\n\n\n1.3.3 Exercice\nEn utilisant les fonctions de résumé abordées jusqu’ici et le tableau weather, répondez aux questions suivante :\n\nDans quel aéroport de New York les précipitations moyennes ont-elle été les plus fortes en 2013 ?\nDans quel aéroport de New York la vitesse du vent moyenne était-elle la plus forte en 2013 ? Quelle est cette vitesse ?\nDans quel aéroport de New York les rafales de vent étaient-elles les plus variables en 2013 ? Quel indice statistique vous donne cette information et quelle est sa valeur ?\nLes précipitation dans les 3 aéroports de New-York ont-elles une distribution symétrique ?\nQuelle est la température médiane observée en 2013 tous aéroports confondus ?\nTous aéroports confondus, quel est le mois de l’année où la température a été la plus variable en 2013 ? Quelles étaient les températures minimales et maximales observées ce mois-là ?"
  },
  {
    "objectID": "01-EDA.html#sec-disp",
    "href": "01-EDA.html#sec-disp",
    "title": "1  Exploration statistique des données",
    "section": "1.4 Dispersion et incertitude",
    "text": "1.4 Dispersion et incertitude\n\n1.4.1 La notion de dispersion\nComme expliqué plus haut, les indices de dispersion nous renseignent sur la variabilités des données autour de la valeur moyenne (ou médiane) d’une population ou d’un échantillon. L’écart-type, la variance et l’intervalle inter-quartiles sont 3 exemples d’indices de dispersion. Prenons l’exemple de l’écart-type. Un écart-type faible indique que la majorité des observations ont des valeurs proches de la moyenne. À l’inverse, un écart-type important indique que la plupart des points sont éloignés de la moyenne. L’écart-type est une caractéristique de la population que l’on étudie grâce à un échantillon, au même titre que la moyenne. En travaillant sur un échantillon, j’espère accéder aux vraies grandeurs de la population. Même si ces vraies grandeurs sont à jamais inaccessibles (on ne connaîtra jamais parfaitement quelle est la vraie valeur de moyenne \\(\\mu\\) ou d’écart-type \\(\\sigma\\) de la population), on espère qu’avec un échantillonnage réalisé correctement, la moyenne de l’échantillon (\\(\\bar{x}\\) ou \\(m\\)) et l’écart-type (\\(s\\)) de l’échantillon reflètent assez fidèlement les valeurs de la population générale. C’est la notion d’estimateur, intimement liée à la notion d’inférence statistique : la moyenne de l’échantillon est un estimateur de la moyenne \\(\\mu\\) de la population. C’est la raison pour laquelle on la note parfois \\(\\hat{\\mu}\\) (en plus de \\(\\bar{x}\\) ou \\(m\\)). De même, l’écart-type \\(s\\) et la variance \\(s^2\\) d’un échantillon sont des estimateurs de l’écart-type \\(\\sigma\\) et de la variance \\(\\sigma^2\\) de la population générale. C’est la raison pour laquelle on les note parfois \\(\\hat{\\sigma}\\) et \\(\\hat{\\sigma}^2\\) respectivement. L’accent circonflexe se prononce “chapeau”. On dit donc que \\(\\hat{\\sigma}\\) (sigma chapeau) est un estimateur de l´écart-type de la population générale. Comme nous l’avons vu, les indices de dispersion doivent accompagner les indices de position lorsque l’on décrit des données, car présenter une valeur de moyenne, ou de médiane seule n’a pas de sens : il faut savoir à quel point les données sont proches ou éloignées de la tendance centrale pour savoir si, dans la population générale, les indicateurs de position correspondent ou non, aux valeurs portées par la majorité des individus.\nNous avons vu plus haut comment calculer des indices de position et de dispersion. Tout ceci devrait donc être clair pour vous à ce stade.\n\n\n1.4.2 La notion d’incertitude\nPar ailleurs, puisqu’on ne sait jamais avec certitude si nos estimations (de moyennes ou d’écarts-types ou de tout autre paramètre) reflètent fidèlement ou non les vraies valeurs de la population, nous devons quantifier à quel point nos estimations s’écartent de celles de la population générale. C’est ce que permettent les indices d’incertitude. Les deux indices d’incertitude les plus connus (et les plus utilisés) sont l’intervalle de confiance à 95% (de la moyenne ou de tout autre estimateur ; les formules sont nombreuses et il n’est pas utile de les détailler ici : nous verrons comment les calculer plus bas) et l’erreur standard de la moyenne (\\(se_{\\bar{x}}\\)), dont la formule est la suivante :\n\\[se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\]\navec \\(s\\), l’écart-type de l’échantillon et \\(n\\) la taille de l’échantillon.\nComme pour la moyenne, on peut calculer l’erreur standard d’un écart-type, d’une médiane, d’une proportion, ou de tout autre estimateur calculé sur un échantillon. Cet indice d’incertitude ne nous renseigne pas sur une grandeur de la population générale qu’on chercherait à estimer, mais bien sur l’incertitude associée à une estimation que nous faisons en travaillant sur un échantillon de taille forcément limitée. Tout processus d’échantillonnage est forcément entaché d’incertitude, causée entre autre par le hasard de l’échantillonnage (ou fluctuation d’échantillonnage). Puisque nous travaillons sur des échantillons forcément imparfaits, les indices d’incertitude vont nous permettre de quantifier à quel point nos estimations s’écartent des vraies valeurs de la population. Ces “vraies valeurs”, faute de pouvoir collecter tous les individus de la population, resteront à jamais inconnues.\n\n\n\n\n\n\nAutrement dit…\n\n\n\nQuand on étudie des populations naturelles grâce à des échantillons on se trompe toujours. Les statistiques nous permettent de quantifier à quel point on se trompe grâce aux indices d’incertitude, et c’est déjà pas mal !\n\n\nEn examinant la formule de l’erreur standard de la moyenne présentée ci-dessus, on comprend intuitivement que plus la taille de l’échantillon (\\(n\\)) augmente, plus l’erreur standard (donc l’incertitude) associée à notre estimation de moyenne diminue. Autrement dit, plus les données sont abondantes dans l’échantillon, meilleure sera notre estimation de moyenne, et donc, moins le risque de raconter des bêtises sera grand.\nL’autre indice d’incertitude très fréquemment utilisé est l’intervalle de confiance à 95% (de la moyenne, de la médiane, de la variance, ou de toute autre estimateur calculé dans un échantillon). L’intervalle de confiance nous renseigne sur la gamme des valeurs les plus probables pour un paramètre de la population étudiée. Par exemple, si j’observe, dans un échantillon, une moyenne de 10, avec un intervalle de confiance calculé de [7 ; 15], cela signifie que, dans la population générale, la vraie valeur de moyenne a de bonnes chances de se trouver dans l’intervalle [7 ; 15]. Dans la population générale, toutes les valeurs comprises entre 7 et 15 sont vraisemblables pour la moyenne alors que les valeurs situées en dehors de cet intervalle sont moins probables. Une autre façon de comprendre l’intervalle de confiance est de dire que si je récupère un grand nombre d’échantillons dans la même population, en utilisant exactement le même protocole expérimental, 95% des échantillons que je vais récupérer auront une moyenne située à l’intérieur de l’intervalle de confiance à 95%, et 5% des échantillons auront une moyenne située à l’extérieur de l’intervalle de confiance à 95%. C’est une notion qui n’est pas si évidente que ça à comprendre, donc prenez bien le temps de relire cette section si besoin, et de poser des questions le cas échéant.\nConcrètement, plus l’intervalle de confiance est large, moins notre confiance est grande. Si, pour une moyenne d’échantillon de 10, l’intervalle de confiance à 95% vaut [9,5 ; 11], la gamme des valeurs probables pour la moyenne est étroite. Autrement dit, la moyenne de l’échantillon, qui vaut 10, a de bonne chances d’être très proche de la vraie valeur de moyenne de la population générale.\nLa notion d’intervalle de confiance à 95% est donc très proche de celle d’erreur standard. D’ailleurs, pour la plupart des grandeurs d’un échantillon, l’intervalle de confiance est très souvent calculé à partir de l’erreur standard.\n\n\n1.4.3 Calcul de l’erreur standard de la moyenne\nContrairement aux indices de position et de dispersion, il n’existe pas de fonction intégrée à R qui permette de calculer l’erreur standard de la moyenne. Toutefois, sa formule très simple nous permet de la calculer à la main quand on en a besoin grâce aux fonction group_by() et summarise().\nPar exemple, reprenons les données de température (tableau weather, colonne temp) dans les 3 aéroports de New York (colonne origin). Imaginons que nous souhaitions étudier les fluctuations de températures au fil des mois de l’année 2013 :\n\nJe vais commencer par transformer les températures (fournies en degrés Fahrenheit) en degrés Celsius :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8)\n\n# A tibble: 26,115 × 16\n   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed wind_g…¹\n   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>    <dbl>\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4        NA\n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06       NA\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5        NA\n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7        NA\n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7        NA\n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5        NA\n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240      15.0        NA\n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250      10.4        NA\n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260      15.0        NA\n10 EWR     2013     1     1    10  41    28.0  59.6      260      13.8        NA\n# … with 26,105 more rows, 5 more variables: precip <dbl>, pressure <dbl>,\n#   visib <dbl>, time_hour <dttm>, temp_celsius <dbl>, and abbreviated variable\n#   name ¹​wind_gust\n\n\n\nEnsuite, je détermine, pour chaque jour de chaque mois de l’année, et pour chaque aéroport, quelle est la température maximale atteinte :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 1,092 × 4\n# Groups:   origin, month [36]\n   origin month   day temperature_max\n   <chr>  <int> <int>           <dbl>\n 1 EWR        1     1            5   \n 2 EWR        1     2            1.10\n 3 EWR        1     3            1.10\n 4 EWR        1     4            4.4 \n 5 EWR        1     5            6.7 \n 6 EWR        1     6            8.9 \n 7 EWR        1     7            8.3 \n 8 EWR        1     8            9.4 \n 9 EWR        1     9           10   \n10 EWR        1    10           10   \n# … with 1,082 more rows\n\n\n\nJe peux maintenant calculer la température moyenne mensuelle pour chaque aéroport :\n\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 3\n# Groups:   origin [3]\n   origin month moyenne\n   <chr>  <int>   <dbl>\n 1 EWR        1    5.69\n 2 EWR        2    4.74\n 3 EWR        3    8.20\n 4 EWR        4   16.4 \n 5 EWR        5   22.2 \n 6 EWR        6   27.4 \n 7 EWR        7   31.0 \n 8 EWR        8   27.8 \n 9 EWR        9   24.6 \n10 EWR       10   20.0 \n# … with 26 more rows\n\n\nPour pouvoir réutiliser ce tableau, je lui donne un nom :\n\ntemperatures <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\nAu final, je peux faire un graphique de l’évolution de ces températures :\n\ntemperatures %>% \n  ggplot(aes(x = factor(month), y = moyenne)) +\n  geom_line(aes(group = 1)) +\n  geom_point() +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\") +\n  theme_bw()\n\n\n\n\nVous remarquerez que :\n\nj’associe factor(month), et non simplement month, à l’axe des x afin d’avoir, sur l’axe des abscisses, des chiffres cohérents allant de 1 à 12, et non des chiffres à virgule\nl’argument group = 1 doit être ajouté pour que la ligne reliant les points apparaisse. En effet, les lignes sont censées relier des points qui appartiennent à une même série temporelle. Or ici, nous avons transformé month en facteur. Préciser group = 1 permet d’indiquer à geom_line() que toutes les catégories du facteur month appartiennent au même groupe, que ce facteur peut être considéré comme une variable continue, et qu’il est donc correct de relier les points.\n\nPour les 3 aéroports, les profils de températures sont très proches. C’est tout à fait logique puisqu’ils sont situés dans un rayon de quelques kilomètres seulement. Le problème de ce graphique est que chaque point a été obtenu en calculant une moyenne. En janvier, nous avons fait la moyenne de 31 valeurs de températures quotidiennes pour chaque aéroport. En février, nous avons fait la moyenne de 28 valeurs de températures quotidiennes pour chaque aéroport. Et ainsi de suite pour tous les mois de l’année 2013. Puisque nous présentons des valeurs de moyennes, il nous faut présenter également l’incertitude associée à ces calculs de moyennes. Pour cela, nous devons calculer l’erreur standard des moyennes :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(temperature_max, na.rm = TRUE) / sqrt(N_obs))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne N_obs erreur_standard\n   <chr>  <int>   <dbl> <int>           <dbl>\n 1 EWR        1    5.69    31           1.14 \n 2 EWR        2    4.74    28           0.762\n 3 EWR        3    8.20    31           0.649\n 4 EWR        4   16.4     30           0.919\n 5 EWR        5   22.2     31           1.02 \n 6 EWR        6   27.4     30           0.769\n 7 EWR        7   31.0     31           0.700\n 8 EWR        8   27.8     31           0.385\n 9 EWR        9   24.6     30           0.716\n10 EWR       10   20.0     31           0.882\n# … with 26 more rows\n\n\nNotre tableau de statistiques descriptives possède maintenant 2 colonnes supplémentaires : le nombre d’observations (que j’ai nommé N_obs), et l’erreur standard associée à chaque moyenne, calculée grâce à la formule vue plus haut \\(se_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\) (la fonction sqrt() permet de calculer la racine carrée). On constate que l’erreur standard, qui s’exprime dans la même unité que la moyenne, est variable selon les mois de l’année. Ainsi, pour l’aéroport de Newark, l’incertitude semble particulièrement faible pour le mois d’août (0.385 ºC) mais presque 3 fois plus forte pour le mois de janvier (1.14 ºC).\nUne fois de plus, je donne un nom à ce tableau de données pour pouvoir le réutiliser plus tard :\n\ntemperatures_se <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(moyenne = mean(temperature_max, na.rm = TRUE),\n            N_obs = n(),\n            erreur_standard = sd(temperature_max, na.rm = TRUE) / sqrt(N_obs))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\nNotez que le package ggplot2 contient une fonction permettant de calculer à la fois la moyenne et erreur standard de la moyenne d’un échantillon :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  4.55  6.82\n 2 EWR        2  4.74  3.97  5.50\n 3 EWR        3  8.20  7.55  8.85\n 4 EWR        4 16.4  15.5  17.4 \n 5 EWR        5 22.2  21.1  23.2 \n 6 EWR        6 27.4  26.6  28.1 \n 7 EWR        7 31.0  30.3  31.7 \n 8 EWR        8 27.8  27.4  28.2 \n 9 EWR        9 24.6  23.9  25.3 \n10 EWR       10 20.0  19.1  20.9 \n# … with 26 more rows\n\n\nLes résultats obtenus ne sont pas exactement au même format :\n\nla colonne y contient les valeurs de moyennes\nla colonne ymin contient la valeur de moyenne moins une fois l’erreur standard\nla colonne ymax contient la valeur de moyenne plus une fois l’erreur standard\n\nIl ne nous restera plus qu’à ajouter des barres d’erreur sur notre graphique pour visualiser l’incertitude associée à chaque valeur de moyenne.\n\n\n1.4.4 Calculs d’intervalles de confiance à 95%\nComme pour les erreurs standard, il est possible de calculer des intervalles de confiance de n’importe quel estimateur calculé à partir d’un échantillon, pour déterminer la gamme des valeurs les plus probables pour les paramètres équivalents dans la population générale. Nous nous concentrerons ici sur le calcul des intervalles de confiance à 95% de la moyenne, mais nous serons amenés à examiner également l’intervalle de confiance de la médiane, puis, dans la Chapitre 4, l’intervalle de confiance à 95% d’une différence de moyennes.\nContrairement à l’erreur standard, il n’y a pas qu’une bonne façon de calculer l’intervalle de confiance à 95% d’une moyenne. Plusieurs formules existent et le choix de la formule dépend en partie de la distribution des données (la distribution suit-elle une loi Normale ou non) et de la taille de l’échantillon dont nous disposons (\\(n\\) est-il supérieur à 30 ou non ?). Dans la situation idéale d’une variable qui suit la distribution Normale, les bornes inférieures et supérieures de l’intervalle de confiance à 95% sont obtenues grâce à cette formule\n\\[\\bar{x} - 1.96 \\cdot \\frac{s}{\\sqrt{n}} < \\mu < \\bar{x} + 1.96 \\cdot \\frac{s}{\\sqrt{n}}\\]\nAutrement dit, la vraie moyenne \\(\\mu\\) d’une population a de bonnes chances de se trouver dans un intervalle de plus ou moins 1.96 fois l’erreur standard de la moyenne. En première approximation, l’intervalle de confiance est donc la moyenne de l’échantillon \\(\\bar{x}\\) plus ou moins 2 fois l’erreur standard (que nous avons appris à calculer à la main un peu plus tôt). On peut donc calculer à la main les bornes inférieures et supérieures de l’intervalle de confiance ainsi :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max, mult = 1.96))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  3.46  7.92\n 2 EWR        2  4.74  3.24  6.23\n 3 EWR        3  8.20  6.93  9.47\n 4 EWR        4 16.4  14.6  18.2 \n 5 EWR        5 22.2  20.1  24.2 \n 6 EWR        6 27.4  25.8  28.9 \n 7 EWR        7 31.0  29.6  32.3 \n 8 EWR        8 27.8  27.0  28.5 \n 9 EWR        9 24.6  23.2  26.0 \n10 EWR       10 20.0  18.3  21.7 \n# … with 26 more rows\n\n\nIci, grâce à l’argument mult = 1.96 de la fonction mean_se() :\n\nla colonne ymin contient maintenant les valeurs de moyennes moins 1.96 fois l’erreur standard\nla colonne ymax contient maintenant les valeurs de moyennes plus 1.96 fois l’erreur standard\n\nDans la pratique, puisque cette méthode reste approximative et dépend de la nature des données dont on dispose, on utilisera plutôt des fonctions spécifiques qui calculeront pour nous les intervalles de confiance à 95% de nos estimateurs. C’est ce que permet en particulier la fonction mean_cl_normal() du package ggplot2. Il est toutefois important de bien comprendre qu’il y a un lien étroit entre l’erreur standard (l’incertitude associées à l’estimation d’un paramètre d’une population à partir des données d’un échantillon), et l’intervalle de confiance à 95% de ce paramètre.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_cl_normal(temperature_max))\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month     y  ymin  ymax\n   <chr>  <int> <dbl> <dbl> <dbl>\n 1 EWR        1  5.69  3.36  8.01\n 2 EWR        2  4.74  3.17  6.30\n 3 EWR        3  8.20  6.88  9.53\n 4 EWR        4 16.4  14.6  18.3 \n 5 EWR        5 22.2  20.1  24.2 \n 6 EWR        6 27.4  25.8  28.9 \n 7 EWR        7 31.0  29.5  32.4 \n 8 EWR        8 27.8  27.0  28.6 \n 9 EWR        9 24.6  23.1  26.0 \n10 EWR       10 20.0  18.2  21.8 \n# … with 26 more rows\n\n\nComme dans les tableaux précédents, 3 nouvelles colonnes ont été crées :\n\ny contient toujours la moyenne des températures mensuelles pour chaque aéroport\nymin contient maintenant les bornes inférieures de l’intervalle à 95% des moyennes\nymax contient maintenant les bornes supérieures de l’intervalle à 95% des moyennes\n\nPour que la suite soit plus claire, nous allons afficher et donner des noms à ces différents tableaux en prenant soin de renommer les colonnes pour plus de clarté.\nTout d’abord, nous disposons du tableau temperatures_se, qui contient, les moyennes des températures mensuelles de chaque aéroport de New York en 2013, et les erreurs standard de ces moyennes :\n\ntemperatures_se\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne N_obs erreur_standard\n   <chr>  <int>   <dbl> <int>           <dbl>\n 1 EWR        1    5.69    31           1.14 \n 2 EWR        2    4.74    28           0.762\n 3 EWR        3    8.20    31           0.649\n 4 EWR        4   16.4     30           0.919\n 5 EWR        5   22.2     31           1.02 \n 6 EWR        6   27.4     30           0.769\n 7 EWR        7   31.0     31           0.700\n 8 EWR        8   27.8     31           0.385\n 9 EWR        9   24.6     30           0.716\n10 EWR       10   20.0     31           0.882\n# … with 26 more rows\n\n\nEnsuite, nous avons produit un tableau presque équivalent que nous allons nommer temperature_se_bornes et pour lequel nous allons modifier le nom des colonnes y, ymin et ymax :\n\ntemperature_se_bornes <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max)) %>% \n  rename(moyenne = y,\n         moyenne_moins_se = ymin,\n         moyenne_plus_se = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_se_bornes\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne moyenne_moins_se moyenne_plus_se\n   <chr>  <int>   <dbl>            <dbl>           <dbl>\n 1 EWR        1    5.69             4.55            6.82\n 2 EWR        2    4.74             3.97            5.50\n 3 EWR        3    8.20             7.55            8.85\n 4 EWR        4   16.4             15.5            17.4 \n 5 EWR        5   22.2             21.1            23.2 \n 6 EWR        6   27.4             26.6            28.1 \n 7 EWR        7   31.0             30.3            31.7 \n 8 EWR        8   27.8             27.4            28.2 \n 9 EWR        9   24.6             23.9            25.3 \n10 EWR       10   20.0             19.1            20.9 \n# … with 26 more rows\n\n\nNous avons ensuite calculé manuellement des intervalles de confiance approximatifs, avec la fonction mean_se() et son argument mult = 1.96. Là encore, nous allons stocker cet objet dans un tableau nommé temperatures_ci_approx, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\ntemperature_ci_approx <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_se(temperature_max, mult = 1.96)) %>% \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_ci_approx\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.46         7.92\n 2 EWR        2    4.74         3.24         6.23\n 3 EWR        3    8.20         6.93         9.47\n 4 EWR        4   16.4         14.6         18.2 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.6         32.3 \n 8 EWR        8   27.8         27.0         28.5 \n 9 EWR        9   24.6         23.2         26.0 \n10 EWR       10   20.0         18.3         21.7 \n# … with 26 more rows\n\n\nEnfin, nous avons calculé les intervalles de confiance avec une fonction spécialement dédiée à cette tâche : la fonction mean_cl_normal(). Nous allons stocker cet objet dans un tableau nommé temperatures_ci, et nous allons modifier le nom des colonnes y, ymin, et ymax :\n\ntemperature_ci <- weather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  group_by(origin, month, day) %>% \n  summarise(temperature_max = max(temp_celsius, na.rm = TRUE)) %>% \n  summarise(mean_cl_normal(temperature_max)) %>% \n  rename(moyenne = y,\n         ci_borne_inf = ymin,\n         ci_borne_sup = ymax)\n\n`summarise()` has grouped output by 'origin', 'month'. You can override using\nthe `.groups` argument.\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\ntemperature_ci\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.36         8.01\n 2 EWR        2    4.74         3.17         6.30\n 3 EWR        3    8.20         6.88         9.53\n 4 EWR        4   16.4         14.6         18.3 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.5         32.4 \n 8 EWR        8   27.8         27.0         28.6 \n 9 EWR        9   24.6         23.1         26.0 \n10 EWR       10   20.0         18.2         21.8 \n# … with 26 more rows\n\n\nMaintenant, si l’on compare les 2 tableaux contenant les calculs d’intervalles de confiance de la moyenne, on constate que les résultats sont très proches :\n\ntemperature_ci_approx\ntemperature_ci\n\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.46         7.92\n 2 EWR        2    4.74         3.24         6.23\n 3 EWR        3    8.20         6.93         9.47\n 4 EWR        4   16.4         14.6         18.2 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.6         32.3 \n 8 EWR        8   27.8         27.0         28.5 \n 9 EWR        9   24.6         23.2         26.0 \n10 EWR       10   20.0         18.3         21.7 \n# … with 26 more rows\n\n\n# A tibble: 36 × 5\n# Groups:   origin [3]\n   origin month moyenne ci_borne_inf ci_borne_sup\n   <chr>  <int>   <dbl>        <dbl>        <dbl>\n 1 EWR        1    5.69         3.36         8.01\n 2 EWR        2    4.74         3.17         6.30\n 3 EWR        3    8.20         6.88         9.53\n 4 EWR        4   16.4         14.6         18.3 \n 5 EWR        5   22.2         20.1         24.2 \n 6 EWR        6   27.4         25.8         28.9 \n 7 EWR        7   31.0         29.5         32.4 \n 8 EWR        8   27.8         27.0         28.6 \n 9 EWR        9   24.6         23.1         26.0 \n10 EWR       10   20.0         18.2         21.8 \n# … with 26 more rows\n\n\n\n\nLes bornes inférieures et supérieures des intervalles de confiance à 95% des moyennes ne sont pas égales quand on les calcule manuellement de façon approchée et quand on les calcule de façon exacte, mais les différences sont minimes."
  },
  {
    "objectID": "01-EDA.html#visualiser-lincertitude-et-la-dispersion",
    "href": "01-EDA.html#visualiser-lincertitude-et-la-dispersion",
    "title": "1  Exploration statistique des données",
    "section": "1.5 Visualiser l’incertitude et la dispersion",
    "text": "1.5 Visualiser l’incertitude et la dispersion\nIl existe plusieurs façons de représenter visuellement les positions, les dispersions et les incertitudes. Concernant les positions et les dispersions tout d’abord, nous avons déjà vu plusieurs façons de faire au semestre 3, en particulier dans les parties consacrées aux histogrammes, aux stripcharts et aux boxplots. Nous reprenons ici brièvement chacun de ces 3 types de graphique afin de les remettre en contexte avec ce que nous avons appris ici.\nDans un dernier temps, nous verrons enfin comment visualiser l’incertitude associée à des calculs de moyennes ou de variance grâce aux barres d’erreurs ou aux encoches des boîtes à moustaches.\n\n1.5.1 Position et dispersion : les histogrammes\nJe vous renvoie à la partie sur les histogrammes du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire. Jetez aussi un œil la partie sur les histogrammes facettés.\nLes histogrammes permettent de déterminer à la fois où se trouvent les valeurs les plus fréquemment observées (la position du pic principal correspond à la tendance centrale), et la dispersion (ou variabilité) des valeurs autour de la tendance centrale. Par exemple, la fonction facet_grid() permet de faire des histogrammes des températures pour chaque aéroport de New York et chaque mois de l’année 2013 :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = temp_celsius, fill = factor(month))) +\n  geom_histogram(bins = 20, color = \"grey20\", show.legend = FALSE) +\n  facet_grid(factor(month) ~ origin, scales = \"free_y\") +\n  labs(x = \"Températures (ºC)\", y = \"Fréquence\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nIci, 36 histogrammes sont produits. Ils permettent de constater que :\n\nles températures évoluent à peu près de la même façon dans les 3 aéroports (les 3 colonnes de graphiques se ressemblent beaucoup)\nles températures moyennes sont plus faibles en hiver qu’en été, et qu’elles sont intermédiaires au printemps et à l’automne. C’est bien la position des pics sur l’axe des abscisses qui nous renseigne là-dessus. On sait aussi que les températures moyennes les plus fortes sont autour de 25 degrés ºC en juillet, alors que ces mêmes températures moyennes sont proches de 0 ºC en janvier, février et décembre.\nla variabilité des température est comparable pour la plupart des mois de l’année, avec une exception au mois d’août où la dispersion des valeurs semble plus limitée. Cette fois, c’est l’étalement de l’histogramme qui nous renseigne sur la dispersion.\n\n\n\n1.5.2 Position et dispersion : les stripcharts\nUne autre façon de visualiser à la fois les tendances centrales et les dispersion consiste à produire un nuage de points “stripchart”. Là encore, je vous renvoie à la partie sur les stripcharts du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_jitter(shape = 21, color = \"grey20\", show.legend = FALSE,\n              width = 0.15, height = 0,\n              alpha = 0.5) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\nCette fois, nous visualisons la totalité des données disponibles, et non les données regroupées dans des classes plus ou moins arbitraires. Mais là encore, on peut facilement comparer la position de chaque série de données : pour les mois d’été, les températures sont plus élevées que pour les mois d’hiver. Et la dispersion des données est aussi facile à comparer entre les mois. Par exemple, la variabilités des températures en janvier est nettement supérieure à celle du mois de février. C’est ici l’étendue du nuage de point sur l’axe des ordonnées qui nous permet de le dire.\n\n\n1.5.3 Position et dispersion : les boxplots\nLa dernière façon classique de visualiser à la fois les tendances centrales et les dispersion consiste à produire graphique boîte à moustaches, ou “boxplot”. Là encore, je vous renvoie à la partie sur les boxplots du livre en ligne de biométrie du semestre 3 si vous avez besoin de vous rafraîchir la mémoire.\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.5) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nVous voyez que le code est très proche pour produire un stripchart ou un boxplot. Comme indiqué au semestre 3, les différents éléments de chaque boîte nous renseignent sur la position et sur la dispersion des données pour chaque mois et chaque aéroport :\n\nLa limite inférieure de la boîte correspond au premier quartile : 25% des données de l’échantillon sont situées au-dessous de cette valeur.\nLa limite supérieure de la boîte correspond au troisième quartile : 25% des données de l’échantillon sont situées au-dessus de cette valeur.\nLe segment épais à l’intérieur de la boîte correspond au second quartile : c’est la médiane de l’échantillon, qui nous renseigne sur la position de la distribution. 50% des données de l’échantillon sont situées au-dessus de cette valeur, et 50% au-dessous.\nLa hauteur de la boîte correspond à l’étendue (ou intervalle) inter-quartile ou Inter Quartile Range (IQR) en anglais. On trouve dans cette boîte 50% des observations de l’échantillon. C’est une mesure de la dispersion des 50% des données les plus centrales. Une boîte plus allongée indique donc une plus grande dispersion.\nLes moustaches correspondent à des valeurs qui sont en dessous du premier quartile (pour la moustache du bas) et au-dessus du troisième quartile (pour la moustache du haut). La règle utilisée dans R est que ces moustaches s’étendent jusqu’aux valeurs minimales et maximales de l’échantillon, mais elles ne peuvent en aucun cas s’étendre au-delà de 1,5 fois la hauteur de la boîte (1,5 fois l’IQR) vers le haut et le bas. Si des points apparaissent au-delà des moustaches (vers le haut ou le bas), ces points sont appelés “outliers”. On peut en observer un pour l’espèce Adélie. Ce sont des points qui s’éloignent du centre de la distribution de façon importante puisqu’ils sont au-delà de 1,5 fois l’IQR de part et d’autre du premier ou du troisième quartile. Il peut s’agir d’anomalies de mesures, d’anomalies de saisie des données, ou tout simplement, d’enregistrements tout à fait valides mais atypiques ou extrêmes. J’attire votre attention sur le fait que la définition de ces outliers est relativement arbitraire. Nous pourrions faire le choix d’étendre les moustaches jusqu’à 1,8 fois l’IQR (ou 2, ou 2,5). Nous observerions alors beaucoup moins d’outliers. D’une façons générale, la longueur des moustaches renseigne sur la variabilité des données en dehors de la zone centrale. Plus elles sont longues, plus la variabilité est importante. Et dans tous les cas, l’examen attentif des outliers est utile car il nous permet d’en apprendre plus sur le comportement extrême de certaines observations.\n\nLorsque les boîtes ont une forme à peu près symétrique de part et d’autre de la médiane (c’est le cas pour cet exemple dans la plupart des catégories), cela signifie qu’un histogramme des mêmes données serait symétrique également\nLes stripcharts et les boxplots sont donc un bon moyen de comparer rapidement la position et la dispersion d’un grand nombre de séries de données : ici, en quelques lignes de code, nous en comparons 12 pour chacun des 3 aéroports de New York.\nLes histogrammes sont plus utiles lorsqu’il y a moins de catégories à comparer. Ils permettent en outre de mieux visualiser les distribution non symétriques, ou qui présentes plusieurs pics.\n\n\n1.5.4 Visualiser l’incertitude : les barres d’erreur\nComme évoqué plus haut, il est important de ne pas confondre dispersion et incertitude. Lorsque l’on visualise des moyennes calculées à partir des données d’un échantillon, il est important de faire apparaître des barres d’erreurs, qui correspondent en générale :\n\nsoit à l’erreur standard de la moyenne\nsoit à l’intervalle de confiance de la moyenne\n\nPuisque deux choix sont possibles, il sera important de préciser systématiquement dans la légende du graphique, la nature des barres représentées. Commençons par visualiser les températures mensuelles avec les erreurs standards. Pour cela, je reprends le tableau temperatures_se créé précédemment :\n\ntemperatures_se %>% \n  ggplot(aes(x = factor(month), y = moyenne)) +\n  geom_line(aes(group = 1)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = moyenne - erreur_standard,\n                    ymax = moyenne + erreur_standard),\n                width = 0.1) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\") +\n  theme_bw()\n\n\n\n\nFigure 1.5: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les erreurs standard\n\n\n\n\nVous remarquerez que :\n\nj’associe factor(month), et non simplement month, à l’axe des x afin d’avoir, sur l’axe des abscisses, des chiffres cohérents allant de 1 à 12, et non des chiffres à virgule\nl’argument group = 1 doit être ajouté pour que la ligne reliant les points apparaisse. En effet, les lignes sont censées relier des points qui appartiennent à une même série temporelle. Or ici, nous avons transformé month en facteur. Préciser group = 1 permet d’indiquer à geom_line() que toutes les catégories du facteur month appartiennent au même groupe, que ce facteur peut être considéré comme une variable continue, et qu’il est donc correct de relier les points.\nla fonction geom_errorbar() contient de nouvelles caractéristiques esthétiques qu’il nous faut obligatoirement renseigner : les extrémités inférieures et supérieures des barres d’erreur. Il nous faut donc associer 2 variables à ces caractéristiques esthétiques. Ici, nous utilisons moyenne - erreur_std pour la borne inférieure des barres d’erreur, et moyenne + erreur_std pour la borne supérieure. Les variables moyenne et erreur_standard faisant partie du tableau temperatures_se, geom_errorbar() les trouve sans difficulté.\nl’argument width de la fonction geom_errorbar() permet d’indiquer la longueur des segments horizontaux qui apparaissent à chaque extrémité des barres d’erreur.\n\nIci, bien que moins lisible, on peut aussi faire apparaître les trois courbes sur le même graphique, afin de mieux visualiser les similarités des fluctuations de températures entre les 3 aéroports :\n\ntemperatures_se %>% \n  ggplot(aes(x = factor(month), y = moyenne, color = origin, group = origin)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = moyenne - erreur_standard,\n                    ymax = moyenne + erreur_standard),\n                width = 0.1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\",\n       color = \"Aéroport\") +\n  theme_bw()\n\n\n\n\nFigure 1.6: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les erreurs standard\n\n\n\n\nDe la même façon, nous pouvons parfaitement faire apparaître, au lieu des erreurs standards, les intervalles de confiance à 95% de chaque valeur de température moyenne. Il nous suffit pour cela d’utiliser le tableau temperatures_ci qui contient les valeurs de moyennes et des bornes supérieures et inférieures de ces intervalles :\n\ntemperature_ci %>% \n  ggplot(aes(x = factor(month), y = moyenne, group = 1)) +\n  geom_line() +\n  geom_point() +\n  geom_errorbar(aes(ymin = ci_borne_inf, ymax = ci_borne_sup), width = 0.1) +\n  facet_wrap(~origin, ncol = 1) +\n  labs(x = \"Mois\", \n       y = \"Moyenne des températures quotidiennes maximales (ºC)\",\n       color = \"Aéroport\") +\n  theme_bw()\n\n\n\n\nFigure 1.7: Températures moyennes mensuelles observées en 2013 dans les 3 aéroports de New York. Les barres d’erreur sont les intervalels de confiance à 95% des moyenes mensuelles.\n\n\n\n\nComme vous voyez, les barres d’erreurs sont maintenant plus longues que sur la Figure 1.5. C’est normal car rappelez-vous que les intervalles de confiance sont à peu près équivalents à 2 fois les erreurs standards. L’intérêt de représenter les intervalles de confiance est qu’ils sont directement liés aux tests statistiques que nous aborderons dans les chapitres suivants. Globalement, quand 2 séries de données ont des intervalles de confiance qui se chevauchent largement (comme les mois de janvier et février par exemple), alors, un test d’hypothèse conclurait presque toujours à l’absence de différence significative entre les 2 groupes. À l’inverse, quand 2 séries de données ont des intervalles de confiance qui ne se chevauchent pas du tout (comme les mois de mars et d’avril par exemple), alors, un test d’hypothèse conclurait presque toujours à l’existence d’une différence significative entre les 2 groupes. Lorsque les intervalles de confiance entre 2 catégorie se chevauchent faiblement ou partiellement (comme entre les mois de juin et juillet pour l’aéroport LGA), la situation est moins tranchée, et nous devrons nous en remettre aux résultats du test pour savoir si une différence devrait être considérée comme significative ou non.\n\n\n1.5.5 Visualiser l’incertitude : les boîtes à moustaches\nOutre les informations de position et de dispersion, les boîtes à moustaches permettent également de visualiser l’incertitude associée aux médianes. Il suffit pour cela d’ajouter l’argument notch = TRUE dans la fonction geom_boxplot() :\n\nweather %>% \n  mutate(temp_celsius = (temp - 32) / 1.8) %>% \n  ggplot(aes(x = factor(month), y = temp_celsius, fill = factor(month))) +\n  geom_boxplot(show.legend = FALSE, alpha = 0.5, notch = TRUE) +\n  facet_wrap(~ origin, ncol = 1) +\n  labs(x = \"Mois\", y = \"Températures (ºC)\") +\n  theme_bw()\n\nWarning: Removed 1 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nDes encoches ont été ajoutées autour de la médiane de chaque boîte à moustache. Ces encoches sont des encoches d’incertitudes. Les limites inférieures et supérieures de ces encoches correspondent aux bornes inférieures et supérieures de l’intervalle de confiance à 95% des médianes. Comme pour les moyennes, le chevauchement ou l’absence de chevauchement entre les encoches de 2 séries de données nous renseigneront sur l’issue probable des futurs tests statistiques que nous devrons réaliser. Il sera donc important de bien examiner ces encoches en amont des tests statistiques pour éviter de faire/dire des bêtises…\n\n\n1.5.6 Exercice\n\nAvec le tableau penguins, calculez les grandeurs suivantes pour chaque espèce de manchot et chaque sexe :\n\n\nla moyenne de la longueur des nageoires\nla variance de la longueur des nageoires\nl’écart-type de la longueur des nageoires\nl’erreur standard de la longueur moyenne des nageoires\nla moyenne de la masse corporelle\nla variance de la masse corporelle\nl’écart-type de la masse corporelle\nl’erreur standard de la masse corporelle moyenne\n\nAttention : pensez à retirer les individus dont le sexe est inconnu.\n\nVérifiez avec la fonction skim() que les moyennes et écart-types calculés ci-dessus sont corrects.\nAvec ces données synthétiques faites le graphique suivant :\n\n\n\n\n\n\n\n\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. nycflights13: Flights that Departed NYC in 2013. https://github.com/hadley/nycflights13.\n\n\n———. 2022. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, et Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "02-PropTests.html",
    "href": "02-PropTests.html",
    "title": "2  Principes des tests statistiques et comparaison de proportions",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "03-OneSampleTests.html",
    "href": "03-OneSampleTests.html",
    "title": "3  Comparaison de moyennes : un échantillon et deux échantillons appariés",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "04-TwoSampleTests.html",
    "href": "04-TwoSampleTests.html",
    "title": "4  Comparaison de moyennes : deux échantillons indépendants",
    "section": "",
    "text": "Sera traité au semestre prochain. Passez directement au Chapitre 5."
  },
  {
    "objectID": "05-Cohortes.html",
    "href": "05-Cohortes.html",
    "title": "5  Analyse de cohortes",
    "section": "",
    "text": "Cette section doit permettre d’illustrer la partie du cours de Population Dynamics de l’EC “Fonctionnement des écosystèmes” consacrée à l’analyse des cohortes. Vous utiliserez les tailes corporelles d’individus échantillonnés sur le terrain à plusieurs dates pour :\n\nProduire la structure démographique instantanée de la population pour chaque date d’échantillonnage\nRéaliser la décomposition polymodale pour identifier les cohortes à chaque date d’échantillonnage\nCréer une courbe de croissance et une courbe de mortalité\nUtiliser une relation allométrique pour passer de la taille des individus à leur masse\nProduire la courbe d’Allen\n\nVous devrez en premier lieu importer les données fournies dans un fichier Excel et vous assurer qu’elles sont dans un format permettant les analyses et représentations graphiques."
  },
  {
    "objectID": "05-Cohortes.html#sec-pres",
    "href": "05-Cohortes.html#sec-pres",
    "title": "5  Analyse de cohortes",
    "section": "5.2 Présentation de l’étude",
    "text": "5.2 Présentation de l’étude\nLe suivi démographique des populations animales est extrêmement fréquent dans le domaine de l’écologie. Le suivi des populations naturelles est particulièrement pertinent dans un contexte de conservation, pour réaliser des études d’impact ou pour mesurer l’évolution de la biodiversité.\nIci, une population de gastéropodes Nassarius reticulatus (la nasse réticulée) a été suivie pendant 5 ans. Deux sessions d’échantillonnage ont été organisées chaque année depuis 2010 : la première a été réalisée en mars, juste après le recrutement des juvéniles, et la seconde a été réalisée 6 mois plus tard, en septembre. Tous les échantillons ont été collectés au même endroit, selon la même méthode (collecte systématique de tous les individus présents à l’intérieur de 10 quadrats positionnés aléatoirement dans la zone d’étude), lors d’une marée basse de fort coefficient (marées de printemps et d’automne). Les individus échantillonnés ont été mesurés sur place, de la base, à l’apex de la coquille (voir Figure 5.1) à l’aide d’un pied à coulisse (précision : centième de millimètre) et relâchés sur place.\n\n\n\n\n\nFigure 5.1: Coquille de nasse réticulée Nassarius reticulatus (Linnaeus, 1758)\n\n\nL’espèce étudiée présente les caractéristiques suivantes :\n\nLes individus ont une durée de vie de 5 ans en moyenne.\nLes plus grand individus peuvent atteindre une taille de plus de 40 millimètres.\nIl n’y a qu’une seule période de reproduction chaque année. Il n’y a donc qu’un unique recrutement chaque année, au tout début du mois de mars.\n\nDes travaux antérieurs réalisés au laboratoire ont montré que la taille et la masse des individus étaient liées par la relation allométrique suivante :\n\\[w = 0.0013 \\cdot l^{2.3}\\]\navec \\(w\\), la masse en grammes et \\(l\\) la longueur des coquilles en millimètres.\n\n\n\n\n\n\nObjectif principal\n\n\n\nL’objectif principal de cette étude est de produire la courbe d’Allen d’une cohorte de cette populations. Cette courbe sera utilisée pour déterminer des gains et des pertes de biomasses au sein de l’écosystème étudié. Les étapes nécessaires à la production de cette courbe sont présentées ci-dessous."
  },
  {
    "objectID": "05-Cohortes.html#avant-de-vous-lancer",
    "href": "05-Cohortes.html#avant-de-vous-lancer",
    "title": "5  Analyse de cohortes",
    "section": "5.3 Avant de vous lancer…",
    "text": "5.3 Avant de vous lancer…\nPour travailler dans de bonnes conditions, vous aurez absolument besoin de travailler dans un script et à l’intérieur d’un Rproject. Si vous ne savez plus comment faire, reportez-vous aux chapitrex correspondants du livre en ligne de Biométrie du semestre 3 : au sujet des scripts et au sujet des Rprojects. Vous devrez notamment (liste non exhaustive !) :\n\nCréer un nouveau script (nommez-le Nasses.R)\nTélécharger (si besoin) et charger quelques packages (voir plus bas)\nTélécharger dans votre répertoire de travail le fichier Nassarius.csv\nImporter dans RStudio les données du fichier Nassarius.csv\n\nPour cette section, vous aurez besoin des packages suivants :\n\ntidyverse : pour manipuler les données et faire des graphiques (Wickham et al. 2019)\nmixdist : pour effectuer les décompositions polymodales (Macdonald et Juan Du 2018)\n\nSi vous ne savez plus comment installer et charger des packages en mémoire, reportez-vous au chapitre correspondant du livre en ligne de Biométrie du semestre 3\n\nlibrary(tidyverse)\nlibrary(mixdist)\n\nPour importer les données, utilisez l’assistant d’importation de RStudio. Notez que :\n\nLes colonnes sont séparées par des tabulations\nLe symbole utilisé pour les décimales dans le fichier Nassarius.csv est la virgule\n\nVous devrez donc spécifier correctement ces éléments pour pouvoir importer le fichier dans le logiciel. Si vous ne savez plus comment faire, reportez-vous au chapitre correspondant du livre en ligne de Biométrie du semestre 3.\nSi l’importation s’est déroulée normalement, vous devriez maintenant disposer de l’objet nommé Nassarius suivant :\n\nNassarius\n\n# A tibble: 3,710 × 2\n    size date    \n   <dbl> <chr>   \n 1  4.8  march-10\n 2  3    march-10\n 3  5.56 march-10\n 4  3.74 march-10\n 5  4.1  march-10\n 6  2.21 march-10\n 7  2.75 march-10\n 8  3.18 march-10\n 9  2.56 march-10\n10  4.36 march-10\n# … with 3,700 more rows\n\n\nEt la commande suivante devrait produire exactement ces résultats :\n\nNassarius %>%\n  count(date)\n\n# A tibble: 10 × 2\n   date         n\n   <chr>    <int>\n 1 march-10   468\n 2 march-11   481\n 3 march-12   460\n 4 march-13   468\n 5 march-14   487\n 6 sept-10    268\n 7 sept-11    276\n 8 sept-12    265\n 9 sept-13    258\n10 sept-14    279\n\n\n\n\n%>% : le pipe est un opérateur spécial qui prend l’objet situé à gauche et le transmets à la fonction placée à droite, en guise de premier argument. P. ex. : Nassarius %>% count(date) est équivalent à count(Nassarius, date)\n count() : compte le nombre d’occurrences de chaque valeur possible (ou niveau/modalité) d’une variable catégorielle (ou facteur)."
  },
  {
    "objectID": "05-Cohortes.html#les-étapes-de-lanalyse",
    "href": "05-Cohortes.html#les-étapes-de-lanalyse",
    "title": "5  Analyse de cohortes",
    "section": "5.4 Les étapes de l’analyse",
    "text": "5.4 Les étapes de l’analyse\nPour produire une courbe d’Allen, de nombreuses étapes sont nécessaires. Vous devrez :\n\nProduire la structure démographique instantanée de la population à chaque date d’échantillonnage\nRéaliser la décomposition polymodale pour identifier les cohortes présentes dans la population à chaque date d’échantillonnage.\nDéterminer la taille moyenne de la coquille des individus de la cohorte d’intérêt, à chaque date d’échantillonnage, pour produire la courbe de croissance\nDéterminer l’abondance des individus de la cohorte d’intérêt, à chaque date d’échantillonnage, pour produire la courbe de survie\n\nJe vais présenter ci-dessous les étapes de cette procédure pour une unique date d’échantillonnage : march 2010. Vous devrez reproduire ces étapes pour les 9 autres dates d’échantillonnage."
  },
  {
    "objectID": "05-Cohortes.html#sélection-des-données-dune-date-spécifique",
    "href": "05-Cohortes.html#sélection-des-données-dune-date-spécifique",
    "title": "5  Analyse de cohortes",
    "section": "5.5 Sélection des données d’une date spécifique",
    "text": "5.5 Sélection des données d’une date spécifique\nNotre jeu de données Nassrisu contient 2 colonnes : la première contient les dates d’échantillonnage et la seconde les tailles individuelles en millimètres. La première étape consiste à créer un nouvel objet (nous le nommerons Nas_01) qui contiendra uniquement les données collectées lors de la toute première session d’échantillonnage de mars 2010. Vous devriez déjà savoir comment utiliser la fonction filter() pour le faire :\n\nNas_01 <- Nassarius %>%\n  filter(date == \"march-10\")\n\nNas_01\n\n# A tibble: 468 × 2\n    size date    \n   <dbl> <chr>   \n 1  4.8  march-10\n 2  3    march-10\n 3  5.56 march-10\n 4  3.74 march-10\n 5  4.1  march-10\n 6  2.21 march-10\n 7  2.75 march-10\n 8  3.18 march-10\n 9  2.56 march-10\n10  4.36 march-10\n# … with 458 more rows\n\n\n\n\nfilter() : permet de filtrer les lignes d’un tableau (ici Nassarius) pour ne conserver que celles qui remplissent une condition spécifiée par l’utilisateur (ici date == \"march_10\")\n\ndim(Nassarius)\n\n[1] 3710    2\n\ndim(Nas_01)\n\n[1] 468   2\n\n\n\n\ndim() : Affiche le nombre de lignes et de colonnes d’un tableau.\nComme nous pouvons le constater, on passe du tableau original Nasarius contenant 3710 lignes à un nouveau tableau Nas_01 qui en contient seulement 468."
  },
  {
    "objectID": "05-Cohortes.html#structure-démographique-instantanée",
    "href": "05-Cohortes.html#structure-démographique-instantanée",
    "title": "5  Analyse de cohortes",
    "section": "5.6 Structure démographique instantanée",
    "text": "5.6 Structure démographique instantanée\nMaintenant que nous disposons d’une table Nas_01 qui contient uniquement la taille des individus collectés en mars 2010, il nous faut visualiser la structure démographique instantanée afin de déterminer combien de cohortes étaient présentes dans la population à cette date. La structure démographique instantanée est ici simplement un histogramme présentant la distribution des tailles individuelles. Nous allons donc placer la taille des individus sur l’axe des x en guise de descripteur individuel, et sur l’axe des y, RStudio placera automatiquement l’abondance pour chaque classe de taille en guise de descripteur populationel.\n\nNas_01 %>%\n  ggplot(aes(x = size)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nFigure 5.2: Structure démographique instantanée\n\n\n\n\n\n\nggplot() : crée un graphique\n aes() : associe une variable d’un jeu de données à une caractéristique esthétique d’un graphique (p. ex. la position le long de l’axe des x ou des y, la couleur, la forme, la taille, etc.)\n geom_histogram() : ajoute un objet géométrique de type histogramme à une graphique produit par ggplot()\nNotez le message d’avertissement qui s’affiche quand vous produisez cet histogramme. Il indique que R a choisi pour nous le nombre de classes de tailles. Par défaut, il crée 30 classes, mais nous indique que ce n’est certainement pas le meilleur choix. Nous allons donc devoir créer nous même manuellement les classes de tailles pour (i) identifier les cohortes présentes dans la population et (ii) fixer des classes identiques que nous utiliserons pour toutes les autres dates d’échantillonnage et qui rendront les comparaisons plus aisées.\nPuisque les individus de cette espèce peuvent atteindre une taille de 40 millimètres environ, nous allons définir des classes de tailles tous les millimètres. On peut faire ça simplement en créant un vecteur qui contient les limites des classes de tailles que l’on souhaite :\n\n# Calcul de la taille maximale observée\ntaille_max <- max(Nassarius$size, na.rm = TRUE)\ntaille_max\n\n[1] 41.33\n\n\n\n\nmax() : affiche la valeur maximale contenue dans un vecteur\n\n# Définition des limites des classes de tailles\nlimites <- 0:(taille_max + 1)\nlimites\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n[26] 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\n\n\n\n\n: : l’opérateur “deux points” permet de créer des suites d’entiers\nL’objet limites contient les limites des classes de tailles que nous utiliserons pour produire les structures démographiques instantanées, pour chaque date d’échantillonnage, et pas seulement pour mars 2010. C’est la raison pour nous avons utilisé max(Nassarius$size) +1 : cette syntaxe nous assure que nos classes de tailles recouvrent bien la taille de tous les individus échantillonnés au cours des 5 années d’études. Ainsi, nous pourrons utiliser les mêmes classes de tailles pour toutes les dates.\nMaintenant, on peut utiliser le vecteur limites comme argument de la fonction geom_histogram(). On modifie aussi les couleurs des barres de l’histogramme pour mieux visualiser les classes :\n\nNas_01 %>%\n  ggplot(aes(x = size)) +\n  geom_histogram(breaks = limites, color = \"black\", fill = \"steelblue\")\n\n\n\n\nFigure 5.3: Structure démographique instantanée utilisable pour la décomposition polymodale\n\n\n\n\nCette structure démographique instantanée (Figure 5.3) fait apparaître 5 cohortes, bien que la plus âgée soit à peine visible. Les pics, ou modes, sont situés autour de 4, 9, 17, 30 et 34 millimètres. Ces valeurs approximatives sont importantes car nous les utiliserons pour réaliser la décomposition polymodale."
  },
  {
    "objectID": "05-Cohortes.html#décomposition-polymodale",
    "href": "05-Cohortes.html#décomposition-polymodale",
    "title": "5  Analyse de cohortes",
    "section": "5.7 Décomposition polymodale",
    "text": "5.7 Décomposition polymodale\n\n5.7.1 Principe et étapes\nEffectuer une décomposition polymodale revient à tenter d’ajuster une distribution Normale à chaque cohorte de la population. Puisque nous avons identifié 5 cohortes dans la population en mars 2010, il nous faut identifier 5 distribution Normales distinctes dont la somme reproduira le plus fidèlement possible la structure démographique instantanée observée. Plusieurs étapes sont requises pour y parvenir.\nNous allons utiliser le package mixdist, que vous devez avoir déjà installé et chargé en mémoire. Ce package fournit plusieurs fonctions relativement simples d’utilisation et qui permettent d’ajuster des distributions Normales à une distribution observée (notre structure démographique instantanée).\nLa fonction du package mixdist qui nous permettra de réaliser la décomposition polymodale est la fonction mix(). Son fichier d’aide nous indique que pour fonctionner correctement, il faut lui fournir plusieurs choses :\n\n?mix()\n\n\n\n? : l’opérateur “point d’interrogation” suivi du nom d’une fonction, permet d’ouvrir le fichier d’aide de cette fonction.\nLes deux premiers arguments de la fonction mix() ne possèdent pas de valeur par défaut. Nous devons donc spécifier nous même ces deux arguments :\n\nLe premier s’appelle mixdat. Il doit obligatoirement s’agir d’un tableau de données (data.frame ou tibble) contenant 2 colonnes. La première colonne doit contenir les limites supérieures des classes de tailles de notre structure démographique instantanée. Le dernier élément de cette colonne doit être “ouvert” : il doit être fixé à \\(+\\infty\\). La seconde colonne doit contenir les abondances pour chaque classe de taille de la structure démographique instantanée. Nous allons voir juste après comment créer cet objet mixdat.\nLe second argument s’appelle mixpar. Là encore, il s’agit d’un tableau de données. Il doit contenir des valeurs approchées pour les paramètres des distributions Normales que nous souhaitons ajuster à chacune de nos 5 cohortes. Chaque distribution Normale possède 2 paramètres : la moyenne (qui correspond à la position du pic sur l’axe des x d’un histogramme) et l’écart-type (qui correspond à l’étalement de la courbe en cloche, c’est-à-dire à la dispersion des données de part et d’autre de la moyenne). Là encore, nous verrons plus bas comment créer cet objet.\n\n\n\n5.7.2 Création du premier objet\nPour créer le premier tableau qui sera utilisé comme argument mixdat de la fonction mix(), nous allons nous servir de la fonction mixgroup() du package mixdist. Elle est très simple `a utiliser puisqu’elle n’a besoin que de 2 arguments :\n\nUn vecteur de données (ici, la taille de tous les individus échantillonnés en mars 2010)\nLa liste des valeurs correspondant aux limites des classes de tailles de la structure démographique instantanée. Nous disposons déjà de cet objet puisque nous l’avons créé plus tôt : c’est le vecteur limites\n\n\nmix_01 <- mixgroup(Nas_01$size, breaks = limites)\nclass(mix_01)\n\n[1] \"mixdata\"    \"data.frame\"\n\n\nL’objet mix_01 que nous venons de créer est donc un data.frame, mais c’est aussi un objet de classe mixdata que nous utiliserons en guise de premier argument de la fonction mix(). Pour afficher son contenu, il suffit comme toujours de taper son nom :\n\nmix_01\n\n     X count\n1    1     1\n2    2     6\n3    3    39\n4    4    70\n5    5    69\n6    6    30\n7    7    19\n8    8    23\n9    9    26\n10  10    33\n11  11    19\n12  12     7\n13  13     2\n14  14     1\n15  15     4\n16  16     5\n17  17    10\n18  18    15\n19  19     7\n20  20    13\n21  21     6\n22  22     5\n23  23     2\n24  24     1\n25  25     4\n26  26     4\n27  27     6\n28  28     2\n29  29     7\n30  30     8\n31  31     3\n32  32     4\n33  33     3\n34  34     4\n35  35     3\n36  36     1\n37  37     2\n38  38     2\n39  39     2\n40  40     0\n41  41     0\n42 Inf     0\n\n\nNotez que la dernière ligne de mix_01 correspond à la catégorie [42 mm ; \\(+\\infty\\)[. Pour vérifier que nous n’avons pas fait d’erreur, on peut faire une représentation graphique de cet objet particulier avec la fonction plot() :\n\nplot(mix_01, xlab = \"Size (mm)\", ylab = \"Probability\")\n\n\n\n\nFigure 5.4: Le contenu de mix_01 correspond exactement à la structure démographique instantanée de mars 2010. La ligne bleue est le contour de la structure démographique instantanée, qui devra être approchée par la superposition de 5 courbes Normales correspondant au 5 cohortes qui composent la population en mars 2010\n\n\n\n\n\n\nplot() : fonction générique permettant de produire des graphiques sans ggplot2. La forme du résultat dépendra de la classe de l’objet utilisé comme argument.\nAu final, l’objet mix_01 est simplement la structure démographique instantanée de mars 2010, présentée dans un format qui sera compris par la fonction mix() que nous utiliserons pour effectuer la décomposition polymodale.\n\n\n5.7.3 Création du deuxième objet\nPour créer le second data.frame() dont la fonction mix() aura besoin, nous allons utiliser une autre fonction du package mixdist : la fonction mixparam(). Cette fonction prend au minimum 2 arguments :\n\nmu : un vecteur qui contient la position approximative des modes (ou pics) de chaque cohorte. Si nous avons 5 cohortes à identifier, il faudra fournir 5 valeurs pour mu. Pour mars 2010, nous utiliserons les valeurs présentées plus au, au niveau de la Figure 5.3 (4, 9, 17, 30 et 34 millimètres). Ces valeurs changeront pour chaque date d’échantillonnage et il faudra donc examiner attentivement les structures démographiques instantanées de chaque échantillonnage pour les déterminer. J’insiste sur le fait que la position des pics peut être approximative, mais qu’elle ne doit malgré tout pas être trop éloignée des vraies valeurs.\nsigma : un vecteur qui contient la valeur approchée de l’écart-type de chaque cohorte. L’écart-type d’une cohorte correspond à l’étalement des tailles de part et d’autres de la moyenne de la cohorte. C’est ce que nous avons appelé “polymorphisme” dans le cours de dynamique des populations, puisque cet étalement représente des performances de croissance variables pour des individus qui sont tous nés approximativement en même temps. Là encore, il faut fournir autant de valeurs que nous avons de cohortes (soit 5 pour mars 2010). Puisque nous n’avons pas d’idée précise d’ordre de grandeur pour ces écarts-types, nous utiliserons la valeur 1 pour chaque cohorte.\n\nAinsi, on obtient le second data.frame ainsi :\n\nparam_01 <- mixparam(mu = c(4, 9, 17, 30, 34),\n                     sigma = c(1, 1, 1, 1, 1))\nparam_01\n\n   pi mu sigma\n1 0.2  4     1\n2 0.2  9     1\n3 0.2 17     1\n4 0.2 30     1\n5 0.2 34     1\n\n\n\n\nmixparam() : fonction qui crée un data.frame dans lequel chaque ligne contient les caractéristiques approchées d’une cohorte de la population échantillonnée\n c() : fonction qui permet de créer un vecteur, donc une collection d’éléments qui sont tous du même type (ici, des valeurs numériques).\nLa première colonne de ce nouveau data.frame, nommée pi, correspond à la proportion de l’effectif total échantillonné, contenu dans chaque cohorte. Puisque nous n’avons rien spécifié, mixparam() suppose que chaque cohorte contient une proportion identiques des individus de la population, et fixe donc la proportion à 0.2 (soit 20% de l’abondance totale dans chacune des 5 cohortes supposées). Nous savons pertinemment que ces proportions sont fausses puisqu’en réalité, l’abondance au sein des cohortes décroit avec l’âge des individus en raison de la mortalité. On sait donc que la cohorte la plus jeune sera la plus abondante, et que les cohortes plus âgées seront de moins en moins abondantes. Ces proportions seront estimées automatiquement par la fonction mix() lorsque nous réaliserons la décomposition polymodale. Les autres colonnes de param_01, qui contiennent les valeurs que nous avons fournies manuellement, seront également ajustées lors de la décomposition polymodale\n\n\n5.7.4 Ajustement des lois Normales aux données\nMaintenant que nous disposons des deux objets nécessaires, nous pouvons réaliser la décomposition polymodale qui consiste, grâce à la fonction mix(), à ajuster une distribution Normale à chaque cohorte supposée de la population échantillonnés.\n\nres_01 <- mix(mix_01, param_01)\n\nWarning in mix(mix_01, param_01): The optimization process terminated because\niteration limit exceeded\n\nres_01\n\n\nParameters:\n       pi     mu sigma\n1 0.46418  3.903 1.086\n2 0.27062  8.795 1.555\n3 0.14355 18.115 2.102\n4 0.09100 28.231 2.861\n5 0.03066 35.021 2.344\n\nDistribution:\n[1] \"norm\"\n\nConstraints:\n   conpi    conmu consigma \n  \"NONE\"   \"NONE\"   \"NONE\" \n\n\n\n\nmix() : réalise la décomposition polymodale. La fonction identifie une combinaison de distributions Normales qui s’ajuste le mieux possible aux données observées compte tenu des données brutes et des caractéristiques approximatives de chaque distribution Normale fournie par l’utilisateur.\nLa fonction mix() peut produire quelques avertissements. Ignorez-les à ce stade : seuls les messages d’erreurs sont problématiques et nous y reviendrons plus tard. L’objet res_01 contient donc les résultats de la décomposition polymodale. C’est une liste de 3 éléments dont seul le premier nous intéresse.1 Il contient les valeurs ajustées pour les 3 paramètres des distributions Normales (pi, mu et sigma) pour chacune des 5 cohortes identifiées en mars 2010.1 Il s’appelle parameters, sans majuscule, même si R affiche son nom avec une majuscule. Vous pouvez le vérifier en tapant str(res_01). \nAinsi, par exemple, pour l’échantillon de mars 2010, la première ligne du tableau parameters contenu dans res_01 nous apprend les choses suivantes :\n\nres_01$parameters\n\n          pi        mu    sigma\n1 0.46417551  3.903201 1.085989\n2 0.27061611  8.795297 1.555004\n3 0.14354832 18.114948 2.101807\n4 0.09099874 28.230516 2.861214\n5 0.03066132 35.020985 2.343520\n\n\n\nLa première cohorte représente 46.42% de l’abondance totale de la population\nLa taille moyenne des individus composant cette cohorte vaut 3.9 millimètres\nL’écart-type (ou polymorphisme de taille) de cette cohorte vaut 1.1 millimètres\n\nLes distributions Normales qui ont été ajustées peuvent être visualisées grâce à la fonction plot() :\n\nplot(res_01)\n\n\n\n\nFigure 5.5: Distribution observée (histogramme bleu) et cohortes ajustées (courbes rouges). Les triangles indiquent la position du mode des cohortes et la courbe verte est la somme de toutes les distributions Normales. Idéallement, cette courbe devrait être étroitement ajustée aux données observées\n\n\n\n\nOn observe sur la Figure 5.5 et dans l’objet res_01$parameters que la qualité de l’ajustement (et donc de la décomposition polymodale) est bonne :\n\nla courbe verte représente bien les données observées : elle est bien ajustée aux contours de l’histogramme.\nl’abondance des cohortes décroit avec l’âge de la cohorte : pi décroit de la cohorte la plus jeune à la cohorte la plus âgée.\nla taille moyenne des individus augmente avec l’âge de la cohorte : mu augmente de la cohorte la plus jeune à la cohorte la plus âgée.\nle polymorphisme (ou dispersion des tailles autour de la moyenne) augmente avec l’âge de la cohorte : sigma augmente de la cohorte la plus jeune à la cohorte la plus âgée.\n\n\n\n\n\n\n\nDe l’importance du choix des valeurs initiales\n\n\n\nSi nous avions choisi d’autres valeurs approchée lors de la création de l’objet param_01 avec la fonction mixparam(), l’ajustement obtenu aurait pu être différent. La décomposition polymodale n’est pas une science exacte et il est souvent nécessaire de procéder par tâtonnements pour trouver des valeurs satisfaisantes. En particulier, les résultats que nous obtenons dépendent :\n\ndu choix des valeurs pour la fonction mixparam()\ndu choix des classes de tailles pour la structure démographique instantanée. Ici, nous avons des classes de tailles de un millimètre de large, mais nous aurions pu faire un autre choix (1,5 millimètre de large, ou 2) et nous aurions alors obtenu des résultats différents.\n\nIl est donc important de retenir que les résultats obtenus ne sont pas “justes” ou “faux” en tant que tel et qu’il n’y a pas qu’une seule “bonne réponse”. La qualité des résultats obtenus s’apprécie au regard de ce qu’on connait de l’espèce étudiée (traits d’histoire de vie, période de reproduction et de recrutement, nombre de cohorte supposées selon la date, etc.), et de ce qu’on connait du comportement “normal” des cohortes (baisse de l’abondance avec l’âge, augmentation de la taille moyenne avec l’âge, augmentation du polymorphisme avec l’âge).\n\n\n\n\n5.7.5 Et en cas de message d’erreur ?\nLorsque vous utiliserez la fonction mix(), il se peut que des messages d’erreurs et/ou des messages d’avertissement apparaissent.\n\n\n Les messages d’information et les avertissements commencent en général par le mot Warning et sont presque toujours sans conséquence. La commande a été comprise par RStudio et un résultat a été produit. Dans le cas de la fonction mix(), les avertissements indiquent que la solution trouvée (l’ajustement des loi Normales aux cohortes observées) n’est peut-être pas optimale, mais une solution a néanmoins été obtenue.\n\n\n Les messages d’erreurs commencent par Erreur ou Error et indiquent que la commande n’a pas abouti. Dans le cas de la fonction mix(), cela peut être lié à 2 choses :\n\nsoit les valeurs choisies pour la fonction mixparam() sont trop éloignées de la position des pics réels, et la fonction mix() ne parvient donc pas à trouver de solution satisfaisante\nsoit le nombre de valeurs choisies pour la fonction mixparam() n’est pas le bon. Par exemple, s’il y a 5 cohortes et qu’on ne fournit que 4 valeurs pour mu et sigma, un message d’erreur apparaîtra. De même, s’il y a 4 cohortes et qu’on fournit 5 valeurs pour mu et sigma, un message d’erreur apparaîtra. Enfin, si on ne fournit pas le même nombre de valeurs pour mu et pour sigma, un message d’erreur apparaîtra.\n\n\n\nDans les deux cas, vous devez\n\nrevenir à votre structure démographique instantanée et l’observer plus attentivement\ndéterminer des valeurs plus appropriées2 pour la fonction mixparam()\nré-exécuter le code depuis la création de l’objet param_01 et jusqu’à la décomposition polymodale avec la fonction mix()\n\n2 ajoutez ou retirez une cohorte si besoin, et choisissez des valeurs plus proches des pics observés"
  },
  {
    "objectID": "05-Cohortes.html#taille-moyenne-et-abondance-des-cohortes",
    "href": "05-Cohortes.html#taille-moyenne-et-abondance-des-cohortes",
    "title": "5  Analyse de cohortes",
    "section": "5.8 Taille moyenne et abondance des cohortes",
    "text": "5.8 Taille moyenne et abondance des cohortes\nÀ partir des résultats de la décomposition polymodale, nous devons maintenant calculer l’abondance (i.e. le nombre d’individus) de chaque cohorte. Nous devrons ensuite stocker dans un nouveau tableau :\n\nla date d’échantillonnage (mars 2010)\nla valeur d’abondance de la cohorte dont on souhaite réaliser le suivi (il s’agit de la cohorte la plus jeune en mars 2010)\nla taille moyenne des individus de la cohorte dont on souhaite réaliser le suivi3\n\n3 on sait déjà que cette taille vaut 3.9 millimètres en mars 2010.L’abondance d’une cohorte est obtenue en multipliant la valeur de pi de la cohorte d’intérêt par le nombre total d’individus échantillonnés en mars 2010. En effet, sur la Figure 5.5, la surface totale comprise entre l’axe des abscisses et la courbe verte vaut 1. Cette surface correspond au nombre total d’individus échantillonnés en mars 2010 :\n\nnrow(Nas_01)\n\n[1] 468\n\n\n\n\nnrow() : affiche le nombre de lignes d’une matrice ou d’un data.frame\nPuisque la première cohorte représente 46.42% de l’abondance totale4, tout ce dont on a besoin pour connaitre l’abondance de la cohorte la plus jeune est :4 c’est ce que nous dit la valeur de pi de la première cohorte dans le tableau res_01$parameters :\n\n\n\n\n\n\npi\nmu\nsigma\n\n\n\n\n0.4642\n3.9032\n1.0860\n\n\n0.2706\n8.7953\n1.5550\n\n\n0.1435\n18.1149\n2.1018\n\n\n0.0910\n28.2305\n2.8612\n\n\n0.0307\n35.0210\n2.3435\n\n\n\n\n\nres_01$parameters[1, 1] * nrow(Nas_01)\n\n[1] 217.2341\n\n\nEn arrondissant à l’entier le plus proche, on obtient :\n\nround(res_01$parameters[1, 1] * nrow(Nas_01), 0)\n\n[1] 217\n\n\n\n\nround() : arrondit des valeurs numériques. Le deuxième argument permet d’indiquer le nombre de décimales.\nNous savons donc maintenant que la première cohorte, la plus jeune de la population échantillonnée en mars 2010, a donc une taille moyenne de 3.9 millimètre et une abondance de 217 individus. Nous allons maintenant utiliser ces valeurs pour créer un tableau et placer le premier point des courbes de croissance, de survie et d’Allen."
  },
  {
    "objectID": "05-Cohortes.html#tableau-et-mise-en-forme-des-données",
    "href": "05-Cohortes.html#tableau-et-mise-en-forme-des-données",
    "title": "5  Analyse de cohortes",
    "section": "5.9 Tableau et mise en forme des données",
    "text": "5.9 Tableau et mise en forme des données\nNous venons de décrire ci-dessus la méthode que vous devrez appliquer pour chaque date d’échantillonnage. En suivant les mêmes étapes, vous devriez être en mesure d’obtenir la taille et l’abondance moyennes de notre cohorte d’intérêt pour les 9 dates restantes. Lorsque vous effectuerez la décomposition polymodale pour ces 9 dates, n’oubliez jamais que vous voulez suivre systématiquement la même cohorte dans le temps. Cette cohorte va progressivement évoluer vers des tailles plus importantes puisque les individus grandissent chaque mois et chaque année. Notre cohorte d’intérêt va donc progressivement se décaler vers la droite des structures démographiques instantanées, et les informations de cette cohortes ne seront donc pas systématiquement situées sur la première ligne des résultats de la décomposition polymodale.\nPour chaque date, on s’attend donc à ce que la taille moyenne des individus soit plus grande que la taille obtenue pour la date d’échantillonnage précédente. De même, l’abondance devrait diminuer au fil du temps en raison de la mortalité naturelle qui affecte tous les individus de la population.\nPour produire les 3 courbes (croissance, survie et Allen) dont nous avons besoin, nous allons créer un tibble contenant 3 colonnes :\n\nla date d’échantillonnage\nla taille moyenne des individus de la cohorte en millimètres\nl’abondance de la cohorte (nombre d’individus de la cohorte)\n\nNous pouvons utiliser la fonction tribble() pour le faire :\n\ncohort <- tribble(\n  ~date,        ~size,                  ~abundance,\n  \"2010-03-01\", res_01$parameters[1,2], res_01$parameters[1,1] * nrow(Nas_01)\n)\n\n\n\ntribble() : permet de créer un tibble ligne par ligne. Le “r” de tribble est l’abréviation de “row”.\nPour chaque nouvelle date d’échantillonnage, vous devrez compléter ce tableau en ajoutant une nouvelle ligne à l’intérieur de cette fonction tribble().\n\n\n\n\n\n\nAttention !\n\n\n\nUne erreur fréquente est de saisir les données obtenues à chaque date d’échantillonnage dans un tableau différent à chaque fois. Ça n’est pas ce qu’il faut faire ! Il faut au contraire compléter le tableau cohorte en ajoutant une nouvelle ligne au tableau existant de la façon suivante :\n\ncohort <- tribble(\n  ~date,        ~size,                  ~abundance,\n  \"2010-03-01\", res_01$parameters[1,2], res_01$parameters[1,1] * nrow(Nas_01),\n  \"2010-09-01\", ...                   , ...\n)\n\n\n\nNous devons spécifier une dernière chose afin de produire les graphiques : à ce stade, la colonne date du nouveau tableau cohort est considéré comme une variable de type character (type <chr>) :\n\ncohort\n\n# A tibble: 1 × 3\n  date        size abundance\n  <chr>      <dbl>     <dbl>\n1 2010-03-01  3.90      217.\n\n\nIl nous faut donc la transformer pour que R la reconnaisse comme étant une variable temporelle afin que les données apparaissent dans l’ordre chronologique (et non alphabétique) sur l’axe des abscisses de nos graphiques. Voilà comment procéder :\n\n# On charge le package lubridate pour travailler avec des dates\n# Ce package fait partie du tidyverse\n# Si vous l'avez installé, lubridate est disponible sur votre ordinateur\nlibrary(lubridate)\ncohort <- cohort %>%\n  mutate(date = date(date))\n\n\n\nmutate() : crée de nouvelles variables dans un tibble, ou modifie des variables existantes.\n date() : transforme des variables de type <chr> (caractères) en variable de type <date> (dates).\nOn vérifie que la variable date possède maintenant le type <date> :\n\ncohort\n\n# A tibble: 1 × 3\n  date        size abundance\n  <date>     <dbl>     <dbl>\n1 2010-03-01  3.90      217.\n\n\nÀ ce stade, nous avons tout ce qu’il nous faut pour produire les courbes de croissance, de survie et d’Allen, à l’aide de ggplot2. Évidemment, chaque courbe ne contiendra ici qu’un seul point puisque nous avons examiné pour l’instant qu’une seule date d’échantillonnage. Elles ne seront complètes que lorsque que vous aurez répété ce travail pour l’ensemble des 10 dates d’échantillonnage."
  },
  {
    "objectID": "05-Cohortes.html#courbe-de-croissance",
    "href": "05-Cohortes.html#courbe-de-croissance",
    "title": "5  Analyse de cohortes",
    "section": "5.10 Courbe de croissance",
    "text": "5.10 Courbe de croissance\nOn place les dates d’échantillonnage sur l’axe des abscisses, et la taille moyennes des individus de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = date, y = size, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Date d'échantillonnage\",\n       y = \"Longueur moyenne de la coquille (mm)\",\n       title = \"Courbe de croissance\") +\n  theme_bw()\n\n\n\ngeom_point() et geom_line() : permettent d’ajouter des points et des lignes (respectivement) sur un graphique\n labs() : permet de spécifier le titre d’un graphique et de ses axes\n theme_bw() : change l’apparence générale du graphique\n\n\n\n\n\nFigure 5.6: Courbe de croissance d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage\n\n\n\n\nL’argument group = 1 est utilisé pour indiquer que toutes les dates d’échantillonnage appartiennent à la même série temporelle, et qu’on souhaite donc relier les dates par une ligne. Le résultat sera nettement plus parlant quand vous aurez ajouté des données d’autres dates d’échantillonnage sur le graphique, afin de visualiser l’évolution de la taille moyenne des individus de la cohorte d’intérêt au fil du temps."
  },
  {
    "objectID": "05-Cohortes.html#courbe-de-survie",
    "href": "05-Cohortes.html#courbe-de-survie",
    "title": "5  Analyse de cohortes",
    "section": "5.11 Courbe de survie",
    "text": "5.11 Courbe de survie\nPour produire la courbe de survie, on procède de la même façon, mais on place les dates d’échantillonnage sur l’axe des abscisses, et l’abondance de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = date, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Date d'échantillonnage\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe de survie\") +\n  theme_bw()\n\n\n\n\nFigure 5.7: Courbe de survie d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#courbe-dallen",
    "href": "05-Cohortes.html#courbe-dallen",
    "title": "5  Analyse de cohortes",
    "section": "5.12 Courbe d’Allen",
    "text": "5.12 Courbe d’Allen\nPour produire la courbe de survie, on place la taille moyenne des individus de la cohorte sur l’axe des abscisses, et l’abondance de la cohorte d’intérêt sur l’axe des ordonnées :\n\ncohort %>%\n  ggplot(aes(x = size, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Longueur moyenne de la coquille (mm)\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe d'Allen\") +\n  theme_bw()\n\n\n\n\nFigure 5.8: Courbe d’Allen d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#relation-allométrique",
    "href": "05-Cohortes.html#relation-allométrique",
    "title": "5  Analyse de cohortes",
    "section": "5.13 Relation allométrique",
    "text": "5.13 Relation allométrique\nL’un des objectifs de ce travail était de produire une courbe d’Allen pour étudier les variations de biomasses. Pour y parvenir, nous devons faire une courbe d’Allen sur laquelle figure la masse moyenne des individus de la cohorte d’intérêt (en grammes), plutôt que la taille moyenne des individus de cette cohorte (en millimètres). Pour passer des tailles en millimètres aux masses en grammes, il nous suffit d’appliquer la relation allométrique fournie dans la Section 5.2, et d’ajouter une nouvelle colonne à notre tableau grâce à la fonction mutate() :\n\ncohort_2 <- cohort %>%\n  mutate(weight = 0.0013 * size ^ 2.3)\n\ncohort_2\n\n# A tibble: 1 × 4\n  date        size abundance weight\n  <date>     <dbl>     <dbl>  <dbl>\n1 2010-03-01  3.90      217. 0.0298\n\n\nIl n’y a plus qu’à utiliser cette nouvelle variable sur l’axe des abscisses d’une courbe d’Allen :\n\ncohort_2 %>%\n  ggplot(aes(x = weight, y = abundance, group = 1)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Masse moyenne des individus (g)\",\n       y = \"Abondance de la cohorte\",\n       title = \"Courbe d'Allen\") +\n  theme_bw()\n\n\n\n\nFigure 5.9: Courbe d’Allen d’une cohorte de la population de Nassarius reticulatus, suivie pendant 5 ans. À compléter avec les données des 9 autres dates d’échantillonnage"
  },
  {
    "objectID": "05-Cohortes.html#à-vous-de-jouer",
    "href": "05-Cohortes.html#à-vous-de-jouer",
    "title": "5  Analyse de cohortes",
    "section": "5.14 À vous de jouer !",
    "text": "5.14 À vous de jouer !\nAu final, voilà comment on peut résumer les différentes étapes de ce travail.\n\n\n\nAssurez vous que vous avez bien compris chaque étape de la méthode décrite pour l’échantillonnage de mars 2010, et pourquoi on fait les choses présentées ici dans l’ordre où on les fait. À ce stade, vous devriez déjà avoir un script assez long qui devrait contenir la plupart des commandes évoquées plus haut. Vous devriez donc pouvoir reproduire ces étapes pour toutes les autres dates d’échantillonnage en faisant des copier-coller et en modifiant quelques éléments précis (dates, nom des objets, valeurs de tailles moyennes pour les cohortes, etc.). Attention aussi à adopter une structure de script la plus clair possible, afin de pouvoir corriger les éventuels bugs ou problèmes plus facilement.\nIl ne vous reste donc plus qu’à reproduire ce travail pour les 9 autres dates d’échantillonnage afin (i) de récupérer les valeurs d’abondance, de taille et de masse moyenne des individus de la cohorte d’intérêt (celle qui a été recrutée en mars 2010) et (ii) de compléter les 3 courbes que nous avons commencées plus haut.\nBon courage !\n\n\n\n\nMacdonald, Peter, et with contributions from Juan Du. 2018. mixdist: Finite Mixture Distribution Models. https://CRAN.R-project.org/package=mixdist.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. « Welcome to the tidyverse ». Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686."
  },
  {
    "objectID": "06-DynamicSystems.html",
    "href": "06-DynamicSystems.html",
    "title": "6  Systèmes dynamiques",
    "section": "",
    "text": "Ne sera pas traité cette année…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Horst, Allison, Alison Hill, and Kristen Gorman. 2022.\nPalmerpenguins: Palmer Archipelago (Antarctica) Penguin Data.\nhttps://CRAN.R-project.org/package=palmerpenguins.\n\n\nMacdonald, Peter, and with contributions from Juan Du. 2018.\nMixdist: Finite Mixture Distribution Models. https://CRAN.R-project.org/package=mixdist.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia,\nHao Zhu, and Shannon Ellis. 2022. Skimr: Compact and Flexible\nSummaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2021. Nycflights13: Flights That Departed NYC in\n2013. https://github.com/hadley/nycflights13.\n\n\n———. 2022. Tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen,\nKohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, and Dewey\nDunnington. 2022. Ggplot2: Create Elegant Data Visualisations Using\nthe Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, and Davis\nVaughan. 2023. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, and Jennifer Bryan. 2022. Readr: Read\nRectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "01-OneSampleTests.html#sec-packages",
    "href": "01-OneSampleTests.html#sec-packages",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.1 Pré-requis",
    "text": "1.1 Pré-requis\nPour travailler dans de bonnes conditions, créez un nouveau dossier sur votre ordinateur, créez un Rproject et un script dans ce dossier, et travaillez systématiquement dans votre script, et surtout pas directement dans la console. Consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nDans ce chapitre, vous aurez besoin d’utiliser des packages spécifiques et d’importer des données depuis des fichiers externes téléchargeables directement depuis ce document. Les packages dont vous aurez besoin ici et que vous devez donc charger en mémoire, sont :\n\nle tidyverse (Wickham 2022), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2022), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2022) pour les représentations graphiques. \nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.  \n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\n\n\n\n\n\n\nImportant\n\n\n\nMême si vous avez déjà installé le tidyverse ou dplyr au semestre précédent, ré-installez dplyr avec install.packages(\"dplyr\"). Ce package a en effet été mis à jour tout récemment, et nous aurons besoin de sa toute dernière version (v1.1.0). Chargez-le ensuite en mémoire avec library(dplyr).\n\n\n\n\n\n\n\n\nAttention\n\n\n\nPensez à installer tous les packages listés ci-dessous avant de les charger en mémoire si vous ne l’avez pas déjà fait. Si vous ne savez plus comment faire, consultez d’urgence la section dédiée aux packages dans le livre en ligne de Biométrie du semestre 3.\n\n\nVous aurez également besoin des jeux de données suivants :\n\nTemperature.csv\nTemperature2.csv\n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "01-OneSampleTests.html#contexte",
    "href": "01-OneSampleTests.html#contexte",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.2 Contexte",
    "text": "1.2 Contexte\nOn s’intéresse ici à la température corporelle des adultes en bonne santé. On souhaite examiner la croyance populaire qui veut que cette température vaut en moyenne 37ºC. Pour le vérifier, on dispose d’un échantillon de 25 adultes en bonne santé choisis au hasard parmi la population américaine et dont on a mesuré la température. Comme pour toute étude statistique, les étapes que nous allons devoir suivre sont les suivantes (dans l’ordre) :\n\nImporter les données dans RStudio, les examiner et éventuellement les (re)mettre en forme si besoin.\nFaire une première exploration des données, grâce au calcul d’indices de statistiques descriptives d’une part, et de représentations graphiques d’autre part.\nRéaliser un test d’hypothèses en respectant la procédure adéquate (en particulier, la vérification des conditions d’application).\n\nC’est donc ce que nous allons faire dans les sections suivantes.\n\n\n\n\n\n\nÀ retenir !\n\n\n\nAvant de se lancer dans les tests d’hypothèses, il est toujours indispensable d’examiner les données dont on dispose à l’aide, d’une part de statistiques descriptives numériques, et d’autres part, de graphiques exploratoires.\nNous avons vu au cours des semestres précédents quels indices statistiques il peut être utile de calculer (dans le livre en ligne du semestre 4) et quelles représentations graphiques il peut être utile de réaliser (dans le livre en ligne du semestre 3) afin de pouvoir se lancer dans des tests d’hypothèses sans risquer de grossières erreurs. N’hésitez pas à cliquer sur ces liens pour vous rafraîchir la mémoire !"
  },
  {
    "objectID": "01-OneSampleTests.html#importation-et-mise-en-forme-des-données",
    "href": "01-OneSampleTests.html#importation-et-mise-en-forme-des-données",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.3 Importation et mise en forme des données",
    "text": "1.3 Importation et mise en forme des données\nNous allons travailler ici sur les données contenues dans le fichier Temperature.csv. Téléchargez ces données dans votre répertoire de travail (attention : ne les ouvrez pas avec Excel !), puis importez les données dans RStudio grâce à l’assistant d’importation. Si vous ne savez plus comment faire, consultez la section dédiée à l’importation des données dans le livre en ligne de Biométrie du semestre 3\nVous stockerez les données dans un objet que vous nommerez Temperature. Après l’importation, tapez son nom dans la console de RStudio et vérifiez que vous obtenez bien exactement ce résultat :\n\nTemperature\n\n# A tibble: 25 × 2\n   individual temperature\n        &lt;dbl&gt;       &lt;dbl&gt;\n 1          1        98.4\n 2          2        98.6\n 3          3        97.8\n 4          4        98.8\n 5          5        97.9\n 6          6        99  \n 7          7        98.2\n 8          8        98.8\n 9          9        98.8\n10         10        99  \n# ℹ 15 more rows\n\n\nLa première chose à faire quand on travaille avec des données inconnues, c’est d’examiner les données brutes. Ici, les données sont importées au format tibble, donc seules les premières lignes sont visibles. Pour visualiser l’ensemble du tableau, utilisez la fonction View() (avec un V majuscule) ou, si vous avez mis en mémoire le tidyverse, la fonction view() (sans majuscule) :\n\nView(Temperature)\n\nCette commande ouvre un nouvel onglet présentant les données dans un tableur simplifié, en lecture seule. On constate ici 2 choses que nous allons modifier :\n\nla première colonne, intitulée individual, n’est pas véritablement une variable. Cette colonne ne contient qu’un identifiant sans intérêt pour notre étude et est en fait identique au numéro de ligne. Nous allons donc supprimer cette colonne.\nles températures sont exprimées en degrés Fahrenheit, ce qui rend leur lecture difficile pour nous qui sommes habitués à utiliser le système métrique et les degrés Celsius. Nous allons donc convertir les températures en degrés Celsius grâce à la formule suivante :\n\n\\[ºC = \\frac{ºF - 32}{1.8}\\]\n\nTemp_clean &lt;- Temperature %&gt;%\n  select(-individual) %&gt;%      # Suppression de la colonne `individual`\n  mutate(                      # Transformation des températures en ºCelsius\n    temperature = (temperature - 32) / 1.8\n    )\n\nTemp_clean\n\n# A tibble: 25 × 1\n   temperature\n         &lt;dbl&gt;\n 1        36.9\n 2        37  \n 3        36.6\n 4        37.1\n 5        36.6\n 6        37.2\n 7        36.8\n 8        37.1\n 9        37.1\n10        37.2\n# ℹ 15 more rows\n\n\nIl nous est maintenant possible d’examiner à nouveau les données avec la fonction View(). Avec des valeurs de températures comprises entre 36.3ºC et 37.8ºC, il n’y a visiblement pas de données aberrantes.\nExaminer les données brutes est donc la première chose que vous devriez prendre l’habitude de faire, et ce de façon systématique, car cela permet de repérer :\n\nLa nature des variables présentes.\nLes variables inutiles qui pourront être supprimées ou négligées.\nLes unités des variables utiles, afin de pouvoir les convertir si nécessaire.\nLes valeurs manquantes, atypiques ou aberrantes qui demanderont toujours une attention particulière.\n\nMaintenant que l’examen préliminaire des données est réalisé, on peut passer au calcul des statistiques descriptives."
  },
  {
    "objectID": "01-OneSampleTests.html#sec-eda",
    "href": "01-OneSampleTests.html#sec-eda",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.4 Exploration statistique des données",
    "text": "1.4 Exploration statistique des données\n\n1.4.1 Position et dispersion\nOn s’intéresse ici au calcul de grandeurs statistiques nous apportant des renseignements sur la position et la dispersion des valeurs de l’échantillon. Les questions auxquelles on tente de répondre à ce stade sont les suivantes :\n\nQuelle est la tendance centrale (moyenne ou médiane) ?\nQuelle est la dispersion des valeurs autour de la tendance centrale (écart-type, variance, intervalle interquartile…) ?\n\nPour répondre à ces questions, on peut faire appel à de multiples fonctions déjà présentées dans le livre en ligne du semestre 4. Par exemple la fonction summarise(), en conjonction avec les fonctions mean(), median(), sd(), var(), min(), max() ou quantile(), ou les fonctions summary() ou skim() (du package skimr).\nJe prends ici un exemple simple, mais n’hésitez pas à expérimenter avec les méthodes décrites dans le livre en ligne du semestre 4.\n\nsummary(Temp_clean)\n\n  temperature   \n Min.   :36.33  \n 1st Qu.:36.67  \n Median :37.00  \n Mean   :36.96  \n 3rd Qu.:37.22  \n Max.   :37.78  \n\n\nOn constate ici que la moyenne et la médiane sont très proches. La distribution des températures doit donc être à peut près symétrique, avec à peu près autant de valeurs au-dessus que de valeurs en dessous de la moyenne. Les premier et troisième quartiles sont à peu près aussi éloignés de la médiane l’un que l’autre, ce qui confirme l’apparente symétrie du jeu de données de part et d’autre de la tendance centrale.\nLa moyenne observée dans l’échantillon vaut 36.96ºC, ce qui est très proche de la moyenne théorique de 37ºC.\nUne autre fonction utile est la fonction IQR(), qui renvoie l’étendue de l’intervalle interquartile (la valeur du troisième quartile moins la valeur de premier quartile) :\n\nTemp_clean %&gt;%\n  summarise(IQ_range = IQR(temperature))\n\n# A tibble: 1 × 1\n  IQ_range\n     &lt;dbl&gt;\n1    0.556\n\n\nOn constate ici que l’intervalle interquartile a une largeur de 0.56ºC. Cela signifie que les 50% des températures les plus centrales de l’échantillon sont situées dans un intervalle d’environ un demi-degré Celsius autour de la médiane.\nEnfin, pour obtenir des informations complémentaires, on peut utiliser la fonction skim() du package skimr :\n\nskim(Temp_clean)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Temp_clean\nNumber of rows             25        \nNumber of columns          1         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            None      \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd   p0  p25 p50  p75 p100 hist \n1 temperature           0             1 37.0 0.377 36.3 36.7  37 37.2 37.8 ▇▇▇▇▂\n\n\nTout comme summary(), la fonction skim() renvoie les valeurs minimales et maximales, les premiers et troisièmes quartiles ainsi que la moyenne et la médiane. Elle nous indique en outre la valeur de l’écart-type de l’échantillon, ainsi que le nombre d’observations et le nombre de données manquantes. Enfin, elle fournit un histogramme très simplifié et sans échelle. Cet histogramme nous permet de nous faire une première idée de la distribution des données et est particulièrement utile pour comparer rapidement un grand nombre de distributions quand il y a plusieurs catégories dans les données (ce qui n’est pas le cas ici).\nOutre ces 3 fonctions (summary(), IQR(), et skim()), il est bien sûr possible de calculer toutes ces valeurs manuellement si besoin :\n\nmean() permet de calculer la moyenne.\nmedian() permet de calculer la médiane.\nmin() et max() permettent de calculer les valeurs minimales et maximales respectivement.\nquantile() permet de calculer les quartiles.\nsd() permet de calculer l’écart-type.\nvar() permet de calculer la variance.\nn() permet de compter le nombre d’observations.\n\nToutes ces fonctions prennent seulement un vecteur en guise d’argument. Il faut donc procéder comme avec IQR() pour les utiliser, en les intégrant à l’intérieur de la fonction summarise(). Par exemple, pour calculer la variance, on peut taper :\n\nTemp_clean %&gt;% \n  summarise(variance = var(temperature))\n\n# A tibble: 1 × 1\n  variance\n     &lt;dbl&gt;\n1    0.142\n\n\nou :\n\nTemp_clean %&gt;%\n  pull(temperature) %&gt;%\n  var()\n\n[1] 0.1417901\n\n\nou encore :\n\nvar(Temp_clean$temperature)\n\n[1] 0.1417901\n\n\nÀ vous d’utiliser la syntaxe qui vous semble la plus simple.\n\n\n1.4.2 Incertitude\nOutre les informations de position et de dispersion, nous avons vu au semestre 4 qu’il était également important d’avoir une idée de l’incertitude associée aux estimations de tendance centrale (erreur standard ou intervalle de confiance de la moyenne ou médiane). Ici, nous allons donc calculer l’intervalle de confiance à 95% de la moyenne. Si vous ne savez plus comment faire, ou que vous ne comprenez pas le code ci-dessous, consultez le livre en ligne du semestre 4 :\n\nTemp_clean %&gt;% \n  reframe(mean_cl_normal(temperature))\n\n# A tibble: 1 × 3\n      y  ymin  ymax\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  37.0  36.8  37.1\n\n\nOn constate ici que les bornes inférieure (36.8ºC) et supérieure (37.1ºC) de l’intervalle de confiance à 95% de la moyenne sont proches de la valeur de moyenne de l’échantillon. Dans la population générale, la moyenne de la température corporelle chez les adultes en bonne santé a de bonnes chances de se trouver quelque part entre 36.8ºC et 37.1ºC. Autrement dit, si la température corporelle des adultes en bonne santé n’est pas exactement de 37ºC, l’écart à cette valeur théorique ne doit pas être très important."
  },
  {
    "objectID": "01-OneSampleTests.html#sec-edagraph",
    "href": "01-OneSampleTests.html#sec-edagraph",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.5 Exploration graphique des données",
    "text": "1.5 Exploration graphique des données\nIci, puisque nous ne disposons que d’une unique variable numérique et que nous n’avons donc qu’un unique groupe, les représentations graphiques que nous allons réaliser doivent nous permettre d’examiner la distribution des données. Pour cela, nous pouvons réaliser soit un histogramme, soit un diagramme de densité.\n\n1.5.1 Histogramme\nVoilà comment produire un histogramme de qualité pour les données de températures :\n\nTemp_clean %&gt;%\n  ggplot(aes(x = temperature)) +\n  geom_histogram(bins = 10, fill = \"firebrick2\", color = \"grey20\", \n                 alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Température (ºC)\",\n       y = \"Fréquence\",\n       title = \"Distribution des températures corporelles\",\n       subtitle = \"n = 25 adultes en bonne santé\")\n\n\n\n\nSi vous ne vous rappelez-plus ce qu’est un histogramme ou comment le faire, ou la signification de l’argument bins, relisez la section consacrée aux histogrammes du livre en ligne de Biométrie du semestre 3. Notez que j’ai ajouté une couleur de remplissage et de la transparence pour rendre le graphique plus facile à lire. J’ai également spécifié des titres pour les axes (en précisant l’unité de la variable numérique dont on représente la distribution) ainsi que le titre (et sous-titre) du graphique, qui précise ce qu’on a sous les yeux et la taille de l’échantillon. Il n’est pas toujours nécessaire de spécifier le titre (et le sous-titre) de cette façon : lorsque vous intégrez des graphiques dans un compte-rendu ou un rapport, le titre est en général précisé sous la figure, au début d’une légende qui la décrit. Enfin, j’ai ajouté geom_rug() pour faire apparaître sous le graphique, le long de l’axe des x, la position des données observées. Cela permet de visualiser les données brutes, et peut donc permettre de mieux comprendre pourquoi un histogramme présente telle ou telle forme.\nIci, la forme de ce l’histogramme est assez proche de celle présentée plus tôt par l’histogramme très simplifié produit par la fonction skim(). Cet histogramme nous apprend qu’en dehors d’un “trou” autour de la température 36.75ºC, la distribution des données est proche d’une courbe en cloche. Il y a fort à parier qu’un test de normalité conclurait à la normalité des données de cet échantillon. C’est ce que nous verrons dans la Section 1.6.1.\n\n\n1.5.2 Diagramme de densité\nUne autre façon de visualiser la distribution d’une variable numérique est de produire un graphique de densité. Il a l’avantage d’éviter à l’utilisateur d’avoir à choisir une valeur pour l’argument bin de la fonction geom_histogram(), mais il a l’inconvénient de présenter une échelle plus difficile à comprendre pour l’axe des ordonnées :\n\nTemp_clean %&gt;%\n  ggplot(aes(x = temperature)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Température (ºC)\",\n       y = \"Densité\",\n       title = \"Distribution des températures corporelles\",\n       subtitle = \"n = 25 adultes en bonne santé\")\n\n\n\n\nLes informations apportées par ce graphique sont cohérentes avec celle de l’histogramme :\n\nles températures les plus fréquemment observées dans notre échantillon de 25 adultes en bonne santé se situent légèrement au dessus de 37ºC. Il s’agit d’une information concernant la position des données (c’est-à-dire où se trouve le pic de la distribution sur l’axe des x)\nles températures observées ont une distribution qui ressemble à peu près à une courbe en cloche, avec des valeurs comprises entre 36.4ºC et 37.8ºC environ. La symétrie de part et d’autre du pic n’est pas parfaite, mais elle reste bonne. Il s’agit d’informations concernant la forme de la distribution et la dispersion des données.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBilan des analyses préliminaires\n\n\n\nSuite à l’exploration statistique et graphique des données de températures, voilà ce qu’on retient :\n\nIl n’y a visiblement pas de données aberrantes.\nLa distribution des données semble suivre à peu près la loi Normale.\nLa médiane et la moyenne sont très proches de 37ºC. Un test devrait donc arriver à la conclusion que la température corporelle des adultes n’est pas significativement différente de 37ºC.\nLa largeur de l’intervalle de confiance à 95% semble faible, ce qui indique une incertitude relativement faible. Si la température réelle des adultes en bonne santé n’est pas exactement de 37ºC, elle ne devrait pas en être très éloignée (quelques dixièmes de degrés Celsuis au plus)."
  },
  {
    "objectID": "01-OneSampleTests.html#le-test-paramétrique",
    "href": "01-OneSampleTests.html#le-test-paramétrique",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.6 Le test paramétrique",
    "text": "1.6 Le test paramétrique\nLe test permettant de comparer la moyenne \\(\\mu\\) d’une population à une valeur théorique, fixée par l’utilisateur, est le test de Student à un échantillon. Il permet de répondre à la question suivante :\n\nLes données observés dans l’échantillon dont je dispose sont-elles compatibles avec l’hypothèse que la moyenne \\(\\mu\\) de la population dont est issu mon échantillon vaut XXX ?\n\navec XXX, une valeur d’intérêt spécifiée par l’utilisateur. Il s’agit d’un test paramétrique très puissant. Comme tous les tests paramétriques, certaines conditions d’application doivent être vérifiées avant de pouvoir l’appliquer.\n\n\n\n\n\n\nImportant\n\n\n\nComme pour tous les tests statistiques que nous allons réaliser lors de ces séances de TP et TEA, nous devrons commencer par spécifier les hypothèses nulles et alternatives de chaque test, ainsi que la valeur du seuil \\(\\alpha\\) que nous allons utiliser. À moins d’avoir une bonne raison de faire autrement, on utilise presque toujours le seuil \\(\\alpha = 0.05\\) dans le domaine des sciences du vivant. C’est donc ce seuil que nous utiliserons dans ce livre en ligne.\n\n\n\n1.6.1 Conditions d’application\nLes conditions d’application du test de Student à un échantillon sont les suivantes :\n\nLes données de l’échantillon sont issues d’un échantillonnage aléatoire au sein de la population générale. Cette condition est partagée par toutes les méthodes que nous verrons dans ces TP. En l’absence d’informations sur la façon dont l’échantillonnage a été réalisé, on considère que cette condition est remplie. Il n’y a pas de moyen statistique de le vérifier, cela fait uniquement référence à la stratégie d’échantillonnage déployée et à la rigueur de la procédure mise en œuvre lors de l’acquisition des données.\nLa variable étudiée doit suivre une distribution Normale dans la population générale. Nous allons vérifier cette condition d’application avec un test de normalité de Shapiro-Wilk.\n\nPour un test de normalité, les hypothèses seront toujours les suivantes :\n\nH\\(_0\\) : la variable étudiée suit une distribution Normale dans la population générale.\nH\\(_1\\) : la variable étudiée ne suit pas une distribution Normale dans la population générale.\n\nLe test de Shapiro-Wilk se réalise de la façon suivante :\n\nshapiro.test(Temp_clean$temperature)\n\nou\n\nTemp_clean %&gt;%\n  pull(temperature) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97216, p-value = 0.7001\n\n\nla fonction pull() permet d’extraire une colonne (ici temperature) d’un tibble (ici Temp_clean) et de la transformer en vecteur.\nW est la statistique du test. Elle permet à RStudio de calculer la p-value. Ici, \\(p &gt; \\alpha\\). On ne peut donc pas rejeter l’hypothèse nulle de normalité : on ne peut pas exclure que dans la population générale, la température suive bel et bien une distribution Normale. Les conditions d’application du test de Student sont bien vérifiées.\n\n\n\n\n\n\nTests et décision : rappel de cours\n\n\n\nÀ l’issue d’un tests statistique, la décision finale est toujours prise par rapport à l’hypothèse nulle (\\(H_0\\)) :\n\nSi la \\(p-\\)value du test est supérieure ou égale à \\(\\alpha\\), on dit qu’on ne peut pas rejeter l’hypothèse nulle \\(H_0\\). Attention, on ne dit jamais que “\\(H_0\\) est vraie”, car il est impossible de le vérifier avec une certitude absolue. Toutefois, les données observées (celles de notre échantillon), sont compatibles avec l’hypothèse nulle que nous avons formulée, jusqu’à preuve du contraire.\nSi la \\(p-\\)value du test est inférieure à \\(\\alpha\\), on dit qu’on rejette l’hypothèse nulle au seuil \\(\\alpha\\). Autrement dit, les données observées ne sont pas compatibles avec l’hypothèse nulle. On accepte alors l’hypothèse alternative (\\(H_A\\)).\n\nL’hypothèse nulle est toujours l’hypothèse la moins “intéressante”, celle pour laquelle “il ne se passe rien de notable” (par exemple : “les données suivent la distribution Normale”, ou “les moyennes sont égales”).\n\n\n\n\n1.6.2 Signification de la \\(p-\\)value\nLa \\(p-\\)value est une grandeur centrale en statistiques et elle est souvent mal comprise et donc mal interprétée. Je prends donc le temps ici d’expliquer ce qu’est la \\(p-\\)value et comment il faut la comprendre.\n\n\n\n\n\n\nDéfinition : la \\(p-\\)value\n\n\n\nLa \\(p-\\)value d’un test statistique, c’est la probabilité, si \\(H_0\\) est vraie, d’obtenir un effet au moins aussi extrême que celui qu’on a observé dans l’échantillon, sous le seul effet du hasard.\n\n\nIci, la \\(p-\\)value de notre test de Normalité de Shapiro-Wilk vaut 0.7101. Cela signifie que si les données suivent réellement la loi Normale dans la population générale (donc si \\(H_0\\) est vraie), l’écart à la Normalité que nous avons observé (ou un écart encore plus important), peut être observé dans 70.1% des cas. Autrement dit, si on prélève un grand nombre d’échantillons de 25 adultes dans la population générale et qu’on regarde à quoi ressemble la distribution des températures dans chacun de ces échantillons, pour 70.1% d’entre eux, la distribution obtenue sera au moins aussi éloignée de la distribution Normale que celle que nous avons observée ici.\nDans notre cas, l’écart entre la loi Normale et les données de notre échantillon peut être visualisé de la façon suivante :\n\n\n\n\n\nLa courbe de densité des données observées est en rouge, et la distribution Normale théorique correspond à la courbe en bleu. Il y a donc un écart entre la courbe en cloche parfaite de la loi Normale et les données observées. La \\(p-\\)value du test de Shapiro-Wilk nous dit que si la température des adultes en bonne santé suit réellement la loi Normale dans la population générale, alors, l’écart que nous avons observé, ou un écart encore plus important, peut être observé simplement par hasard dans 70.1% des cas. Autrement dit, c’est très probable, et on peut donc considérer que l’écart à la loi Normale que nous avons observé est le fruit du hasard et que notre variable suit donc bien la Loi Normale.\nPour bien comprendre cette notion importante, je simule ci-dessous 36 échantillons de 25 adultes dont les températures suivent parfaitement la loi Normale dans la population générale. Je me place donc dans la situation ou je sais que \\(H_0\\) est vraie, pour illustrer la notion de fluctuation d’échantillonnage. En raison du seul hasard de l’échantillonnage, et alors même que les échantillons que je génère sont issus d’une population qui suit parfaitement la Normale, la distribution dans chaque échantillon s’écarte parfois fortement de la courbe en cloche théorique :\n\n\n\n\n\nOn voit bien ici que certains échantillons s’écartent fortement de la distribution théorique alors même que tous les échantillons sont issus d’une population Normale. Et plus l’échantillon sera de taille réduite, plus les écarts à la courbe en cloche parfaite seront grands. La preuve ci-dessous avec des échantillons de n = 15 adultes au lieu de 25 :\n\n\n\n\n\nAu final, la \\(p-\\)value de 0.701 de notre test de Shapiro-Wilk nous indique que l’hypothèse de la Normalité n’est pas incompatible avec les données que nous avons observées.\nImaginons qu’à l’inverse, nous ayons obtenu une \\(p-\\)value très faible, égale à 0.01 par exemple (donc inférieure à notre seuil \\(\\alpha\\) de 0.05). Nous aurions alors rejeté l’hypothèse nulle. En effet, obtenir une \\(p-\\)value de 0.01, signifie que si \\(H_0\\) est vraie, obtenir un écart à la courbe en cloche théorique aussi important que celui que nous observons est très peu probable (une chance sur 100). Puisqu’il est très improbable d’observer un tel écart si \\(H_0\\) est vraie, on en conclu que \\(H_0\\) n’est pas vraie : les données sont incompatibles avec l’hypothèse nulle et on la rejette donc logiquement.\nCette logique sera valable pour tous les autres tests statistiques que nous aborderons dans cet ouvrage. Pour un test de Normalité, on regarde l’écart entre la distribution Normale et les données observées. Pour un test de comparaison de moyennes, on regarde l’écart entre la moyenne théorique et la moyenne observée, ou entre les 2 moyennes qu’on essaie de comparer. Mais la philosophie reste la même.\n\n\n1.6.3 Réalisation du test de Student et interprétation\nPuisque les conditions d’application du test de Student à un échantillon sont vérifiées, nous avons le droit de faire ce test, et nous devons donc maintenant spécifier les hypothèses nulles et alternatives que nous allons utiliser pour le réaliser :\n\nH\\(_0\\) : dans la population générale, la température corporelle moyenne des adultes en bonne santé vaut 37ºC (\\(\\mu = 37\\)).\nH\\(_1\\) : dans la population générale, la température corporelle moyenne des adultes en bonne santé est différente de 37ºC (\\(\\mu \\neq 37\\)).\n\n\n\n\n\n\n\nHypothèses et paramètres\n\n\n\nNotez que les hypothèses des tests statistiques concernent toujours la valeur d’un paramètre de la population générale, et non la valeur des estimateurs calculés dans un échantillon.\n\n\nOn réalise ensuite le test de la façon suivante :\n\nt.test(Temp_clean$temperature, mu = 37)\n\nou\n\nt.test(temperature ~ 1, mu = 37, data = Temp_clean)\n\nou encore,\n\nTemp_clean %&gt;%\n  pull(temperature) %&gt;%\n  t.test(mu = 37)\n\n\n    One Sample t-test\n\ndata:  .\nt = -0.56065, df = 24, p-value = 0.5802\nalternative hypothesis: true mean is not equal to 37\n95 percent confidence interval:\n 36.80235 37.11321\nsample estimates:\nmean of x \n 36.95778 \n\n\nLes résultats fournis ont une forme particulière qui est utilisée par de nombreuses fonctions de tests statistiques dans R. Ils méritent donc qu’on s’y attarde un peu.\nSur la première ligne, R nous confirme que nous avons bien réalisé un test de Student à un échantillon. La première ligne de résultats fournit la valeur du \\(t\\) calculé (ici, -0.56), le nombre de degrés de libertés (ici, df = 24), et la \\(p-\\)value (ici, 0.58, soit une valeur supérieure à \\(\\alpha\\)). Cette première ligne contient donc tous les résultats du test qu’il conviendrait de rappeler dans un rapport. On devrait ainsi dire :\n\nAu seuil \\(\\alpha\\) de 5%, le test de Student ne permet pas rejeter l’hypothèse nulle \\(\\mu = 37\\) (\\(t = -0.56\\), ddl = 24, \\(p = 0.58\\)). Les données observées sont donc compatibles avec l’hypothèse selon laquelle la température corporelle moyenne des adultes en bonne santé vaut 37ºC.\n\nC’est de cette manière que vous devriez rapporter les résultats de ce test dans un compte-rendu ou un rapport à partir de maintenant.\nDans les résultats du test, la ligne suivante (alternative hypothesis: ...) ne donne pas la conclusion du test. Il s’agit simplement d’un rappel concernant l’hypothèse alternative qui a été utilisée pour réaliser le test. Ici, l’hypothèse alternative utilisée est une hypothèse bilatérale (\\(\\mu \\neq 37\\)). Nous verrons plus tard comment spécifier des hypothèses alternatives uni-latérales, même si la plupart du temps, mieux vaut s’abstenir de réaliser de tels tests (à moins bien sûr d’avoir une bonne raison de le faire).\nLes résultats fournis ensuite concernent, non plus le test statistique à proprement parler, mais l’estimation. Ici, la moyenne de l’échantillon est fournie. Il s’agit de la meilleure estimation possible de la moyenne de la population : \\(\\bar{x} = \\hat{\\mu} = 36.96\\). Comme pour toutes les estimations, cette valeur est entachée d’incertitude liée à la fluctuation d’échantillonnage. L’intervalle de confiance à 95% de cette estimation de moyenne est donc également fourni : \\([36.80 ; 37.11]\\). Vous notez qu’il s’agit des mêmes valeurs que celles que nous avions calculées dans la Section 1.4.2. Autrement dit, cet intervalle contient les valeurs les plus vraisemblables pour la véritable valeur de moyenne dans la population générale. Cela confirme bien que nous n’avons pas prouvé au sens strict que la moyenne de la population vaut 37ºC. Nous avons en réalité montré que nous ne pouvions pas exclure que la moyenne de la population générale soit de 37ºC. Puisque cette valeur est comprise dans l’intervalle de confiance, on ne peut donc pas l’exclure : nos données sont compatibles avec cette hypothèse. Mais beaucoup d’autres valeurs figurent aussi dans cet intervalle. Il est donc tout à fait possible que la moyenne soit en réalité différente de 37ºC (par exemple, 36.9ºC). Pour en être sûr, il faudrait probablement un échantillon de plus grande taille afin de limiter l’incertitude, d’augmenter la puissance statistique de notre test, et ainsi d’être en mesure de détecter des différences subtiles."
  },
  {
    "objectID": "01-OneSampleTests.html#lalternative-non-paramétrique",
    "href": "01-OneSampleTests.html#lalternative-non-paramétrique",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.7 L’alternative non paramétrique",
    "text": "1.7 L’alternative non paramétrique\nSi jamais les conditions d’application du test de Student à un échantillon n’étaient pas remplies, il faudrait alors réaliser son équivalent non paramétrique : le test de Wilcoxon des rangs signés. Ce test est moins puissant que son homologue paramétrique. On ne l’effectue donc que lorsque l’on n’a pas le choix :\n\nwilcox.test(Temp_clean$temperature, mu = 37, conf.int = TRUE)\n\nou\n\nwilcox.test(temperature ~ 1, mu = 37, conf.int = TRUE, data = Temp_clean)\n\nou encore\n\nTemp_clean %&gt;%\n  pull(temperature) %&gt;%\n  wilcox.test(mu = 37, conf.int = TRUE)\n\nWarning in wilcox.test.default(., mu = 37, conf.int = TRUE): impossible de\ncalculer la p-value exacte avec des ex-aequos\n\n\nWarning in wilcox.test.default(., mu = 37, conf.int = TRUE): impossible de\ncalculer un intervalle de confiance exact avec des ex-aequos\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  .\nV = 143, p-value = 0.6077\nalternative hypothesis: true location is not equal to 37\n95 percent confidence interval:\n 36.77780 37.11114\nsample estimates:\n(pseudo)median \n      36.94446 \n\n\nLa syntaxe est identique à celle du test de Student à un échantillon à une exception près : l’ajout de l’argument conf.int = TRUE qui permet d’afficher la (pseudo)médiane de l’échantillon et son intervalle de confiance à 95%.\nLes hypothèses nulles et alternatives de ce test sont les mêmes que celles du test de Student à un échantillon. En toute rigueur, on compare la médiane à une valeur théorique, et non la moyenne. Mais dans la pratique, la grande majorité des utilisateurs de ce test font l’amalgame entre moyenne et médiane. Ici, la conclusion correcte devrait donc être :\n\nAu seuil \\(\\alpha\\) de 5%, on ne peut pas rejeter l’hypothèse nulle (test de Wilcoxon des rangs signés, \\(V\\) = 143, \\(p\\) = 0.6077). La médiane de la population (\\(\\widehat{med}\\) = 36.94) n’est pas significativement différente de 37ºC (IC 95% : \\([36.78 ; 37.11]\\)).\n\nSi les données ne suivent pas la loi Normale, la médiane est bien la métrique la plus intéressante puisque c’est elle qui nous renseigne sur la tendance centrale des données.\nEnfin, les tests de Wilcoxon renvoient souvent des messages d’avertissement. Il ne s’agit que de ça : des avertissements. Tant que la \\(p\\)-value d’un test est éloignée de la valeur seuil \\(\\alpha\\), cela n’a pas d’importance. Quand en revanche la \\(p\\)-value est très proche de \\(\\alpha\\), les messages d’avertissement doivent vous alerter : il faut être très prudent face aux conclusions du test qui peuvent alors être assez “fragiles”."
  },
  {
    "objectID": "01-OneSampleTests.html#sec-puiss",
    "href": "01-OneSampleTests.html#sec-puiss",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.8 Les notions d’erreur et de puissance statistique",
    "text": "1.8 Les notions d’erreur et de puissance statistique\nPour avoir le droit de réaliser un test paramétrique, il faut au préalable vérifier qu’un certain nombre de conditions sont vérifiées. Si ce n’est pas le cas, on réalise un équivalent non paramétrique. On peut alors se demander pourquoi ne pas se contenter de faire des tests non paramétrique systématiquement, sans s’embêter à faire des tests supplémentaires ou des tests paramétriques.\nLa raison est simple et elle est liée aux notions d’erreur et de puissance statistique.\n\n\n\n\n\n\nDéfinitions\n\n\n\n\nErreur de type I : notée \\(\\alpha\\), c’est la probabilité de rejeter à tort l’hypothèse nulle. C’est donc la probabilité de rejeter \\(H_0\\) alors qu’elle est vraie.\nErreur de type II : notée \\(\\beta\\), c’est la probabilité d’accepter à tort l’hypothèse nulle. C’est donc la probabilité d’accepter \\(H_0\\) alors qu’elle est fausse.\nPuissance statistique : notée 1 - \\(\\beta\\)), c’est la probabilité de rejeter l’hypothèse nulle à raison. C’est donc la probabilité de rejeter \\(H_0\\) quand elle est réellement fausse.\n\n\n\nÀ chaque fois que l’on réalise un test statistique, on commet nécessairement les 2 types d’erreurs \\(\\alpha\\) et \\(\\beta\\). On souhaite évidemment minimiser les erreurs, mais on ne peut malheureusement pas faire baisser les 2 en même temps. Faire baisser \\(\\alpha\\) (pour diminuer les faux positifs) conduit toujours à augmenter \\(\\beta\\) (les faux négatifs). Faire baisser \\(\\alpha\\) revient en effet à accepter plus souvent l’hypothèse nulle quand elle est vraie. Cela conduit inévitablement accepter aussi plus souvent l’hypothèse nulle quand elle est fausse (et donc, à augmenter les faux négatifs).\nPour bien comprendre l’enjeu associé à ces erreurs, prenons l’exemple de notre système judiciaire. Lorsqu’un accusé est jugé, il est présumé innocent jusqu’à preuve du contraire. Le procès est l’équivalent d’un test statistique, avec :\n\n\\(H_0\\) : l’accusé est innocent\n\\(H_1\\) : l’accusé est coupable\n\nCommettre une erreur de type I revient à condamner à tort l’accusé (on rejette à tort \\(H_0\\)), donc on condamne un innocent. À l’inverse, commettre une erreur de type II revient à libérer un coupable (accepter à tort \\(H_0\\)). Un système de justice plus strict condamnera un plus grand nombre d’accusés, qu’ils soient coupables ou non. Un système plus strict fera donc augmenter l’erreur de type I et baisser l’erreur de type II. À vous de voir ce que vous préférez : libérer plus de coupables, ou condamner plus d’innocents ?\nEn statistiques, la question est tranchée puisqu’on préfère maintenir l’erreur de type I à un niveau assez faible (à 5% ou moins), quitte à laisser augmenter l’erreur de type II (qui est considérée comme acceptable jusqu’à 20% environ). Toutefois, seule l’erreur de type I est sous notre contrôle. En effet, c’est nous qui la choisissons lorsque l’on fixe le seuil \\(\\alpha\\) de nos tests statistiques.\n\n\n\n\n\n\nÀ retenir\n\n\n\nC’est vous qui fixez l’erreur de type I lorsque vous faites un test statistique. L’erreur de type I est le seuil \\(\\alpha\\) du test, que l’on fixe en général à 0,05 (soit 5%) dans le domaine des sciences du vivant.\n\n\nUne fois que le seuil \\(\\alpha\\) est fixé, l’erreur \\(\\beta\\) l’est aussi dans une certaine mesure. Mais on ne peut la connaitre avec précision car elle dépend de beaucoup de choses, notamment la taille des échantillons dont on dispose, la variabilité des données, le type de test réalisé, etc. En général, plus la taille de l’échantillon sera grande, plus l’erreur \\(\\beta\\) sera faible, et donc plus la puissance sera élevée. De même, par rapport aux tests non paramétriques, les tests paramétriques permettent de minimiser l’erreur \\(\\beta\\) et donc d’augmenter la puissance.\nPuisque la puissance statistique vaut \\(1 - \\beta\\), cela revient à dire que les tests paramétriques sont plus puissants que les tests non paramétriques (parfois, beaucoup plus). Au contraire des erreurs de type I et II, la puissance est une grandeur que l’on souhaite maximiser. On aimerait en effet être capables de systématiquement rejeter \\(H_0\\) quand elle est fausse. Nous avons vu plus haut que c’est hélas impossible. Mais choisir le bon test et la bonne procédure statistique permettent néanmoins d’augmenter la puissance, jusqu’à un certain point. C’est la raison pour laquelle on réalisera toujours un test paramétrique si les données dont on dispose le permettent (donc si les conditions d’application des tests paramétriques sont respectées). Et ce n’est qu’en dernier recours qu’on se tournera vers les tests non paramétriques, toujours moins puissants.\n\n\n\n\n\n\nImportant\n\n\n\nUn test paramétrique est toujours plus puissant que ses homologues non paramétriques. Avec un test paramétrique, il est donc plus probable de rejeter \\(H_0\\) à raison qu’avec un test non paramétrique."
  },
  {
    "objectID": "01-OneSampleTests.html#bilan",
    "href": "01-OneSampleTests.html#bilan",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.9 Bilan",
    "text": "1.9 Bilan\nNous avons vu dans ce chapitre quelle est la procédure à suivre pour réaliser un test de comparaison de la moyenne d’une population à une valeur théorique :\n\nexamen préliminaire des données\ncalcul de statistiques descriptives\ncréation de graphiques exploratoires\nvérification des conditions d’application du test paramétrique\nréalisation du test paramétrique ou non paramétrique selon l’issue de l’étape 4\n\nMais nous avons aussi abordé des notions statistiques essentielles pour la suite :\n\nLes ingrédients indispensables pour réaliser un test statistique (les hypothèses nulle et alternative, la statistique du test et le seuil \\(\\alpha\\)).\nLa \\(p-\\)value et la décision du test.\nLes erreurs de type I (\\(\\alpha\\)) et II (\\(\\beta\\)).\nLa puissance statistique (1 - \\(\\beta\\)) qui n’a rien à voir avec la notion de précision.\nLa notion de test paramétrique ou non paramétrique.\n\nAssurez-vous d’avoir les idées claires sur toutes ces notion car elles sont absolument centrales pour ne pas faire/dire de bêtises lorsque l’on analyse des données."
  },
  {
    "objectID": "01-OneSampleTests.html#exercice-dapplication",
    "href": "01-OneSampleTests.html#exercice-dapplication",
    "title": "1  Comparaison de la moyenne d’une population à une valeur théorique",
    "section": "1.10 Exercice d’application",
    "text": "1.10 Exercice d’application\nLe fichier Temperature2.csv contient les données brutes d’une seconde étude similaire, réalisée à plus grande échelle. Importez ces données et analysez-les afin de vérifier si la température corporelle moyenne des adultes en bonne santé vaut bien 37ºC. Comme toujours, avant de vous lancer dans la réalisation des tests statistiques, prenez le temps d’examiner vos données comme nous l’avons décrit dans la Section 1.4 et la Section 1.5, afin de savoir où vous allez, et de repérer les éventuelles données manquantes ou aberrantes. Enfin, interprétez les résultats à la lumière des notions que nous avons abordées ici (en particulier la notion de puissance statistique).\n\n\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2022. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, et Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#sec-packages2",
    "href": "02-TwoSamplePairedTests.html#sec-packages2",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.1 Pré-requis",
    "text": "2.1 Pré-requis\nPour ce nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser les mêmes packages que précédemment :\n\nle tidyverse (Wickham 2022), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2022), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2022) pour les représentations graphiques. \nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.  \n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\nVous aurez également besoin des jeux de données suivants que vous pouvez dès maintenant télécharger dans votre répertoire de travail :\n\nAutruches.csv\nTestosterone.csv  \n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#contexte",
    "href": "02-TwoSamplePairedTests.html#contexte",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.2 Contexte",
    "text": "2.2 Contexte\nOn s’intéresse ici à la comparaison de 2 séries de données dont les observations sont liées 2 à 2. C’est par exemple le cas lorsque l’on fait subir un traitement à différents sujets et que l’on souhaite comparer les mesures obtenues avant et après le traitement.\nAutrement dit, dans les plans d’expériences appariés, les deux traitements ou modalités sont appliqués à chaque unité d’échantillonnage : chaque sujet ou unité d’échantillonnage fournit plusieurs valeurs. Ça n’était pas le cas du chapitre précédent (@#sec-moy1) où chaque adulte n’avait fourni qu’une unique valeur de température.\nVoici quelques exemples de situations qui devraient être traitées avec des tests sur données appariées :\n\nComparaison de la masse de patients avant et après une hospitalisation.\nComparaison de la diversité de peuplements de poissons dans des lacs avant et après contamination par des métaux lourds.\nTest des effets d’une crème solaire appliquée sur un bras de chaque volontaire alors que l’autre bras ne reçoit qu’un placébo.\nTest des effets du tabagisme dans un échantillon de fumeurs, dont chaque membre est comparé à un non fumeur choisi pour qu’il lui ressemble le plus possible en terme d’âge, de masse, d’origine ethnique et sociale, etc.\nTest des effets que les conditions socio-économiques ont sur les préférences alimentaires en comparant des vrais jumeaux élevés dans des familles adoptives séparées qui diffèrent en termes de conditions socio-économiques.\n\nLes 2 derniers exemples montrent que même des individus séparés peuvent constituer une “paire statistique” s’ils partagent un certain nombre de caractéristiques (physiques, environnementales, génétiques, comportementales, etc.) pertinentes pour l’étude.\nIci, nous allons nous intéresser au lien qui pourrait exister entre la production de testostérone et l’immunité chez une espèce d’oiseau vivant en Amérique du Nord, le carouge à épaulettes.\n\n\n\nLe carouge à épaulettes\n\n\nChez de nombreuses espèces, les mâles ont plus de chances d’attirer des femelles s’ils produisent des niveaux de testostérone élevés. Est-ce que la forte production de testostérone de certains mâles a un coût, notamment en terme d’immunocompétence ? Autrement dit, est-ce que produire beaucoup de testostérone au moment de la reproduction (ce qui fournit un avantage sélectif) se traduit par une immunité plus faible par la suite, et donc une plus forte susceptibilité de contracter des maladies (ce qui constitue donc un désavantage sélectif) ? Ce type de question est central pour comprendre comment l’allocation des ressources affecte à la fois la survie et la fécondité des individus.\nPour étudier cette question, une équipe de chercheurs (Hasselquist et al. 1999) a mis en place le dispositif expérimental suivant. Les niveaux de testostérone de 13 carouges à épaulettes mâles ont été artificiellement augmentés par l’implantation chirurgicale d’un microtube perméable contenant de la testostérone. L’immunocompétence a été mesurée pour chaque oiseau avant et après l’opération chirurgicale. La variable mesurée est la production d’anticorps suite à l’exposition des oiseaux avec un antigène non pathogène mais censé déclencher une réponse immunitaire. Les taux de production d’anticorps sont exprimés en logarithmes de densité optique par minute \\(\\left(\\ln\\frac{mOD}{min}\\right)\\). Si la production de testostérone influence l’immunocompétence, on s’attend à observer des différence de production d’anticorps avant et après l’intervention chirurgicale."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#importation-et-mise-en-forme-des-données",
    "href": "02-TwoSamplePairedTests.html#importation-et-mise-en-forme-des-données",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.3 Importation et mise en forme des données",
    "text": "2.3 Importation et mise en forme des données\nLes données se trouvent dans le fichier Testosterone.csv. Importez ces données dans un objet nommé Testo et affichez son contenu.\n\nTesto\n\n# A tibble: 13 × 5\n   blackbird beforeImplant afterImplant logBeforeImplant logAfterImplant\n       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;            &lt;dbl&gt;           &lt;dbl&gt;\n 1         1           105           85             4.65            4.44\n 2         2            50           74             3.91            4.3 \n 3         3           136          145             4.91            4.98\n 4         4            90           86             4.5             4.45\n 5         5           122          148             4.8             5   \n 6         6           132          148             4.88            5   \n 7         7           131          150             4.88            5.01\n 8         8           119          142             4.78            4.96\n 9         9           145          151             4.98            5.02\n10        10           130          113             4.87            4.73\n11        11           116          118             4.75            4.77\n12        12           110           99             4.7             4.6 \n13        13           138          150             4.93            5.01\n\n\nVisiblement, il n’y a pas de données manquantes mais certaines variables sont inutiles. En effet, nous aurons besoin des variables transformées en logarithmes, mais pas des 2 colonnes beforeImplant et afterImplant. Nous allons donc les retirer avec la fonction select(). Par ailleurs, la variable blackbird est importante puisque chaque individu a fourni 2 valeurs de production d’anticorps : 1 avant et 1 après l’opération chirurgicale. Il sera donc important de conserver cet identifiant individuel. Toutefois, il apparaît ici sous la forme d’une variable numérique alors qu’il s’agit d’un identifiant, d’un code. Il faut donc le transformer en facteur car cela n’aurait pas de sens calculer une moyenne des identifiants par exemple. Pour cela, nous utiliserons la fonction factor() à l’intérieur de mutate(). Enfin, nous renommerons les colonnes avec rename() pour avoir des noms plus courts et plus faciles à utiliser. Si vous ne vous rappelez plus comment utilisez ces fonctions, consulter ces chapitres du livre en ligne de biométrie du semestre 3 : select() et rename(), mutate() et factor(). Enfin, nous donnerons le nom Testo_large au tableau modifié :\n\nTesto_large &lt;- Testo %&gt;% \n  select(-beforeImplant, -afterImplant) %&gt;%  # Suppression des colonnes inutiles\n  mutate(blackbird = factor(blackbird)) %&gt;%  # Transformation en facteur \n  rename(ID = blackbird,                     # Changement des noms de variables\n         Before = logBeforeImplant,\n         After = logAfterImplant)\n\nTesto_large\n\n# A tibble: 13 × 3\n   ID    Before After\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 1       4.65  4.44\n 2 2       3.91  4.3 \n 3 3       4.91  4.98\n 4 4       4.5   4.45\n 5 5       4.8   5   \n 6 6       4.88  5   \n 7 7       4.88  5.01\n 8 8       4.78  4.96\n 9 9       4.98  5.02\n10 10      4.87  4.73\n11 11      4.75  4.77\n12 12      4.7   4.6 \n13 13      4.93  5.01\n\n\nLe tableau Testo_large dont nous disposons maintenant n’est pas dans un format qui nous permettra de réaliser toutes les opérations dont nous aurons besoin. En réalité, il ne s’agit pas d’un “tableau rangé” au sens du tidyverse. Un tableau rangé est un tableau dans lequel chaque ligne correspond à une unique observation et chaque colonne correspond à une unique variable. Ici, nous devrions avoir les 3 variables suivantes :\n\nL’identifiant des individus. La colonne ID correspond à cette variable.\nLe moment auquel chaque mesure a été effectuée, avant ou après l’opération chirurgicale. Cette information est pour l’instant stockée dans l’en-tête des colonnes 2 et 3 du tableau Testo_large\nLa mesure de réponse immunitaire (en logarithme de la densité optique par minute). Cette information est pour l’instant stockée sous forme de valeurs numériques dans les colonnes 2 et 3 du tableau Testo_large\n\nPour obtenir un tableau rangé, il nous faut donc réorganiser les colonnes 2 et 3 du tableau Testo_large :\n\nl’entête de ces 2 colonnes devrait constituer une nouvelle variable que nous nommerons Moment\nle contenu de ces 2 colonnes (les valeurs numériques) devrait constituer une nouvelle variable que nous nommerons DO (pour densité optique).\n\nPour effectuer cette transformation, nous utiliserons la fonction pivot_longer() du package tidyr (il est déjà chargé en mémoire si vous avez chargé le tidyverse). Comme son nom l’indique, cette fonction produira un tableau plus “long” (qui aura plus de lignes) que le tableau de départ. Nous l’appellerons donc Testo_long :\n\nTesto_long &lt;- Testo_large %&gt;%\n  pivot_longer(cols = c(Before, After),   # Les colonnes qu'on veut réorganiser\n               names_to = \"Moment\",   # Quel nom donner à la variable qui contiendra les noms des anciennes colonnes\n               values_to = \"DO\") %&gt;%      # Quel nom donner à la variable qui contiendra le contenu des anciennes colonnes\n  mutate(Moment = factor(Moment, levels = c(\"Before\", \"After\")))\n\nTesto_long\n\n# A tibble: 26 × 3\n   ID    Moment    DO\n   &lt;fct&gt; &lt;fct&gt;  &lt;dbl&gt;\n 1 1     Before  4.65\n 2 1     After   4.44\n 3 2     Before  3.91\n 4 2     After   4.3 \n 5 3     Before  4.91\n 6 3     After   4.98\n 7 4     Before  4.5 \n 8 4     After   4.45\n 9 5     Before  4.8 \n10 5     After   5   \n# ℹ 16 more rows\n\n\nCe nouvel objet contient les mêmes données que précédemment, mais sous un format différent (il contient maintenant 26 lignes et non plus 13) : il s’agit d’un tableau rangé.\nLa plupart du temps, on a besoin de ces 2 formats de tableaux quand nous traitons des données. Le tableau au format long est à privilégier pour les représentations graphiques et les tests statistiques, et le format court sert souvent à présenter des résultats sous une forme synthétique. Mais parfois (et c’est justement le cas quand on dispose de données appariées comme pour notre exemple de lien entre testostérone et immunocompétence), le tableau au format large permettra de faire certains graphiques, certains tests ou certaines manipulations plus facilement que le tableau rangé au format long.\nSi on ne dispose que d’un tableau au format large, on peut passer au format long, comme nous venons de le faire, grâce à la fonction pivot_longer(). Et si on ne dispose que d’un tableau au format long, on peut passer au format large grâce à la fonction pivot_wider(). Nous avons déjà d’ailleurs évoqué cette fonction dans le livre en ligne de biométrie du semestre 4 pour mettre en forme des résultats obtenus avec summarise() (par exemple ici) ou reframe() (ou là), et je vous encourage à y jeter un œil à nouveau pour vous remémorer la syntaxe. Car il est important que vous maîtrisiez ces 2 fonctions dont vous aurez très souvent besoin.\nMaintenant que nous disposons de ces 2 tableaux, Testo_large et Testo_long, nous pouvons commencer à décrire nos données."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#exploration-statistique-des-données",
    "href": "02-TwoSamplePairedTests.html#exploration-statistique-des-données",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.4 Exploration statistique des données",
    "text": "2.4 Exploration statistique des données\nPour décrire simplement les données, nous nous en tiendront ici à l’utilisation des fonctions summary() et skim().\nPour la fonction summary(), le plus simple est toujours d’utiliser le tableau au format large :\n\nsummary(Testo_large)\n\n       ID        Before          After     \n 1      :1   Min.   :3.910   Min.   :4.30  \n 2      :1   1st Qu.:4.700   1st Qu.:4.60  \n 3      :1   Median :4.800   Median :4.96  \n 4      :1   Mean   :4.734   Mean   :4.79  \n 5      :1   3rd Qu.:4.880   3rd Qu.:5.00  \n 6      :1   Max.   :4.980   Max.   :5.02  \n (Other):7                                 \n\n\nOn constate ici que pour les 2 traitements, les valeurs des différents indices sont très proches entre les 2 séries de données, avec des valeurs de densité optiques (DO) légèrement supérieures après l’opération chirurgicale (sauf pour le premier quartile).\nPour la fonction skim() le plus simple est là aussi d’utiliser le tableau large :\n\nskim(Testo_large)\n\n── Data Summary ────────────────────────\n                           Values     \nName                       Testo_large\nNumber of rows             13         \nNumber of columns          3          \n_______________________               \nColumn type frequency:                \n  factor                   1          \n  numeric                  2          \n________________________              \nGroup variables            None       \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts            \n1 ID                    0             1 FALSE         13 1: 1, 2: 1, 3: 1, 4: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd   p0 p25  p50  p75 p100 hist \n1 Before                0             1 4.73 0.280 3.91 4.7 4.8  4.88 4.98 ▁▁▁▃▇\n2 After                 0             1 4.79 0.262 4.3  4.6 4.96 5    5.02 ▂▁▂▁▇\n\n\nOn arrive toutefois aux mêmes résultats avec le tableau long, à condition de grouper les données par traitement (variable Traitement) avec group_by() :\n\nTesto_long %&gt;%\n  group_by(Moment) %&gt;%\n  skim(DO)\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             26        \nNumber of columns          3         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            Moment    \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable Moment n_missing complete_rate mean    sd   p0 p25  p50  p75\n1 DO            Before         0             1 4.73 0.280 3.91 4.7 4.8  4.88\n2 DO            After          0             1 4.79 0.262 4.3  4.6 4.96 5   \n  p100 hist \n1 4.98 ▁▁▁▃▇\n2 5.02 ▂▁▂▁▇\n\n\nCela revient à demander à la fonction skim() de produire un résumé des données de densité optique (variable DO), pour chaque catégorie de la variable Moment, soit un résumé pour la catégorie Before (avant l’intervention chirurgicale), et un résumé pour la catégorie After (après l’intervention chirurgicale).\nPar rapport aux résultats fournis par la fonction summary(), la fonction skim() nous permet de confirmer que les valeurs de DO sont très légèrement supérieures après l’opération (sauf pour le premier quartile). Elle nous permet également de constater que l’écart-type est du même ordre de grandeur pour les 2 catégories, bien qu’il soit légèrement plus faible après l’opération. Enfin, les petits histogrammes laissent entrevoir une distribution très asymétrique des données dans chacun des 2 groupes de mesures."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#exploration-graphique-des-données",
    "href": "02-TwoSamplePairedTests.html#exploration-graphique-des-données",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.5 Exploration graphique des données",
    "text": "2.5 Exploration graphique des données\nIci, c’est le tableau rangé au format long qui sera le plus adapté. Lorsque nous avions une unique série de données, nous avons utilisé 2 types de représentations graphiques très similaires pour visualiser les données (les histogrammes et les graphiques de densités). Ici, nous allons utiliser ces mêmes types de graphiques mais “facettés”. Les graphiques facettés ont été abordés dans le livre en ligne de biométrie du semestre 3. Ils permettent de faire des sous-graphiques pour chaque catégorie d’un facteur. Ici, le facteur Moment contient 2 catégories. Les facets nous permettrons donc de comparer les 2 distributions de densités optiques.\nOutre ces graphiques, nous utiliserons aussi les stripcharts et les boites à moustaches pour comparer les 2 catégories. Ces 2 types de graphiques sont particulièrement adaptés pour ce genre de tâche, et seront aussi très utiles pour l’ANOVA lorsque nous aurons plus de 2 catégories à comparer.\nD’une façon générale, nous disposons :\n\nd’une variable numérique, DO : la mesure de densité optique qui rend compte de l’immunocompétence des carouges à épaulettes\nd’une variable catégorielle, le facteur Moment : indique si les valeurs d’immunocompétences ont été mesurées avant ou après l’opération chirurgicale d’implantation de la capsule de testostérone.\n\nTous les graphiques présentés dans le chapitre consacré à cette situation précise dans le livre en ligne de biométrie du semestre 3, peuvent être réalisés. N’hésitez pas à le relire, en particulier la section expliquant comment faire apparaître et interpréter les encoches d’incertitudes sur des boiites à moustaches.\n\n2.5.1 Avec un stripchart\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO)) +\n  geom_jitter(height = 0, width = 0.25) +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"immunocompétence\\navant et après l'opération\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n\n\n\n\n2.5.2 Avec des histogrammes facettés\nNous allons faire un histogramme pour chaque série de données en utilisant des facettes :\n\nTesto_long %&gt;%\n  ggplot(aes(x = DO)) +\n  geom_histogram(bins = 10, fill = \"firebrick2\", color = \"grey20\", alpha = 0.5)+\n  geom_rug() +\n  facet_wrap(~Moment, ncol = 1) +\n  labs(x = \"immunocompétence (log DO / minute)\",\n       y = \"Fréquence\",\n       title = \"Comparaison de l'immunocompétence avant et après l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n2.5.3 Avec des diagrammes de densité facettés\n\nTesto_long %&gt;%\n  ggplot(aes(x = DO)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~Moment, ncol = 1) +\n  labs(x = \"immunocompétence (log DO / minute)\",\n       y = \"Densité\",\n       title = \"Comparaison de l'immunocompétence avant et après l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\n\n\n2.5.4 Avec des boîtes à moustaches\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO)) +\n  geom_boxplot(notch = TRUE) +\n  expand_limits(y = 5.2) +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"Comparaison de l'immunocompétence avant et après opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\nNotch went outside hinges\nℹ Do you want `notch = FALSE`?\n\n\n\n\n\n\n\n\n\nDu point de vue de la position des données, ces différents graphiques montrent tous que la seconde série de données (catégorie After : après l’opération chirurgicale) présente en moyenne des valeurs très légèrement plus élevées que la première (catégorie Before avant l’opération). En terme de dispersion, si l’on met de côté la valeur minimale de la série Before qui semble atypique (un individu outlier qui présente une immunocompétence très faible avant l’opération), la dispersion des données autour de la tendance centrale semble globalement plus importante pour la série After. Enfin, pour ce qui concerne l’incertitude, les intervalles de confiance à 95% des médianes (qui apparaissent sous la forme d’encoches sur les boîtes à moustaches) se chevauche assez largement, ce qui nous permet d’anticiper les résultats des tests que nous ferons ensuite : puisque les encoches se chevauchent, il y a fort à parier que le test de comparaison de moyenne ne montrera aucune différence significative. On note également que l’encoche de la série After est particulièrement large : la limite supérieure de l’intervalle de confiance à 95% de la médiane est supérieure à la valeur maximale observée dans l’échantillon. Cela traduit le fait que compte de la grande variabilité des données dans cette série, un échantillon de taille n = 13 n’est probablement pas suffisant pour avoir une estimation précise de la médiane.\n\n\n2.5.5 Avec un nuage de points appariés\nToutes ces représentations graphiques sont certes utiles, mais elles masquent un élément crucial : ce sont les mêmes individus qui sont étudiés avant et après l’opération. Il s’agit de données appariées ! Les graphiques que nous avons faits jusque là ne permettent pas de visualiser ce lien entre les deux séries de données. Pour avoir une bonne vision de ce qui se passe, il nous faut faire apparaître ce lien entre les 2 séries de données :\n\nTesto_long %&gt;%\n  ggplot(aes(x = Moment, y = DO, group = ID, color = ID)) +\n  geom_line() +\n  geom_point() +\n  labs(y = \"immunocompétence (log DO / minute)\",\n       title = \"Comparaison de l'immunocompétence avant et après opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\", \n       color = \"Individu\")\n\n\n\n\n\n\n\n\nCe graphique nous donne une image très différente de la réalité des données. On constate ici que l’immunocompétence de certains individus augmente après l’opération (parfois fortement), alors que pour d’autres, elle diminue.\nUne façon d’estimer si les changements d’immunocompétence sont majoritairement orientés dans un sens ou non est de calculer l’intervalle de confiance à 95% de la différence d’immunocompétence entre avant et après l’opération. Pour cela, on peut calculer, grâce au tableau large Testo_large, les différences d’immunocompétences (DO après opération moins DO avant opération), pour chacun des 13 individus. puis, grâce à la fonction mean_cl_normal() déjà utilisée à plusieurs reprises, on calcul l’intervalle de confiance à 95% de la moyenne de cette différence :\n\n# Calcul de la différence de DO (After - Before)\nTesto_large &lt;- Testo_large %&gt;% \n  mutate(Difference = After - Before)\n\n# Affichage du tableau\nTesto_large\n\n# A tibble: 13 × 4\n   ID    Before After Difference\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 1       4.65  4.44    -0.21  \n 2 2       3.91  4.3      0.390 \n 3 3       4.91  4.98     0.0700\n 4 4       4.5   4.45    -0.0500\n 5 5       4.8   5        0.200 \n 6 6       4.88  5        0.120 \n 7 7       4.88  5.01     0.130 \n 8 8       4.78  4.96     0.180 \n 9 9       4.98  5.02     0.0400\n10 10      4.87  4.73    -0.140 \n11 11      4.75  4.77     0.0200\n12 12      4.7   4.6     -0.100 \n13 13      4.93  5.01     0.0800\n\n# Calcul de la moyenne des différences et de son IC95%\nTesto_large %&gt;% \n  reframe(mean_cl_normal(Difference))\n\n# A tibble: 1 × 3\n       y    ymin  ymax\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 0.0562 -0.0401 0.152\n\n\nOn constate ici que la moyenne des différences de densité optique vaut 0.06, soit une valeur positive, qui montre que l’immunocompétence augmente après l’opération (ce qui semble aller à l’opposé de l’hypothèse des chercheurs). Cette moyenne reste néanmoins très proche de 0. D’ailleurs, l’intervalle de confiance de cette moyenne comprend les valeurs situées entre -0.04 et +0.15. La valeur 0 est donc comprise dans cet intervalle. Le zéro fait donc partie des valeurs les plus probables pour la moyenne de ces différences dans la populations générale. C’est là encore un résultat qui nous permet d’anticiper sur les résultats du tests statistique que nous ferons ensuite."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#le-test-paramétrique",
    "href": "02-TwoSamplePairedTests.html#le-test-paramétrique",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.6 Le test paramétrique",
    "text": "2.6 Le test paramétrique\n\n2.6.1 Procédure\nLe test paramétrique permettant de comparer la moyenne sur des séries appariées est là encore un test de Student : le test de Student sur données appariées (étonnant non ?…). En réalité, ce test de Student n’est pas un test de comparaison de moyennes entre 2 séries de données. La procédure est la suivante :\n\nPour chaque individu, calculer la différence d’immunocompétence entre les deux temps de l’expérience (DO après - DO avant opération). C’est ce que nous avons fait plus haut en ajoutant la colonne Difference au tableau Testo_large.\nPuisque nous avons 13 individus, nous aurons 13 valeurs de différences. La moyenne de cette différence sera comparée à la valeur théorique 0. Autrement dit, si cette moyenne vaut 0, l’immunocompétence sera la même avant et après l’opération. Si la moyenne des différence n’est pas égale 0, alors nous aurons prouvé qu’il existe une différence d’immunocompétence entre les 2 groupes, nous aurons prouvé que la procédure chirurgicale d’implantation de la capsule de testostérone a un impact sur l’immunocompétence des carouges à épaulettes\n\n\n\n\n\n\n\nAttention\n\n\n\nDans un test sur données appariées, on s’intéresse à la moyenne des différences entre les données des 2 séries. Cette moyenne est alors comparée à la valeur théorique \\(\\mu\\) = 0. Ce test est donc équivalent au test vu dans le Chapitre 1 sur la comparaison de la moyenne d’une population à une valeur théorique.\nNotez également que la moyenne des différences n’est pas équivalente à la différence des moyennes. La différence des moyennes est une grandeur qui nous sera utile dans le chapitre suivant (Chapitre 3) sur la comparaison de la moyenne de deux populations lorsque les données sont indépendantes.\n\n\n\n\n2.6.2 Conditions d’application\nLes conditions d’application de ce test paramétrique sont presque les mêmes que pour le test de Student à un échantillon :\n\nLes individus sur lesquels portent la comparaison doivent être issus d’un échantillonnage aléatoire. Comme toujours, en l’absence d’indication contraire, on considère que cette condition est vérifiée.\nLes différences par paires entre les 2 modalités du traitement doivent suivre une distribution Normale. Attention, ce n’est donc pas les données brutes de chaque série qui doivent suivre une loi Normale, mais bien la différence “après” - “avant” calculée pour chaque individu. Nous avons déjà calculé ces différences plus haut :\n\n\n# On s'intéresse aux 13 différences calculées sur les 13 individus\nTesto_large \n\n# A tibble: 13 × 4\n   ID    Before After Difference\n   &lt;fct&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 1       4.65  4.44    -0.21  \n 2 2       3.91  4.3      0.390 \n 3 3       4.91  4.98     0.0700\n 4 4       4.5   4.45    -0.0500\n 5 5       4.8   5        0.200 \n 6 6       4.88  5        0.120 \n 7 7       4.88  5.01     0.130 \n 8 8       4.78  4.96     0.180 \n 9 9       4.98  5.02     0.0400\n10 10      4.87  4.73    -0.140 \n11 11      4.75  4.77     0.0200\n12 12      4.7   4.6     -0.100 \n13 13      4.93  5.01     0.0800\n\n\nIl nous faut donc tester la Normalité de la nouvelle variable Difference. Commençons par en faire un graphique :\n\nTesto_large %&gt;%\n  ggplot(aes(x = Difference)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  labs(x = \"Différence d'immunocompétence 'Après - Avant' l'opération (log DO / minute)\",\n       y = \"Densité\",\n       title = \"Distribution de la différence d'immunocompétence entre après et avant l'opération chirurgicale\",\n       subtitle = \"n = 13 carouges à épaulettes\")\n\n\n\n\nCompte tenu du faible nombre d’individus (n = 13), la forme de cette courbe de densité n’est pas si éloignée que ça d’une courbe en cloche (notez que ce n’était pas du tout le cas pour les données brutes de chaque série de départ qui ont toutes les deux des distributions très éloignées de la distribution Normale). On le vérifie avec un test de normalité de Shapiro-Wilk :\n\nH\\(_0\\) : la différence d’immunocompétence des individus suit une distribution Normale.\nH\\(_1\\) : la différence d’immunocompétence des individus ne suit pas une distribution Normale.\n\n\nTesto_large %&gt;%\n  pull(Difference) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.97949, p-value = 0.977\n\n\n\nAu seuil \\(\\alpha = 0.05\\), on ne peut pas rejeter l’hypothèse nulle de normalité pour la différence d’immunocompétence entre après et avant l’intervention chirurgicale (test de Shapiro-Wilk, \\(W = 0.98\\), \\(p = 0.977\\)).\n\nLes conditions d’application du test paramétrique sont donc réunies.\n\n\n\n\n\n\nAttention !\n\n\n\nPour ce test, la Normalité doit bien être verifiée sur la différence entre les 2 groupes de valeurs, et non sur chaque groupe de valeur pris séparément. C’est une source d’erreur fréquente. Ici, les données de départ (DO avant et DO après) ne suivaient pas du tout une distribution Normale. Pourtant, la différence de DO suit bel et bien la distribution Normale, nous permettant de faire le test paramétrique.\n\n\n\n\n2.6.3 Réalisation du test et interprétation\nLe test de Student sur données appariées peut se faire de 3 façons distinctes. Les 3 méthodes fournissent exactement les mêmes résultats, seule la syntaxe utilisée change. Quelle que soit la méthode utilisée, les hypothèses nulles et alternatives sont toujours les mêmes :\n\nH\\(_0\\) : le changement moyen de production d’anticorps après la pose chirurgicale de l’implant de testostérone est nul (\\(\\mu_{Diff} = 0\\)). La procédure chirurgicale n’a pas d’effet sur l’immunocompétence. Les variations observées ne sont que le fruit du hasard de l’échantillonnage.\nH\\(_1\\) : le changement moyen de production d’anticorps après la pose chirurgicale de l’implant de testostérone n’est pas nul (\\(\\mu_{Diff} \\neq 0\\)). La procédure chirurgicale a effet significatif sur l’immunocompétence. Les variations observées ne sont pas uniquement dues à la fluctuation d’échantillonnage.\n\n\n2.6.3.1 Première syntaxe\n\n# Méthode nº1 : avec une formule et le tableau au format long\nt.test(DO ~ Moment, data = Testo_long, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  DO by Moment\nt = -1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.15238464  0.04007695\nsample estimates:\nmean difference \n    -0.05615385 \n\n\nPlusieurs remarques concernant cette première syntaxe :\n\nOn utilise le symbole “~” pour indiquer une formule. On cherche à regarder l’effet du Moment sur la DO qui traduit l’immunocompétence. Le “~” se lit : “en fonction de”.\nAvec la syntaxe utilisant les formules, on doit spécifier l’argument data = Testo_long pour indiquer à RStudio que les variables DO et Moment sont des colonnes de ce tableau.\nEnfin, il est important d’indiquer paired = TRUE puisque nous réalisons un test de Student sur données appariées. Si on ne mets pas cet argument, on réalise un test de Student sur échantillons indépendants, ce qui peut grandement fausser les résultats.\n\nIci, voilà la conclusion de ce test :\n\nLe test de Student sur données appariées ne permet pas de montrer de changement d’immunocompétence suite à l’intégration de l’implant chirurgical de testostérone. On ne peut pas rejeter l’hypothèse nulle au seuil \\(\\alpha = 0.05\\) (\\(t = -1.27\\), \\(ddl = 12\\), \\(p = 0.223\\)). La moyenne des différences de densités optiques observées entre avant et après l’intervention chirurgicale vaut -0.056 (intervalle de confiance à 95% de cette différence : [-0.152 ; 0.040])\n\nDonc visiblement, une forte production de testostérone n’est pas significativement associée à une baisse de l’immunocompétence.\n\n\n\n\n\n\nPoint de vigilance\n\n\n\nAvec cette première syntaxe, la différence qui est calculée n’est pas After - Before comme nous l’avons fait manuellement dans le tableau Testo_large, mais Before - After. En effet, dans le facteur Moment du tableau Testo-Long, la première modalité est Before, et la seconde modalité est After. Par défaut, le test de Student calcule toujours la différence dans le même sens : première modalité moins seconde modalité.\nCela explique pourquoi la différence calculée ici vaut -0.056, alors qu’elle valait +0.056 quand nous l’avions calculée manuellement plus haut. C’est important d’en prendre conscience pour ne pas interpréter à l’envers les résultats du test statistique.\nPour la même raison, les signes et l’ordre des bornes de l’intervalle de confiance à 95% de la moyenne des différences est également inversé. Manuellement, nous avions calculé un intervalle de confiance de [-0.04 ; +0.15], ici, il vaut [-0.15 ; +0.04].\n\n\n\n\n2.6.3.2 Deuxième syntaxe\n\n# Méthode nº2 : avec les 2 séries de données et le tableau au format large\nt.test(Testo_large$Before, Testo_large$After, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  Testo_large$Before and Testo_large$After\nt = -1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.15238464  0.04007695\nsample estimates:\nmean difference \n    -0.05615385 \n\n\nCette deuxième syntaxe est différente de la première puisque nous n’utilisons plus le format formule. Ici, on indique le nom des 2 colonnes du tableau Testo_large qui contiennent les 2 séries de données. Puisque nous n’utilisons plus de formule, l’argument “data = ...” n’existe plus. C’est pourquoi il nous faut taper spécifiquement “Testo_large$Before” et “Testo_large$After”, et non pas simplement le nom des colonnes. En revanche, comme pour le test précédent, il est indispensable d’indiquer “paired = TRUE” pour faire un test de Student sur données appariées.\nLes résultats fournis et leur interprétation sont identiques à ceux de la syntaxe précédente. Vous notez aussi que les résultats et leurs interprétation dépendent de l’ordre dans lequel les 2 séries de données sont indiquées dans la fonction t.test(). Pour vous en convaincre, regardez ce que donne cette commande :\n\nt.test(Testo_large$After, Testo_large$Before, paired = TRUE)\n\nQu’est-ce qui change ? Et qu’est-ce qui reste inchangé ?\n\n\n2.6.3.3 Troisième syntaxe\n\n# Méthode nº3 : avec la variable Diff, mu = 0, et le tableau au format large\nt.test(Testo_large$Difference, mu = 0)\n\n\n    One Sample t-test\n\ndata:  Testo_large$Difference\nt = 1.2714, df = 12, p-value = 0.2277\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -0.04007695  0.15238464\nsample estimates:\n mean of x \n0.05615385 \n\n\nEnfin, comme expliqué plus haut, le test de Student sur données appariées est strictement équivalent à un test de Student à un échantillon pour lequel on compare la moyenne des différences individuelles à 0. Là encore, les résultats produits et leur interprétation sont identiques aux deux tests précédents. La seule différence concerne les signes puisque les deux premiers tests regardaient la différence “Before - After” alors que ce troisième test regarde la différence “After - Before” (que nous avons calculée manuellement).\nÀ vous donc de choisir la syntaxe qui vous paraît la plus parlante ou celle que vous avez le plus de facilité à retenir et à interpréter."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#lalternative-non-paramétrique",
    "href": "02-TwoSamplePairedTests.html#lalternative-non-paramétrique",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.7 L’alternative non paramétrique",
    "text": "2.7 L’alternative non paramétrique\nComme pour le test de Student à un échantillon, lorsque les conditions d’application du test de Student sur données appariées ne sont pas vérifiées (c’est à dire lorsque la différence entre les données appariées des deux séries ne suit pas une loi Normale), il faut utiliser un test non paramétrique équivalent.\nIl s’agit là encore du test de Wilcoxon des rangs signés qui s’intéresse aux médianes. Les hypothèses nulles et alternatives sont les suivantes :\n\nH\\(_0\\) : le changement médian de production d’anticorps après la pose chirurgicale de l’implant de testostérone est nul (\\(med_{Diff} = 0\\)).\nH\\(_1\\) : le changement médian de production d’anticorps après la pose chirurgicale de l’implant de testostérone n’est pas nul (\\(med_{Diff} \\neq 0\\)).\n\nComme pour le test de Student, 3 syntaxes sont possibles et strictement équivalentes. Il est important de ne pas oublier l’argument paired = TRUE pour les 2 premières syntaxes afin de s’assurer que l’on réalise bien un test sur données appariées. Enfin, l’argument conf.int = TRUE doit être ajouté pour les 3 syntaxes afin que la (pseudo-) médiane et son intervalle de confiance à 95% soient calculés et affichés.\n\nwilcox.test(DO ~ Moment, data = Testo_long, paired = TRUE, conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  DO by Moment\nV = 30, p-value = 0.3054\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.145  0.040\nsample estimates:\n(pseudo)median \n        -0.055 \n\nwilcox.test(Testo_large$Before, Testo_large$After, paired = TRUE,\n            conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  Testo_large$Before and Testo_large$After\nV = 30, p-value = 0.3054\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -0.145  0.040\nsample estimates:\n(pseudo)median \n        -0.055 \n\nwilcox.test(Testo_large$Difference, mu = 0, conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  Testo_large$Difference\nV = 61, p-value = 0.3054\nalternative hypothesis: true location is not equal to 0\n95 percent confidence interval:\n -0.040  0.145\nsample estimates:\n(pseudo)median \n         0.055 \n\n\nIci, la conclusion de ce test est :\n\nLe test de Wilcoxon des rangs signés n’a pas permis de montrer de changement d’immunocompétence suite à l’intégration de l’implant chirurgical de testostérone. On ne peut pas rejeter l’hypothèse nulle au seuil \\(\\alpha = 0.05\\) (\\(V = 61\\), \\(p = 0.305\\)). La médiane des différences de densités optiques observées entre après et avant l’intervention chirurgicale vaut 0.055 (intervalle de confiance à 95% de cette différence : [-0.040 ; 0.145])."
  },
  {
    "objectID": "02-TwoSamplePairedTests.html#exercice-dapplication",
    "href": "02-TwoSamplePairedTests.html#exercice-dapplication",
    "title": "2  Comparaison de moyennes : deux échantillons appariés",
    "section": "2.8 Exercice d’application",
    "text": "2.8 Exercice d’application\nLes autruches vivent dans des environnements chauds et elles sont donc fréquemment exposées au soleil durant de longues périodes. Dans des environnements similaires, les mammifères ont des mécanismes physiologiques leur permettant de réduire la température de leur cerveau par rapport à celle de leur corps. Une équipe de chercheurs (Fuller et al. 2003) a testé si les autruches pouvaient faire de même. La température du corps et du cerveau de 37 autruches a été enregistrée par une journée chaude typique. Les résultats, exprimés en degrés Celsius, figurent dans le fichier Autruches.csv.\nImportez ces données et faites-en l’analyse pour savoir s’il existe une différence de température moyenne entre le corps et le cerveau des autruches. Vos observations chez les autruches sont-elles conformes à ce qui est observé chez les mammifères dans un environnement similaire ? Comme toujours, vous commencerez par faire une analyse descriptive des données, sous forme numérique et graphique, avant de vous lancer dans les tests d’hypothèses.\n\n\n\n\nFuller, Andrea, Peter R. Kamerman, Shane K. Maloney, Graham Mitchell, et Duncan Mitchell. 2003. « Variability in brain and arterial blood temperatures in free-ranging ostriches in their natural habitat ». Journal of Experimental Biology 206 (7): 1171‑81. https://doi.org/10.1242/jeb.00230.\n\n\nHasselquist, Dennis, James A. Marsh, Paul W. Sherman, et John C. Wingfield. 1999. « Is avian humoral immunocompetence suppressed by testosterone? » Behavioral Ecology and Sociobiology 45 (3): 167‑75. https://doi.org/10.1007/s002650050550.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2022. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, et Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr."
  },
  {
    "objectID": "03-TwoSampleTests.html#sec-packages3",
    "href": "03-TwoSampleTests.html#sec-packages3",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.1 Pré-requis",
    "text": "3.1 Pré-requis\nComme pour chaque nouveau chapitre, je vous conseille de travailler dans un nouveau script que vous placerez dans votre répertoire de travail, et dans une nouvelle session de travail (Menu Session &gt; Restart R). Inutile en revanche de créer un nouveau Rproject : vos pouvez tout à fait avoir plusieurs script dans le même répertoire de travail et pour un même Rproject. Comme toujours, consultez le livre en ligne du semestre 3 si vous ne savez plus comment faire.\nSi vous êtes dans une nouvelle session de travail (ou que vous avez quitté puis relancé RStudio), vous devrez penser à recharger en mémoire les packages utiles. Dans ce chapitre, vous aurez besoin d’utiliser :\n\nle tidyverse (Wickham 2022), qui comprend notamment le package readr (Wickham, Hester, et Bryan 2022), pour importer facilement des fichiers .csv au format tibble, le package dplyr (Wickham et al. 2023), pour manipuler des tableaux, et le package ggplot2 (Wickham et al. 2022) pour les représentations graphiques.\nreadxl (Wickham et Bryan 2022), pour importer facilement des fichiers Excel au format tibble.\nskimr (Waring et al. 2022), qui permet de calculer des résumés de données très informatifs.\ncar (Fox, Weisberg, et Price 2022), qui permet d’effectuer le test de comparaison des variances de Levene.\nle package palmerpenguins (Horst, Hill, et Gorman 2022) pour accéder au jeu de données penguins que nous utiliserons pour les exercices d’application.\n\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(car)\nlibrary(palmerpenguins)\n\nVous aurez également besoin des jeux de données suivants que vous pouvez dès maintenant télécharger dans votre répertoire de travail :\n\nHommesFemmes.xls\nHornedLizards.csv\n\nEnfin, je spécifie ici une fois pour toutes le thème que j’utiliserai pour tous les graphiques de ce chapitre. Libre à vous de choisir un thème différent ou de vous contenter du thème proposé par défaut :\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "03-TwoSampleTests.html#contexte",
    "href": "03-TwoSampleTests.html#contexte",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.2 Contexte",
    "text": "3.2 Contexte\nOn s’intéresse maintenant aux méthodes permettant de comparer la moyenne de deux groupes ou de deux traitements dans la cas d’échantillons indépendants. Au contraire de la situation décrite dans le Chapitre 2, dans ce type de design expérimentaux, les deux traitements sont appliqués à des échantillons indépendants issus de 2 groupes ou populations distincts. Chaque individu collecté, ou chaque unité expérimentale observée, ne fournit qu’une seule valeur, indépendante de toutes les autres.\nCette situation est extrêmement classique dans le domaine de l’écologie au sens large. Ainsi, par exemple, lorsque l’on souhaite comparer 2 sites, on réalise des prélèvements dans chacun des 2 sites. Chaque prélèvement ne fournit qu’une valeur pour l’un des 2 sites.\nIci, nous allons examiner une espèce intéressante, le lézard cornu Phrynosoma mcallii, qui possède une frange de piquants autour de la tête. Une équipe d’herpétologues (Young, Brodie, et Brodie 2004) a étudié la question suivante : des piquants plus longs autour de la tête protègent-ils le lézard cornu de son prédateur naturel, la pie grièche migratrice Lanius ludovicianus ? Ce prédateur a en effet une particularité : il accroche ses proies mortes à des barbelés ou des branches pour les consommer plus tard. Les chercheurs ont donc mesuré la longueur des cornes de 30 lézards retrouvés morts et accrochés dans des arbres par la pie grièche migratrice. Et en parallèle, ils ont mesuré les cornes de 154 individus vivants et en bonne santé choisis au hasard dans la population.\n\n\n\n\n\n\n\n(a) Lézard cornu vivant\n\n\n\n\n\n\n\n(b) Lézard cornu mort\n\n\n\n\n\n\n\n(c) Pie grièche\n\n\n\n\nFigure 3.1: Le lézard cornu et son prédateur\n\n\nNous disposons donc de 2 groupes indépendants : chaque lézard n’a fourni qu’une valeur de longueur de cornes, et chaque lézard n’appartient qu’à un groupe, vivant ou mort. Nous sommes donc dans la situation typique de la comparaison de moyennes de 2 populations avec des données indépendantes. Avant de procéder aux tests, et comme toujours, nous allons commencer par importer et mettre en forme les données (si besoin), puis nous devrons explorer les données, à l’aide d’une part d’indices statistiques de position, de dispersion et d’incertitude et d’autre part de représentations graphiques pertinentes. Enfin, nous vérifierons les conditions d’application du test paramétrique de Student avant de réaliser ce test si les conditions d’application sont remplies, ou son équivalent non paramétrique si elles ne le sont pas."
  },
  {
    "objectID": "03-TwoSampleTests.html#importation-et-mise-en-forme-des-données",
    "href": "03-TwoSampleTests.html#importation-et-mise-en-forme-des-données",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.3 Importation et mise en forme des données",
    "text": "3.3 Importation et mise en forme des données\nLes données de cette étude sont stockées dans le fichier HornedLizards.csv. Importez ces données dans un objet nommé Lizard_raw et examinez le tableau obtenu.\n\nLizard_raw\n\n# A tibble: 185 × 2\n   squamosalHornLength Survival\n                 &lt;dbl&gt; &lt;chr&gt;   \n 1                25.2 living  \n 2                26.9 living  \n 3                26.6 living  \n 4                25.6 living  \n 5                25.7 living  \n 6                25.9 living  \n 7                27.3 living  \n 8                25.1 living  \n 9                30.3 living  \n10                25.6 living  \n# ℹ 175 more rows\n\n\n\nView(Lizard_raw)\n\nOn constate ici 3 choses :\n\nla variable Survival devrait être un facteur.\nle nom de la première colonne (squamosalHornLength), qui contient les mesures des longueurs de cornes, est bien trop long.\npour l’un des lézards vivants, la mesure de longueur des cornes est manquante. Nous allons donc retirer cet individu pour éviter les messages d’erreurs par la suite.\n\nNous pouvons facilement réaliser les 3 modifications d’un coup, et stokcer le résultat dans un nouveau tableau Lizard :\n\nLizard &lt;- Lizard_raw %&gt;%\n  mutate(Survival = factor(Survival)) %&gt;%\n  rename(Horn_len = squamosalHornLength) %&gt;%\n  filter(!is.na(Horn_len))\n\nLizard\n\n# A tibble: 184 × 2\n   Horn_len Survival\n      &lt;dbl&gt; &lt;fct&gt;   \n 1     25.2 living  \n 2     26.9 living  \n 3     26.6 living  \n 4     25.6 living  \n 5     25.7 living  \n 6     25.9 living  \n 7     27.3 living  \n 8     25.1 living  \n 9     30.3 living  \n10     25.6 living  \n# ℹ 174 more rows\n\n\nCe tableau est bien un tableau rangé, au format long : chaque colonne contient une unique variable (Horn_len : longueur des cornes, Survival : groupe de l’individu mesuré, vivant ou mort), et chaque ligne contient les informations d’un unique individu.\nIci, il ne serait pas correct de présenter les données au format large. il nous faudrait en effet une colonne pour chaque groupe, lézard vivant et lézard mort, mais puisque les données de ces 2 groupes sont indépendantes, nous aurions 2 problèmes :\n\nsi le nombre d’individu n’est pas le même dans les 2 groupes, les deux colonnes n’auraient pas la même longueur. C’est impossible dans RStudio, et le logiciel remplierait donc la colonne la plus courte de NAs pour y remédier.\nles lignes de cet hypothétique tableau large ne correspondraient plus à des observations uniques. Chaque ligne renseignerait en effet sur les mesures de 2 individus distincts, un vivant et un mort.\n\nLorsque vous disposez de données appartenant à des groupes indépendants, il faut donc toujours travailler avec un tableau rangé, nécessairement au format long."
  },
  {
    "objectID": "03-TwoSampleTests.html#exploration-statistique-des-données",
    "href": "03-TwoSampleTests.html#exploration-statistique-des-données",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.4 Exploration statistique des données",
    "text": "3.4 Exploration statistique des données\nComme dans le Chapitre 2 sur les données appariées, les statistiques descriptives doivent ici être réalisées pour chaque groupe d’individus, et non tous groupes confondus. Ici, le plus simple est d’utiliser la fonction skim() sur les données groupées par niveau du facteur Survival (avec la fonction group_by()) :\n\nLizard %&gt;%\n  group_by(Survival) %&gt;%\n  skim()\n\n── Data Summary ────────────────────────\n                           Values    \nName                       Piped data\nNumber of rows             184       \nNumber of columns          2         \n_______________________              \nColumn type frequency:               \n  numeric                  1         \n________________________             \nGroup variables            Survival  \n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable Survival n_missing complete_rate mean   sd   p0  p25  p50  p75\n1 Horn_len      killed           0             1 22.0 2.71 15.2 21.1 22.2 23.8\n2 Horn_len      living           0             1 24.3 2.63 13.1 23   24.6 26  \n  p100 hist \n1 26.7 ▂▂▇▇▃\n2 30.3 ▁▁▅▇▂\n\n\nOn constate ici qu’il n’y a pas de données manquantes (n_missing = 0 dans les deux groupes). La moyenne des longueurs de cornes est plus grande chez les lézards vivants (\\(\\bar{x}_{living} = 24.3\\) mm) que chez les lézards retrouvés morts (\\(\\bar{x}_{killed} = 22.0\\)), de plus de 2 millimètres. On retrouve cette tendance pour les médianes, ainsi que pour les premiers et troisièmes quartiles. En revanche, les écarts-types des 2 groupes sont proches, et celui du groupe living est très légèrement plus faible (0.08 mm) que celui du groupe killed.\nEnfin, les histogrammes très simplifiés fournis laissent penser que les données de chaque groupe ne s’écartent pas trop fortement d’une courbe en cloche.\nOutre ces informations sur les ordres de grandeurs observés dans chaque groupe de lézards pour les indices de position (moyennes, médianes et quartiles), et de dispersion (écarts-types et histogrammes), la fonction skim() ne fournit pas les effectifs observés dans chaque groupe. On sait qu’il y a en tout 184 individus, mais on ne sait pas comment ils se répartissent dans les 2 groupes de lézards. Pour le déterminer, on peut utiliser une fonction décrite plus tôt, la fonction count() :\n\nLizard %&gt;%\n  count(Survival)\n\n# A tibble: 2 × 2\n  Survival     n\n  &lt;fct&gt;    &lt;int&gt;\n1 killed      30\n2 living     154\n\n\nOn peut obtenir la même information avec la fonction summarise() et son argument .by, et la fonction n() :\n\nLizard %&gt;% \n  summarize(Effectif = n(), .by = Survival)\n\n# A tibble: 2 × 2\n  Survival Effectif\n  &lt;fct&gt;       &lt;int&gt;\n1 living        154\n2 killed         30\n\n\nOn constate ici que les tailles d’échantillons sont très différentes. C’est normal compte tenu de la difficulté de repérer des individus morts dans la nature, et ce n’est pas gênant pour nos analyses puisque la taille des deux échantillons reste élevée.\nEnfin, on peut calculer des indices d’incertitude. C’est d’autant plus important qu’il est difficile de se faire une idée de la signification d’une différence moyenne de longueur de cornes de 2 millimètres. Est-ce important ou négligeable ? Est-ce que ces estimations sont précises ou non ? Comme dans les chapitres précédents, nous allons calculer les intervalles de confiance à 95% de la moyenne de chaque groupe :\n\nLizard %&gt;% \n  reframe(mean_cl_normal(Horn_len), .by = Survival)\n\n# A tibble: 2 × 4\n  Survival     y  ymin  ymax\n  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 living    24.3  23.9  24.7\n2 killed    22.0  21.0  23.0\n\n\nLa colonne y nous présente à nouveau la moyenne de chaque groupe, la colonne ymin contient les bornes inférieures des intervalles de confiance à 95%, et la colonne ymax les bornes supérieures. On constate ici que les intervalles de confiance à 95% des longueurs de cornes des 2 groupes ne se chevauchent pas du tout : la borne inférieure du groupe living est au-dessus de la borne supérieure du groupe killed. Autrement dit, dans la population générale, la longueur moyenne des cornes chez les lézards vivants a de bonnes chances de se trouver dans l’intervalle [23.9 ; 24.7] millimètres, alors qu’elle a de bonnes chances de se trouver dans l’intervalle [21 ; 23] millimètres chez les lézards morts. La différence de moyennes entre ces 2 groupes vaut donc probablement entre 0.9 millimètres au moins, et 3.7 millimètres au plus. Le test statistique que nous ferons ensuite devrait donc confirmer que ces différences sont significatives, autrement dit, qu’elles ne sont pas liées au simple hasard de l’échantillonnage."
  },
  {
    "objectID": "03-TwoSampleTests.html#exploration-graphique-des-données",
    "href": "03-TwoSampleTests.html#exploration-graphique-des-données",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.5 Exploration graphique des données",
    "text": "3.5 Exploration graphique des données\nComme toujours, nous pouvons réaliser plusieurs types de graphiques pour en apprendre plus sur la distribution des données dans les deux groupes. Si nous faisons un nuage de points, il est évidemment impossible ici de relier les points deux à deux. Non seulement cela n’aurait aucun sens puisque les échantillons sont indépendants, mais en outre, nous ne disposons pas du même nombre d’individus dans les 2 échantillons. Nous nous contenterons donc da faire un stripchart.\n\n3.5.1 Avec un stripchart\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_jitter(height = 0, width = 0.2, alpha = 0.5) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Visualisation des longueurs\\nde cornes du lézard cornu\")\n\n\n\n\n\n\n\n\nCe premier graphique permet de visualiser très clairement les différences de tailles d’échantillons entre les deux groupes. Il permet également de voir que l’étendue des longueurs de cornes est plus importante dans le groupe des individus vivants que dans celui des individus morts. En outre, le nuage de points des vivants semble être plus haut sur l’axe des y que celui des morts, confirmant les statistiques descriptives qui montraient des tailles de cornes en moyenne plus importantes dans le groupe des vivants.\n\n\n3.5.2 Avec des histogrammes facettés\n\nLizard %&gt;%\n  ggplot(aes(x = Horn_len)) +\n  geom_histogram(bins = 15, fill = \"firebrick2\", color = \"grey20\", alpha = 0.5)+\n  geom_rug() +\n  facet_wrap(~Survival, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Longueur des cornes (mm)\",\n       y = \"Fréquence\",\n       title = \"Distribution de la longueur des cornes dans 2 groupes de lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\nNotez ici l’utilisation de l’argument scales = \"free_y\" dans la fonction facet_wrap(). Cet argument permet de ne pas imposer la même échelle pour l’axe des ordonnées des 2 graphiques. Ce choix est ici pertinent puisque les effectifs des 2 groupes sont très différents. Faîtes un essai sans cet argument pour voir la différence. Il est en revanche important de conserver le même axe des x afin de faciliter la comparaison des 2 groupes.\nCette visualisation nous montre que les données doivent suivre à peu près une distribution Normale dans les 2 groupes, et que globalement la longueur des cornes semble légèrement plus élevée dans le groupe des vivants (avec un mode autour de 25-26 mm) que dans le groupes des morts (avec un mode autour de 23-24 mm). L’étendue des données semble légèrement plus grande dans le groupe des vivants, mais cela n’est peut-être dû qu’à la différence marquée des tailles d’échantillons.\n\n\n3.5.3 Avec des diagrammes de densité facettés\n\nLizard %&gt;%\n  ggplot(aes(x = Horn_len)) +\n  geom_density(fill = \"firebrick2\", alpha = 0.5) +\n  geom_rug() +\n  facet_wrap(~Survival, ncol = 1, scales = \"free_y\") +\n  labs(x = \"Longueur des cornes (mm)\",\n       y = \"Densité\",\n       title = \"Distribution de la longueur des cornes dans 2 groupes de lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\nLes diagrammes de densité ressemblent ici beaucoup aux histogrammes. C’est normal car la taille des échantillons est importante (30 et 154 pour les groupes killed et living respectivement). C’était moins vrais dans les chapitres précédents car les tailles d’échantillons étaient plus faibles, et la forme des histogrammes dépendait alors beaucoup du nombre de classes que l’on choisissait de représenter. Avec des échantillons de grande taille (n ≥ 30), c’est moins problématique.\nEn général, il est donc inutile de faire à la fois les histogrammes et les diagrammes de densité. Choisissez l’un ou l’autre selon vos préférences et la situation.\n\n\n3.5.4 Avec des boites à moustaches\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_boxplot(notch = TRUE) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Comparaison de 2 groupes\\nde lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\")\n\n\n\n\n\n\n\n\nNous visualisons ici encore plus clairement que sur les histogrammes le fait que les longueurs de cornes des individus vivants sont légèrement plus longues que celles des individus morts. D’ailleurs, puisque les intervalles de confiance à 95% des médianes des 2 groupes (les encoches) ne se chevauchent pas, un test de comparaison des moyennes devrait logiquement conclure à une différence significative en faveur des individus vivants. On peut également noter que la largeur de l’encoche pour les individus morts est plus importante que celle des vivants. Cela traduit une incertitude plus grande autour de la médiane estimée dans le groupe des individus morts. C’est tout à fait logique compte tenu des effectifs plus faibles dans ce groupe.\nEnfin, il est tout à fait possible de représenter sur le même graphique les boîtes à moustaches et les données brutes sous forme de stripchart. On a ainsi à la fois (i) une visualisation simplifiée de la position et de la dispersion des données avec les boîtes à moustache, et (ii) accès à l’ensemble des données brutes, ce qui permet parfois de voir des structures invisibles sur les boîtes à moustaches (regroupement de points par exemples). Afin de ne pas dupliquer les valeurs les plus extrêmes du jeu de données, nous indiquerons à geom_boxplot() de ne pas afficher les outliers sur le graphique : tous les points seront en effet déjà affichés par geom_jitter() :\n\nLizard %&gt;%\n  ggplot(aes(x = Survival, y = Horn_len, fill = Survival)) +\n  geom_boxplot(notch = TRUE, color = \"grey20\", alpha = 0.2,\n               outlier.color = NA, show.legend = FALSE) +\n  geom_jitter(height = 0, width = 0.2, alpha = 0.4, shape = 21,\n              show.legend = FALSE, size = 0.8) +\n  labs(x = \"Groupe de lézards\",\n       y = \"Longueur des cornes (mm)\",\n       title = \"Comparaison de 2 groupes\\nde lézards cornus\",\n       subtitle = \"nb morts : 30, nb vivants : 154\") +\n  scale_fill_manual(values = c(\"purple3\", \"royalblue2\"))"
  },
  {
    "objectID": "03-TwoSampleTests.html#le-test-paramétrique",
    "href": "03-TwoSampleTests.html#le-test-paramétrique",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.6 Le test paramétrique",
    "text": "3.6 Le test paramétrique\nLe test paramétrique le plus puissant que nous puissions faire pour comparer la moyenne de 2 populations est le test de Student. Ce test étant paramétrique, nous devons nous assurer que ses conditions d’application sont vérifiées avant de pouvoir le réaliser.\n\n3.6.1 Conditions d’application\nLes conditions d’application de ce test sont au nombre de 3 :\n\nChacun des deux échantillons est issu d’un échantillonnage aléatoire de la population générale. Comme toujours, en l’absence d’indication contraire, on considère que cette condition est toujours vérifiée.\nLa variable numérique étudiée est distribuée normalement dans les deux populations. Il nous faudra donc faire deux tests de Shapiro-Wilk, un pour chaque échantillon.\nLa variance de la variable numérique étudiée est la même dans les deux populations. C’est ce que l’on appelle l’homoscédasticité.\n\nEn réalité, le test du \\(t\\) de Student sur deux échantillons indépendants est assez robuste face au non respect de cette troisième condition d’application. Cela signifie que si cette troisième condition d’application n’est pas strictement vérifiée, les résultats du tests peuvent malgré tout rester valides. Lorsque les 2 échantillons comparés ont des tailles supérieures ou égales à 30, ce test fonctionne bien même si l’écart-type d’un groupe est jusqu’à 3 fois supérieur ou inférieur à l’écart-type du second groupe, à condition que la taille des 2 échantillons soit proche (ce qui n’est pas le cas ici !). En revanche, si les écart-types diffèrent de plus d’un facteur 3, ou si les tailles d’échantillons sont très différentes, le test du \\(t\\) de Student ne devrait pas être utilisé. De même, si la taille des échantillons est inférieure à 30 et que les variances ne sont pas homogènes, ce test ne devrait pas être réalisé. En conclusion, les résultats du test du \\(t\\) de Student à deux échantillons indépendants peuvent rester valides si la troisième condition d’homoscédasticité n’est pas respectée, mais dans certains cas seulement.\nLe test du \\(t\\) de Student sur deux échantillons indépendants est également assez robuste face à des écarts mineurs à la distribution Normale, tant que la forme des deux distributions comparées reste similaire et unimodale. En outre, la robustesse de ce test augmente avec la taille des échantillons.\n\n\n\n\n\n\nRobustesse\n\n\n\nLa robustesse d’un tests statistique est sa capacité à rester valide même lorsque certaines de ses conditions d’application ne sont pas parfaitement respectées. Plus un test est robuste, plus il est capable de supporter des “entorses” importantes à ses conditions d’application.}\n\n\nAu final, avec un peu d’habitude, même lorsque les conditions d’application ne sont pas toutes vérifiées, on peut parfois passer outre. Mais à ce stade, on préfère s’en tenir à des choses plus simples et claires.\n\n\n\n\n\n\nLa procédure à suivre\n\n\n\n\nFaites un test de Normalité pour chacune des deux séries de données. Si elles suivent la loi Normale toutes les deux, passez au point 2. Sinon, rendez-vous au point 4.\nFaites un test d’homoscédasticité (homogénéité des variances). Si les variances sont homogènes, passez au point 3. Sinon,rendez-vous au point 5.\nFaite un test de comparaison des moyennes paramétrique : le test de Student. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nFaite un test de comparaison des moyennes non paramétrique : le test de Wilcoxon de la somme des rangs. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nFaite un test de comparaison des moyennes non paramétrique : le test \\(t\\) de Welch. Examinez la \\(p-\\)value pour conclure, et rendez-vous au point 6.\nSi la \\(p-\\)value est supérieure à \\(\\alpha\\), on ne peut pas rejeter l’hypothèse nulle et on conclut alors à une absence de différence significative entre les 2 populations. À l’inverse, si la \\(p-\\)value est inférieure ou égale à \\(\\alpha\\), on rejette l’hypothèse nulle et on valide l’hypothèse alternative. Les deux populations ont des moyennes significativement différentes, et pour savoir laquelle est supérieure ou inférieure à l’autre, on revient aux estimations des moyennes et des intervalles de confiances à 95%, calculés dans la partie consacrée aux statistiques descriptives.\n\n\n\n\n3.6.1.1 Normalité des données\nNous commençons donc par tester la Normalité des 2 populations dont sont issus les échantillons, c’est le point 1 de la procédure détaillée ci-dessus. Pour les individus morts, les hypothèses sont les suivantes :\n\nH\\(_0\\) : la taille des cornes suit une distribution Normale dans la population des lézards cornus morts.\nH\\(_1\\) : la taille des cornes ne suit pas une distribution Normale dans la population des lézards cornus morts.\n\n\nLizard %&gt;%\n  filter(Survival == \"killed\") %&gt;%\n  pull(Horn_len) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.93452, p-value = 0.06482\n\n\n\nLa \\(p\\)-value est supérieure à \\(\\alpha = 0.05\\), donc on ne peut pas rejeter l’hypothèse nulle de normalité pour la taille des cornes de la population des lézards cornus morts (test de Shapiro-Wilk, \\(W = 0.93\\), \\(p = 0.065\\)).\n\nPour les individus vivants :\n\nH\\(_0\\) : la taille des cornes suit une distribution Normale dans la population des lézards cornus vivants.\nH\\(_1\\) : la taille des cornes ne suit pas une distribution Normale dans la population des lézards cornus vivants.\n\n\nLizard %&gt;%\n  filter(Survival == \"living\") %&gt;%\n  pull(Horn_len) %&gt;%\n  shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.96055, p-value = 0.0002234\n\n\n\nLa \\(p\\)-value est inférieure à \\(\\alpha = 0.05\\), donc on rejette l’hypothèse nulle de normalité pour la taille des cornes de la population des lézards cornus vivants (test de Shapiro-Wilk, \\(W = 0.96\\), \\(p &lt; 0.001\\)).\n\nL’une des 2 séries de données ne suit pas la loi Normale, nous sommes donc censés passer directement au point 4 de la procédure.\nToutefois, si l’on examine les histogrammes (Section 3.5.2) ou les diagrammes de densité (Section 3.5.3) des 2 échantillons, on constate que la forme des distributions des 2 séries de données est très proche. Pour les 2 échantillons, la distribution est en effet uni-modale, avec une asymétrie gauche assez marquée (une longue queue de distribution du côté gauche). La forme des distributions étant similaire (on parle bien de la forme des histogrammes et non de la position du pic), et les histogrammes étant proches de la forme typique d’une courbe en cloche, le test de Student devrait rester valide car il est robuste dans cette situation. Ici, pour l’exemple, on va donc passer au point 2 de la procédure. Notez toutefois que passer directement au point 4 de la procédure serait tout à fait correct : on ne pourrait rien vous reprocher si vous passez directement au test non paramétrique de comparaison des moyennes lorsque vous constatez que l’une des 2 séries de données ne suit pas la distribution Normale.\n\n\n3.6.1.2 Homogénéité des variances\nLe test le plus simple pour comparer la variance de 2 populations est le test \\(F\\) :\n\nH\\(_0\\) : la variance des 2 populations est égale, leur ratio vaut 1 \\(\\left(\\frac{\\sigma^2_{killed}}{\\sigma^2_{living}} = 1\\right)\\).\nH\\(_1\\) : la variance des 2 populations est différente, leur ratio ne vaut pas 1 \\(\\left(\\frac{\\sigma^2_{killed}}{\\sigma^2_{living}} \\neq 1\\right)\\).\n\n\nvar.test(Horn_len ~ Survival, data = Lizard)\n\n\n    F test to compare two variances\n\ndata:  Horn_len by Survival\nF = 1.0607, num df = 29, denom df = 153, p-value = 0.7859\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.6339331 1.9831398\nsample estimates:\nratio of variances \n          1.060711 \n\n\n\nIci, le ratio des variances (la variance des individus morts divisée par la variance des individus vivants) est très proche de 1 (\\(F = 1.06\\), IC 95% : [0.63 ; 1.98]). Le test \\(F\\) nous montre qu’il est impossible de rejeter H\\(_0\\) : au seuil \\(\\alpha = 0.05\\), le ratio des variances n’est pas significativement différent de 1 (ddl = 29 et 153, \\(p = 0.79\\)), les variances sont homogènes.\n\nLe test de Bartlett est un autre test qui permet de comparer la variance de plusieurs populations (2 ou plus). Lorsque le nombre de populations est égal à 2 (comme ici), ce test est absolument équivalent au test \\(F\\) ci-dessus.\n\nH\\(_0\\) : toutes les populations ont même variance (\\(\\sigma^2_A = \\sigma^2_B = \\sigma^2_C = \\cdots = \\sigma^2_N\\)).\nH\\(_1\\) : au moins une population a une variance différente des autres.\n\n\nbartlett.test(Horn_len ~ Survival, data = Lizard)\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  Horn_len by Survival\nBartlett's K-squared = 0.042411, df = 1, p-value = 0.8368\n\n\nEnfin, le test de Levene (attention, le package car doit être chargé) devrait être préféré la plupart du temps. Comme le test de Bartlett, il permet de comparer la variance de plusieurs populations, mais il est plus robuste vis à vis de la non-normalité des données.\n\nH\\(_0\\) : toutes les populations ont même variance (\\(\\sigma^2_A = \\sigma^2_B = \\sigma^2_C = \\cdots = \\sigma^2_N\\)).\nH\\(_1\\) : au moins une population a une variance différente des autres.\n\n\n# Le test de Levene fait partie du package car. Il doit être chargé en mémoire\n# library(car)\nleveneTest(Horn_len ~ Survival, data = Lizard)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.0035  0.953\n      182               \n\n\nIci encore, les conclusions sont les mêmes :\n\nIl est impossible de rejeter l’hypothèse nulle d’homogénéité des variances au seuil \\(\\alpha = 0.05\\) (test de Levene, \\(F\\) = 0.004, ddl = 1, \\(p = 0.953\\)).\n\nÀ vous de choisir lequel de ces 3 tests vous souhaitez réaliser : il est évident qu’on ne fait jamais les 3 !\nIci, puisque l’homoscédasticité est vérifiée, on passe au point 3 de la procédure.\n\n\n\n3.6.2 Réalisation du test et interprétation\nPuisque la taille des cornes du lézard cornu suit approximativement la même distribution “presque Normale” dans les 2 populations (lézards morts et vivants) et que ces 2 populations ont des variances homogènes, on peut réaliser le test du \\(t\\) de Student sur deux échantillons indépendants.\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nNotez bien la syntaxe :\n\nNous utilisons ici la syntaxe du type “formule” faisant appel au symbole “~” (Longueur des cornes en fonction de la Survie) et à l’argument “data =”.\nL’argument “paired = TRUE” a disparu puisque nous avons ici deux échantillons indépendants\nL’argument “var.equal = TRUE” doit obligatoirement être spécifié : nous nous sommes assuré que l’homogénéité des variances était vérifiée. Il faut donc l’indiquer afin que le test de Student classique soit réalisé. Si on omet de le spécifier, c’est un autre test qui est réalisé (voir plus bas).\n\n\nAu seuil \\(\\alpha\\) de 5%, on rejette l’hypothèse nulle d’égalité des moyennes de la longueur des cornes entre lézards vivants et morts (test \\(t\\) de Student sur deux échantillons indépendant, \\(t = -4.35\\), ddl = 182, \\(p &lt; 0.001\\)). Les lézards morts ont en moyenne des cornes plus courtes (\\(\\hat{\\mu}_{killed} = 21.99\\) millimètres) que les lézards vivants (\\(\\hat{\\mu}_{living} = 24.28\\) millimètres). La gamme des valeurs les plus probables pour la différence de moyenne entre les deux populations est fournie par l’intervalle de confiance à 95% de la différence de moyennes : [-3.34 ; -1.25].\n\nCe test confirme donc bien l’impression des chercheurs : les lézards principalement pris pour cibles par les pies grièches migratrices ont des cornes en moyenne plus courtes (probablement entre 1.25 et 3.34 millimètres de moins) que les lézards de la population générale. Avoir des cornes plus longues semble donc protéger les lézards de la prédation, du moins dans une certaine mesure.\nNotez bien que l’intervalle de confiance à 95% qui est fourni avec les résultats du test est l’intervalle de confiance à 95% de la différence de moyenne entre les 2 groupes. Cet intervalle nous donne donc une idée de la magnitude de l’effet, de son ampleur. En effet, dire que les lézards morts ont des cornes en moyenne plus courtes est intéressant, mais cela n’aura pas la même portée si leurs cornes sont plus courtes de 0.02 millimètres ou si elles sont plus courtes de 5 millimètres. Un test statistique permet de rejeter ou non une hypothèse nulle, mais c’est bien l’estimation (la moyenne de chaque groupe et l’intervalle de confiance à 95% de la différence) qui nous dit ce qu’on doit penser des résultats, et de leur pertinence (écologique, biologique, physiologie, comportementale, etc.)."
  },
  {
    "objectID": "03-TwoSampleTests.html#lalternative-non-paramétrique",
    "href": "03-TwoSampleTests.html#lalternative-non-paramétrique",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.7 L’alternative non paramétrique",
    "text": "3.7 L’alternative non paramétrique\nSi les conditions d’application du test de Student ne sont pas vérifiées (c’est bien le cas ici puisque la longueur des cornes ne suit pas une distribution Normale dans la population des lézards vivants), notre procédure nous conduit à l’étape 4 : nous devons utiliser un équivalent non paramétrique au test de Student. C’est le cas du test de Wilcoxon sur la somme des rangs (également appelé test de Mann-Whitney). Comme pour tous les tests de Wilcoxon, la comparaison porte alors non plus sur les moyennes mais sur les médianes.\n\nH\\(_0\\) : la médiane des 2 populations est égale, leur différence vaut 0 (\\(med_{killed}-med_{living} = 0\\)).\nH\\(_1\\) : la médiane des 2 populations est différente, leur différence ne vaut pas 0 (\\(med_{killed}-med_{living}\\neq 0\\)).\n\n\nwilcox.test(Horn_len ~ Survival, data = Lizard, conf.int = TRUE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Horn_len by Survival\nW = 1181.5, p-value = 2.366e-05\nalternative hypothesis: true location shift is not equal to 0\n95 percent confidence interval:\n -3.200076 -1.300067\nsample estimates:\ndifference in location \n             -2.200031 \n\n\nL’argument var.equal = TRUE n’existe pas pour ce test, puisque c’est justement un test non paramétrique qui ne requiert pas l’homogénéité des variances. En revanche, comme pour tous les autres tests de Wilcoxon que nous avons réalisés jusqu’ici, l’argument conf.int = TRUE permet d’afficher les estimateurs pertinents, ici, la différence de médiane entre les 2 populations et l’intervalle de confiance à 95% de cette différence de médiane.\nLa conclusion est ici la même que pour le test de Student : puisque la \\(p\\)-value est très inférieure à \\(\\alpha\\), on rejette l’hypothèse nulle : les médianes sont bel et bien différentes."
  },
  {
    "objectID": "03-TwoSampleTests.html#lautre-alternative-non-paramétrique",
    "href": "03-TwoSampleTests.html#lautre-alternative-non-paramétrique",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.8 L’autre alternative non paramétrique",
    "text": "3.8 L’autre alternative non paramétrique\nEnfin, dans le cas où la variable étudiée suit la loi Normale dans les deux populations mais qu’elle n’a pas la même variance dans les deux populations (donc si vous arrivez au point 5 de la procédure décrite plus haut), il est toujours possible de réaliser un test de Wilcoxon, il est préférable de réaliser un test de Student modifié : le test approché du \\(t\\) de Welch. Ce test est moins puissant que le test de Student classique, mais il reste plus puissant que le test de Wilcoxon, et surtout, il permet de comparer les moyennes et non les médianes.\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard)\n\n\n    Welch Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.2634, df = 40.372, p-value = 0.0001178\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.381912 -1.207092\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nLa seule différence par rapport à la syntaxe du test \\(t\\) de Student paramétrique est la suppression de l’argument var.equal = TRUE. Attention donc, à bien utiliser la syntaxe correcte. Le test du \\(t\\) de Welch ne devrait être réalisé que lorsque la Normalité est vérifiée pour les 2 populations, mais pas l’homoscédasticité. Par rapport au test de Student classique, on constate que le nombre de degrés de libertés est très différent, et donc la \\(p\\)-value également. Les bornes de l’intervalle de confiance à 95% de la différence de moyenne sont différentes également puisque leur calcul a été fait en supposant que les 2 populations n’avaient pas même variance."
  },
  {
    "objectID": "03-TwoSampleTests.html#exercices-dapplication",
    "href": "03-TwoSampleTests.html#exercices-dapplication",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.9 Exercices d’application",
    "text": "3.9 Exercices d’application\n\n3.9.1 La taille des hommes et des femmes\nOn s’intéresse à la différence de taille supposée entre hommes et femmes. Le fichier HommesFemmes.xls contient les tailles en centimètres de 38 hommes et 43 femmes choisis au hasard parmi les étudiants de première année à La Rochelle Université. Importez, mettez en forme et analysez ces données. Vous prendrez soin de retirer les éventuelles valeurs manquantes, vous prendrez le temps d’examiner les données à l’aide de statistiques descriptives et de représentations graphiques adaptées, puis vous tenterez de répondre à la question suivante : les hommes et les femmes inscrits en première année à La Rochelle Université ont-il la même taille ? Si non, caractérisez cette différence de taille.\n\n\n3.9.2 La longueur du bec des manchots Adélie\nDans le jeu de données penguins du package palmerpenguins, récupérez les lignes du tableau qui correspondent aux manchots Adélie, et comparez la longueur des becs entre mâles et femelles. Comme toujours, avant de vous lancer dans les tests, vous prendrez soin de retirer les éventuelles valeurs manquantes, et vous prendrez le temps d’examiner les données à l’aide de statistiques descriptives et de représentations graphiques adaptées. Faites l’effort d’expliquer votre démarche, de préciser les hypothèses nulles et alternatives de chaque test, et de rédiger l’interprétation que vous faites de chaque résultat."
  },
  {
    "objectID": "03-TwoSampleTests.html#sec-bilat",
    "href": "03-TwoSampleTests.html#sec-bilat",
    "title": "3  Comparaison de moyennes : deux échantillons indépendants",
    "section": "3.10 Tests bilatéraux et unilatéraux",
    "text": "3.10 Tests bilatéraux et unilatéraux\n\n3.10.1 Principe\nJusqu’à maintenant, tous les tests que nous avons réalisés sont des tests bilatéraux. Pour chaque test, l’hypothèse nulle est imposée. En revanche, pour certains tests, l’hypothèse alternative est à choisir (et à spécifier) par l’utilisateur parmi 3 possibilités :\n\nUne hypothèse bilatérale. C’est celle qui est utilisée par défaut si l’utilisateur ne précise rien.\nDeux hypothèses unilatérales possibles, qui doivent être spécifiées explicitement par l’utilisateur.\n\nLes tests unilatéraux peuvent concerner tous les tests pour lesquels les hypothèses sont de la forme suivante :\n\nH\\(_0\\) : la valeur d’un paramètre de la population est égale à \\(k\\) (\\(k\\) peut être une valeur fixe, arbitraire, choisie par l’utilisateur, ou la valeur d’un paramètre d’une autre populations).\nH\\(_1\\) : la valeur d’un paramètre de la population n’est pas égale à \\(k\\).\n\nEn réalité, si nous remplaçons l’hypothèse H\\(_1\\) par :\n\nH\\(_1\\) : la valeur d’un paramètre de la population est supérieure à \\(k\\).\n\nou par :\n\nH\\(_1\\) : la valeur d’un paramètre de la population est inférieure à \\(k\\).\n\nnous réalisons un test unilatéral.\nDans RStudio, la syntaxe permettant de spécifier l’hypothèse alternative que nous souhaitons utiliser est toujours la même. Il faut préciser, au moment de faire le test l’argument suivant :\n\nalternative = \"two.sided\" : pour faire un test bilatéral. Si on ne le fait pas explicitement, c’est de toutes façons cette valeur qui est utilisée par défaut.\nalternative = \"greater\" : pour choisir l’hypothèse unilatérale “&gt;”.\nalternative = \"less\" : pour choisir l’hypothèse unilatérale “&lt;”.\n\nAttention : le choix d’utiliser “greater” ou “less” dépend donc de l’ordre dans lequel les échantillons sont spécifiés. Cette syntaxe est valable pour tous les tests de Student vus jusqu’ici (un échantillon, deux échantillons appariés, deux échantillons indépendants) et pour leurs alternatives non paramétriques (test de Wilcoxon des rangs signés, test de Wilcoxon de la somme des rangs, test du \\(t\\) de Welch).\n\n\n\n\n\n\nAttention\n\n\n\nL’utilisation de tests unilatéraux doit être réservée exclusivement aux situations pour lesquelles le choix de l’hypothèse unilatérale est possible à justifier par un mécanisme quelconque (biologique, physiologique, comportemental, écologique, génétique, évolutif, biochimique, etc.). Observer que l’un des échantillons a une moyenne plus grande ou plus faible qu’un autre lors de la phase des statistiques descriptives des données n’est pas du tout une raison suffisante. Il faut pouvoir justifier le choix de l’hypothèse alternative par une explication valable. D’ailleurs, si on veut être rigoureux, il faudrait toujours formuler les hypothèses que l’on souhaite tester avant de mettre en place le protocole expérimental et avant d’acquérir les données.\n\n\nPour s’embêter avec les tests unilatéraux puisqu’il est si rare qu’on ait le droit de les faire ? Tout simplement parce que toutes choses étant égales par ailleurs, un test unilatéral est toujours plus puissant (parfois, beaucoup plus puissant) qu’un test bilatéral. Or, la puissance est quelque chose qu’on cherche à maximiser (voir sec-puiss). Lorsqu’il est pertinent de réaliser un test unilatéral, on doit donc toujours le faire.\nReprenons l’un des exemples examinés précédemment pour mieux comprendre comment tout cela fonctionne.\n\n\n3.10.2 Un exemple pas à pas\nReprenons l’exemple des lézards cornus. L’étude a été réalisée parce que les chercheurs supposaient que la longueur des cornes des lézards était susceptible de leur fournir une protection face à la prédation. Autrement dit, les chercheurs supposaient que des cornes plus longues devaient fournir une meilleure protection vis à vis de la prédation. Ainsi, les lézards morts devaient avoir des cornes moins longues en moyenne que les les lézards vivants, simplement parce que porter des cornes courtes expose plus fortement les individus à la prédation. Nous avons donc une bonne raison “écologique/évolutive” de considérer un test unilatéral (la susceptibilité face à la prédation qui a entraîné une pression de sélection sur la longueur des cornes des lézards), avant même de collecter les données.\nLorsque nous avons examiné cette question, nous avons fait le test du \\(t\\) de Student sur échantillons indépendants de la façon suivante :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nComme l’indiquent les résultats fournis, l’hypothèse alternative utilisée pour faire le test est : “La vraie différence de moyenne n’est pas égale à 0”. Autrement dit, nous avons fait un test bilatéral avec les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne des 2 populations est égale, leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne des 2 populations est différente, leur différence ne vaut pas 0 (\\(\\mu_{killed}-\\mu_{living} \\neq 0\\)).\n\nCe test est donc rigoureusement équivalent à celui-ci :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"two.sided\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 2.27e-05\nalternative hypothesis: true difference in means between group killed and group living is not equal to 0\n95 percent confidence interval:\n -3.335402 -1.253602\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nIci, nous souhaitons en fait réaliser un test unilatéral avec les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards morts est égale à celle des lézards vivants. Leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards morts est inférieure à celle des lézards vivants. Leur différence est inférieure à 0 (\\(\\mu_{killed}-\\mu_{living} &lt; 0\\)).\n\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"less\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 1.135e-05\nalternative hypothesis: true difference in means between group killed and group living is less than 0\n95 percent confidence interval:\n      -Inf -1.422321\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nPuisque la \\(p\\)-value de ce test est inférieure à \\(\\alpha = 0.05\\), on rejette l’hypothèse nulle de l’égalité des moyennes. On valide donc l’hypothèse alternative : les lézards cornus morts ont en moyenne des cornes plus courtes que les lézards vivants. Cette différence de longueur de cornes est en faveur des lézards vivants et vaut très probablement au moins \\(1.4\\) millimètres (c’est l’intervalle de confiance à 95% de la différence de moyennes qui nous le dit).\nDernière chose importante : il ne faut pas se tromper dans le choix de l’hypothèse alternative. En effet, nous aurions pu tenter de tester exactement la même chose en formulant les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards vivants est égale à celle des lézards morts. Leur différence vaut 0 (\\(\\mu_{living}-\\mu_{killed} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards vivants est supérieure à celle des lézards morts. Leur différence est supérieure à 0 (\\(\\mu_{living}-\\mu_{killed} &gt; 0\\)).\n\nCe test est normalement exactement le même que précédemment. Toutefois, si on essaie de le réaliser, on rencontre un problème :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"greater\")\n\n\n    Two Sample t-test\n\ndata:  Horn_len by Survival\nt = -4.3494, df = 182, p-value = 1\nalternative hypothesis: true difference in means between group killed and group living is greater than 0\n95 percent confidence interval:\n -3.166684       Inf\nsample estimates:\nmean in group killed mean in group living \n            21.98667             24.28117 \n\n\nIci, la \\(p\\)-value est très supérieure à \\(\\alpha\\) puisqu’elle vaut 1. Une \\(p\\)-value de 1 devrait toujours attirer votre attention. La conclusion devrait donc être que l’on ne peut pas rejeter H\\(_0\\) : les lézards morts et vivants ont en moyenne des cornes de même longueur. Nous savons pourtant que c’est faux.\nLe problème est ici liè à l’ordre des catégories “vivant” ou “mort” dans le facteur Survival du tableau Lizard. Les dernières lignes des tests que nous venons de faire indiquent la moyenne de chaque groupe, mais le groupe “killed” apparaît toujours avant le groupe “living”. C’est l’ordre des niveaux dans le facteur Survival qui doit dicter la syntaxe appropriée :\n\nLizard$Survival\n\n  [1] living living living living living living living living living living\n [11] living living living living living living living living living living\n [21] living living living living living living living living living living\n [31] living living living living living living living living living living\n [41] living living living living living living living living living living\n [51] living living living living living living living living living living\n [61] living living living living living living living living living living\n [71] living living living living living living living living living living\n [81] living living living living living living living living living living\n [91] living living living living living living living living living living\n[101] living living living living living living living living living living\n[111] living living living living living living living living living living\n[121] living living living living living living living living living living\n[131] living living living living living living living living living living\n[141] living living living living living living living living living living\n[151] living living living living killed killed killed killed killed killed\n[161] killed killed killed killed killed killed killed killed killed killed\n[171] killed killed killed killed killed killed killed killed killed killed\n[181] killed killed killed killed\nLevels: killed living\n\n\nPar défaut, dans RStudio, les niveaux d’un facteur sont classés par ordre alphabétique sauf si on spécifie manuellement un ordre différent. Ici, le niveau “killed” est donc le premier niveau du facteur, et “living” le second. Attention, on parle bien ici des niveaux, ou modalités, et non des données elles-mêmes. Ici, le premier lézard mesuré appartient à la catégorie living. Ça n’est pas ça qui est important : c’est bien l’ordre des niveaux qui compte, et on peut le vour tout en bas, après Levels: .... Lorsque l’on réalise un test de Student avec ces données (ou un test de Wilcoxon d’ailleurs), la différence de moyenne qui est examinée par le test est donc “moyenne des killed - moyenne des living”. Lorsque nous avons tapé ceci :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"greater\")\n\nnous avons donc en réalité posé les hypothèses suivantes :\n\nH\\(_0\\) : la moyenne de longueur des cornes de la population des lézards morts est égale à celle des lézards vivants. Leur différence vaut 0 (\\(\\mu_{killed}-\\mu_{living} = 0\\)).\nH\\(_1\\) : la moyenne de longueur des cornes de la population des lézards morts est supérieure à celle des lézards vivants. Leur différence est supérieure à 0 (\\(\\mu_{killed}-\\mu_{living} &gt; 0\\)).\n\nCe test est donc erroné, ce qui explique qu’il nous renvoie un résultat faux et une \\(p\\)-value de 1. Ici, puisque l’ordre des catégories est “killed” d’abord et “living” ensuite, la seule façon correcte de faire un test unilatéral qui a du sens est donc celle que nous avons réalisée en premier :\n\nt.test(Horn_len ~ Survival, data = Lizard, var.equal = TRUE,\n       alternative = \"less\")\n\nFaites donc toujours attention à l’ordre des catégories de vos facteurs pour ne pas vous tromper. Une façon simple de vérifier cet ordre et d’observer vos graphiques (par exemple, les boîtes à moustaches). L’ordre dans lequel les catégories apparaissent sur l’axe des x reflète l’ordre des catégorie du facteur porté par cet axe :\n\nLizard %&gt;% \n  ggplot(aes(x = Survival, y = Horn_len)) +\n  geom_boxplot()\n\n\n\n\n\n\n3.10.3 Exercice d’application\nReprenez chaque exemple et exercice traité depuis le premier chapitre et identifiez les situations où un test unilatéral aurait du sens. Si vous en trouvez, faites ce test et assurez-vous que les hypothèses choisies sont bien celles qui sont utilisées lors du test.\n\n\n\n\nFox, John, Sanford Weisberg, et Brad Price. 2022. car: Companion to Applied Regression. https://CRAN.R-project.org/package=car.\n\n\nHorst, Allison, Alison Hill, et Kristen Gorman. 2022. palmerpenguins: Palmer Archipelago (Antarctica) Penguin Data. https://CRAN.R-project.org/package=palmerpenguins.\n\n\nWaring, Elin, Michael Quinn, Amelia McNamara, Eduardo Arino de la Rubia, Hao Zhu, et Shannon Ellis. 2022. skimr: Compact and Flexible Summaries of Data. https://CRAN.R-project.org/package=skimr.\n\n\nWickham, Hadley. 2022. tidyverse: Easily Install and Load the Tidyverse. https://CRAN.R-project.org/package=tidyverse.\n\n\nWickham, Hadley, et Jennifer Bryan. 2022. readxl: Read Excel Files. https://CRAN.R-project.org/package=readxl.\n\n\nWickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, Hiroaki Yutani, et Dewey Dunnington. 2022. ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics. https://CRAN.R-project.org/package=ggplot2.\n\n\nWickham, Hadley, Romain François, Lionel Henry, Kirill Müller, et Davis Vaughan. 2023. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr.\n\n\nWickham, Hadley, Jim Hester, et Jennifer Bryan. 2022. readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n\n\nYoung, Kevin V., Edmund D. Brodie, et Edmund D. Brodie. 2004. « How the Horned Lizard Got Its Horns ». Science 304 (5667): 65‑65. https://doi.org/10.1126/science.1094790."
  },
  {
    "objectID": "index.html#objectifs",
    "href": "index.html#objectifs",
    "title": "TP de Biométrie Semestre 5",
    "section": "Objectifs",
    "text": "Objectifs\nCe livre contient l’ensemble du matériel (contenus, exemples, exercices…) nécessaire à la réalisation des travaux pratiques de Biométrie de l’EC ‘Outils pour l’étude et la compréhension du vivant 4’ du semestre 5 de la licence Sciences de la Vie de La Rochelle Université.\nÀ la fin du semestre, vous devriez être capables de faire les choses suivantes dans le logiciel RStudio :\n\nExplorer des jeux de données en produisant des résumés statistiques de variables de différentes nature (numériques continues ou catégorielles) et en produisant des graphiques appropriés\nCalculer des statistiques descriptives (moyennes, médianes, quartiles, écart-types, variances, erreurs standard, intervalles de confiance, etc.) pour plusieurs sous-groupes de vos jeux de données, et les représenter sur des graphiques adaptés\nChoisir et formuler des hypothèses adaptées à la question scientifique posée (hypothèses bilatérales ou unilatérales)\nChoisir les tests statistiques permettant de répondre à une question scientifique précise selon la nature de la question posée et la nature des variables à disposition\nRéaliser les tests usuels de comparaison de proportions et de moyennes (\\(\\chi^2\\), \\(t\\) de Student à 1 ou 2 échantillons, appariés ou indépendants, etc.)\nVérifier les conditions d’application des tests, et le cas échéant, réaliser des tests non paramétriques équivalents\nInterpréter correctement les résultats des tests pour répondre aux questions scientifiques posées\nIdentifier des cohortes dans une population et en étudier les caractéristiques et l’évolution temporelle\nSimuler le comportement de populations théoriques simples suivant des modèles démographiques précis (mortalité exponentielle, croissance exponentielle, croissance logistique, système prédateur-proies de Lotka et Volterra, et systèmes de compétition à 2 ou 3 espèces…)\nSimuler, par chaînes de Markov, les successions écologiques dans un écosystème théorique"
  },
  {
    "objectID": "index.html#pré-requis",
    "href": "index.html#pré-requis",
    "title": "TP de Biométrie Semestre 5",
    "section": "Pré-requis",
    "text": "Pré-requis\nPour atteindre les objectifs fixés ici, et compte tenu du volume horaire restreint qui est consacré aux TP et TEA de Biométrie au S5, je suppose que vous possédez un certain nombre de pré-requis. En particulier, vous devriez avoir à ce stade une bonne connaissance de l’interface des logiciels R et RStudio, et vous devriez être capables :\n\nde créer un Rproject et un script d’analyse dans RStudio\nd’importer des jeux de données issus de tableurs dans RStudio\nd’effectuer des manipulations de données simples (sélectionner des variables, trier des colonnes, filtrer des lignes, créer de nouvelles variables, etc.)\nde produire des graphiques de qualité, adaptés à la fois aux variables dont vous disposez et aux questions auxquelles vous souhaitez répondre.\n\n\n\n\n\n\n\nSi ces pré-requis ne sont pas maîtrisés\n\n\n\n\nmettez-vous à niveau de toute urgence en lisant attentivement le livre en ligne de Biométrie du semestre 3\nmettez-vous en binôme avec un·e collègue qui a suivi l’EC Immersion R et RStudio en début de semestre. Ça ne vous dispensera pas de lire le livre en ligne de Biométrie S3, mais ça vous fera certainement gagner pas mal de temps."
  },
  {
    "objectID": "index.html#organisation",
    "href": "index.html#organisation",
    "title": "TP de Biométrie Semestre 5",
    "section": "Organisation",
    "text": "Organisation\n\nVolume de travail\nLes travaux pratiques et TEA de biométrie auront lieu entre le 17 octobre et le 02 décembre 2022 :\n\nSemaine 42 (du 17 au 21 octobre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 43 (du 24 au 28 octobre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 45 (du 07 au 10 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 46 (du 14 au 18 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 47 (du 21 au 25 novembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\nSemaine 48 (du 28 novembre au 02 décembre) : 1 séance de TP d’1h30 et 1 séance de TEA d’1h30\n\n\nTous les TP ont lieu en salle MSI 217. Tous les TEA sont à distance.\nAu total, chaque groupe aura donc 6 séances de TP et 6 séances de TEA, soit un total de 18 heures prévues dans vos emplois du temps. C’est peu pour atteindre les objectifs fixés et il y aura donc évidemment du travail personnel à fournir en dehors de ces séances. J’estime que vous devrez fournir à peu près une vingtaine d’heures de travail personnel en plus des séances prévues dans votre emploi du temps. Attention donc : pensez bien à prévoir du temps dans vos plannings car le travail personnel est essentiel pour progresser dans cette matière. J’insiste sur l’importance de faire l’effort dès maintenant : vous allez en effet avoir des enseignements qui reposent sur l’utilisation de ces logiciels jusqu’à la fin du S6 (y compris pendant vos stage et, très vraisemblablement, dans vos futurs masters également). C’est donc maintenant qu’il faut acquérir des automatismes, cela vous fera gagner énormément de temps ensuite.\n\n\nModalités d’enseignement\nPour suivre cet enseignement vous pourrez utiliser les ordinateurs de l’université, mais je ne peux que vous encourager à utiliser vos propres ordinateurs, sous Windows, Linux ou MacOS. Lors de vos futurs stages et pour rédiger vos comptes-rendus de TP, vous utiliserez le plus souvent vos propres ordinateurs, autant prendre dès maintenant de bonnes habitudes en installant les logiciels dont vous aurez besoin tout au long de votre licence. Si vous n’avez pas suivi l’EC immersion et que les logiciels R et RStudio ne sont pas encore installés sur vos ordinateurs, suivez la procédure décrite ici. Si vous ne possédez pas d’ordinateur, manifestez vous rapidement auprès de moi car des solutions existent (prêt par l’université, travail sur tablette via RStudio cloud…).\n\n\n\n\n\n\nImportant\n\n\n\nL’essentiel du contenu de cet enseignement peut être abordé en autonomie, à distance, grâce à ce livre en ligne, aux ressources mises à disposition sur Moodle et à votre ordinateur personnel. Cela signifie que la présence physique lors de ces séances de TP n’est pas obligatoire.\n\n\nPlus que des séances de TP classiques, considérez plutôt qu’il s’agit de permanences non-obligatoires : si vous pensez avoir besoin d’aide, si vous avez des points de blocage ou des questions sur le contenu de ce document ou sur les exercices demandés, alors venez poser vos questions lors des séances de TP. Vous ne serez d’ailleurs pas tenus de rester pendant 1h30 : si vous obtenez une réponse en 10 minutes et que vous préférez travailler ailleurs, vous serez libres de repartir !\nDe même, si vous n’avez pas de difficulté de compréhension, que vous n’avez pas de problème avec les exercices de ce livre en ligne ni avec les quizz Moodle, votre présence n’est pas requise. Si vous souhaitez malgré tout venir en salle de TP, pas de problème, vous y serez toujours les bienvenus.\nCe fonctionnement très souple a de nombreux avantages :\n\nvous vous organisez comme vous le souhaitez\nvous ne venez que lorsque vous en avez vraiment besoin\ncelles et ceux qui se déplacent reçoivent une aide personnalisée\nvous travaillez sur vos ordinateurs\nles effectifs étant réduits, c’est aussi plus confortable pour moi !\n\nToutefois, pour que cette organisation fonctionne, cela demande de la rigueur de votre part, en particulier sur la régularité du travail que vous devez fournir. Si la présence en salle de TP n’est pas requise, le travail demandé est bel et bien obligatoire ! Si vous venez en salle de TP sans avoir travaillé en amont, votre venue sera totalement inutile puisque vous n’aurez pas de question à poser et que vous passerez votre séance à lire et suivre ce livre en ligne, choses que vous pouvez très bien faire chez vous. Vous perdrez donc votre temps, celui de vos collègues, et le mien. De même, si vous attendez la 4e semaine pour vous y mettre, vous irez droit dans le mur. Je le répète, outre les heures de TP/TEA prévus dans vos emplois du temps, vous devez prévoir au moins 20 heures de travail personnel supplémentaire.\nJe vous laisse donc une grande liberté d’organisation. À vous d’en tirer le maximum et de faire preuve du sérieux nécessaire. Le rythme auquel vous devriez avancer est présenté dans la partie suivante intitulée “Progression conseillée”.\n\n\nUtilisation de Slack\nOutre les séances de permanence non-obligatoires, nous échangerons aussi sur l’application Slack, qui fonctionne un peu comme un “twitter privé”. Slack facilite la communication des équipes et permet de travailler ensemble. Créez-vous un compte en ligne et installez le logiciel sur votre ordinateur (il existe aussi des versions pour tablettes et smartphones). Lorsque vous aurez installé le logiciel, cliquez sur ce lien pour vous connecter à notre espace de travail commun intitulé L3 SV 22-23 / EC outils (ce lien expire régulièrement : faites moi signe s’il n’est plus valide).\nVous verrez que 3 “chaînes” sont disponibles :\n\n#général : c’est là que les questions liées à l’organisation générale du cours, des TP et TEA, des évaluations, etc. doivent être posées. Si vous ne savez pas si une séance de permanence a lieu, posez la question ici.\n#questions-rstudio : c’est ici que toutes les questions pratiques liées à l’utilisation de R et RStudio devront êtres posées. Problèmes de syntaxe, problèmes liés à l’interface, à l’installation des packages ou à l’utilisation des fonctions, à la création des graphiques, à la manipulation des tableaux… Tout ce qui concerne directement les logiciels sera traité ici. Vous êtes libres de poser des questions, de poster des captures d’écran, des morceaux de code, des messages d’erreur. Et vous êtes bien entendus vivement encouragés à vous entraider et à répondre aux questions de vos collègues. Je n’interviendrai ici que pour répondre aux questions laissées sans réponse ou si les réponses apportées sont inexactes. Le fonctionnement est celui d’un forum de discussion instantané. Vous en tirerez le plus grand bénéfice en participant et en n’ayant pas peur de poser des questions, même si elles vous paraissent idiotes. Rappelez-vous toujours que si vous vous posez une question, d’autres se la posent aussi probablement.\n#questions-stats : C’est ici que vous pourrez poser vos questions liées aux méthodes statistiques ou aux choix des modèles de dynamique des populations. Tout ce qui ne concerne pas directement l’utilisation du logiciel (comme par exemple le choix d’un test ou des hypothèses nulles et alternatives, la démarche d’analyse, la signification de tel paramètre ou estimateur, le principe de telle ou telle méthode…) peut être discuté ici. Comme pour le canal #questions-rstudio, vous êtes encouragés à vous entraider et à répondre aux questions de vos collègues.\n\nAinsi, quand vous travaillerez à vos TP ou TEA, que vous soyez installés chez vous ou en salle de TP, prenez l’habitude de garder Slack ouvert sur votre ordinateur. Même si vous n’avez pas de question à poser, votre participation active pour répondre à vos collègues est souhaitable et souhaitée. Je vous incite donc fortement à vous entraider : c’est très formateur pour celui qui explique, et celui qui rencontre une difficulté a plus de chances de comprendre si c’est quelqu’un d’autre qui lui explique plutôt que la personne qui a rédigé les instructions mal comprises.\nCe document est fait pour vous permettre d’avancer en autonomie et vous ne devriez normalement pas avoir beaucoup besoin de moi si votre lecture est attentive. L’expérience montre en effet que la plupart du temps, il suffit de lire correctement les paragraphes précédents et/ou suivants pour obtenir la réponse à ses questions. J’essaie néanmoins de rester disponible sur Slack pendant les séances de TP et de TEA de tous les groupes. Cela veut donc dire que même si votre groupe n’est pas en TP, vos questions ont des chances d’être lues et de recevoir des réponses dès que d’autres groupes sont en TP ou TEA. Vous êtes d’ailleurs encouragés à échanger sur Slack aussi pendant vos phases de travail personnel."
  },
  {
    "objectID": "index.html#progession-conseillée",
    "href": "index.html#progession-conseillée",
    "title": "TP de Biométrie Semestre 5",
    "section": "Progession conseillée",
    "text": "Progession conseillée\nSi vous avez suivi le document de prise en main de R et RStudio (lors de l’immersion ou lors d’une remise à niveau en autonomie), vous savez que pour apprendre à utiliser ces logiciels, il faut faire les choses soi-même, ne pas avoir peur des messages d’erreurs (il faut d’ailleurs apprendre à les déchiffrer pour comprendre d’où viennent les problèmes), essayer maintes fois, se tromper beaucoup, recommencer, et surtout, ne pas se décourager. J’utilise ce logiciel presque quotidiennement depuis plus de 15 ans et à chaque session de travail, je rencontre des messages d’erreur. Avec suffisamment d’habitude, on apprend à les déchiffrer, et on corrige les problèmes en quelques secondes. Ce livre est conçu pour vous faciliter la tâche, mais ne vous y trompez pas, vous rencontrerez des difficultés, et c’est normal. C’est le prix à payer pour profiter de la puissance du meilleur logiciel permettant d’analyser des données, de produire des graphiques de qualité et de réaliser toutes les statistiques dont vous aurez besoin d’ici la fin de vos études et au-delà.\nPour que cet apprentissage soit le moins problématique possible, il convient de prendre les choses dans l’ordre. C’est la raison pour laquelle les chapitres de ce livre doivent être lus dans l’ordre, et les exercices d’application faits au fur et à mesure de la lecture.\nIdéalement, voilà les étapes que vous devriez avoir franchi chaque semaine :\n\nLa première semaine (42) est consacrée l’exploration statistique des jeux de données. Avant votre seconde séance de TP, vous devriez avoir compris comment calculer et interpréter des résumés statistiques de vos jeux de données. Vous devriez en particulier être capable de calculer des estimateurs de position (moyennes, médianes, quartiles…) et de dispersion (variances, écart-types, intervalles inter-quartiles…) sur des variables numériques, et ce, pour plusieurs modalités d’une variable catégorielle ou pour chaque combinaison de modalités de plusieurs variables catégorielles (par exemple, quelles sont les moyennes et variances des longueurs de becs pour chaque espèce de manchots et chaque sexe). Vous devrez être capables de distinguer la notion de dispersion de celle de précision, et vous devrez être capables de calculer l’erreur standard de la moyenne (ou erreur type). Vous devrez en outre être capables de produire des graphiques sur lesquels apparaissent des barres d’incertitude (erreurs standards ou intervalles de confiance).\nLa deuxième semaine (43) est consacrée aux tests statistiques. Avant votre troisième séance de TP, vous devriez être capable de formuler des hypothèses nulles et alternatives pertinentes, et vous devriez connaître le concept de \\(p-\\)value. Vous devriez en outre être capables, avec des données de comptages, de réaliser des tests de comparaison de proportions, et d’en interpréter correctement les résultats.\nLa troisième semaine (45) est également consacrée aux tests d’hypothèses. Avant votre quatrième séance de TP, vous devriez être capable de comparer la moyenne d’une population à une valeur théorique, et de comparer la moyenne de 2 populations, dans le cas où vous disposez d’échantillons appariés. Dans les deux cas, vous devrez être capable de vérifier les conditions d’application des tests paramétriques, et de choisir des tests non-paramétriques équivalents si les conditions d’application ne sont pas vérifiées.\nLa quatrième semaine (46) est consacrée aux derniers tests de comparaison de moyennes. Avant votre cinquième séance de TP, vous devrez donc être capable de comparer la moyenne de deux populations lorsque les échantillons sont indépendants. Comme pour la semaine précédente, vous devrez être capable de vérifier les conditions d’application du test paramétrique, et de réaliser le tests non paramétrique équivalent le cas échéant. Enfin, vous devrez aussi être en mesure de spécifier les hypothèses alternatives unilatérales ou bilatérales pertinentes selon la question scientifique posée. Pour chaque semaine consacrée aux tests, vous devrez aussi toujours penser à examiner les données graphiquement, et par le biais des statistiques descriptives décrites lors de la première semaine\nLa cinquième semaine (47) est consacrée à la mise en pratique des notions vues dans le cours magistral de Population Dynamics (EC “Fonctionnement des Écosystèmes). Nous aborderons ici les analyses de cohorte. Avant votre dernière séance de TP, vous devriez être en mesure de réaliser l’analyse de cohorte d’une population étudiée pendant plusieurs années afin de produire les courbes de croissance, de survie et d’Allen d’une cohorte d’intérêt. Vous devrez en particulier importer et mettre en forme des données issues d’un suivi de terrain, produire les structures démographiques instantanées à chaque date d’échantillonnage, faire les décompositions polymodales afin de récupérer les informations utiles au sujet de la cohorte dont on souhaite assurer le suivi.\nLa sixième semaine (48) est consacrée à la mise en pratique des notions vues dans le cours magistral de Population Dynamics (EC “Fonctionnement des Écosystèmes). Nous aborderons ici l’étude des systèmes dynamiques. Vous devrez coder différents modèles d’évolution d’une populations (mortalité exponentielle, croissance exponentielle, croissance logistique) ou de plusieurs populations ou espèces (modèle prédateurs-proies, modèle de compétition). Ces modèles généreront des données que vous devrez représenter graphiquement. Vous devrez enfin modifier la valeur de certains paramètres de ces modèles afin de comprendre leur influence sur le comportement des systèmes dynamiques étudiés."
  },
  {
    "objectID": "index.html#évaluations",
    "href": "index.html#évaluations",
    "title": "TP de Biométrie Semestre 5",
    "section": "Évaluation(s)",
    "text": "Évaluation(s)\nL’évaluation de la partie “Biométrie” de l’EC “Outils pour l’étude et la compréhension du vivant” aura lieu dans le cadre du travail de stratégie d’échantillonnage que vous mettez en œuvre avec Pierrick Bocher. Le compte-rendu de stratégie d’échantillonnage servira donc à évaluer 3 choses :\n\nles grands principes de stratégie d’échantillonnage abordés par Pierrick\nla mise en œuvre de méthodes statistiques adaptées pour répondre aux questions scientifiques posées, telles que nous les traitons en Biométrie\nla maîtrise du logiciel RStudio pour réaliser les analyses de données pertinentes (de l’importation des données et leur mise en forme dans le logiciel, à la réalisation et l’interprétation correcte des tests statistiques appropriés, en passant par l’exploration des statistiques descriptives et la création de graphiques informatifs). Pour ce dernier volet, vous devrez rendre, en plus de votre compte-rendu, votre script d’analyse. C’est ce script qui me permettra d’évaluer votre niveau de compétence et de maîtrise de l’outil, tant sur la forme du script (lisibilité, structure, reproductibilité, etc.) que sur le fond (pertinence des analyses réalisées).\n\nPour vous aider à comprendre ce qui est attendu, je vous fournis ci-dessous la grille critériée dont je me servirai pour évaluer la forme de votre script. Je ne peux que vous encourager à lire attentivement les critères d’évaluation ci-dessous et à tenter de vous les approprier. Les séances de TP et de TEA qui viennent doivent vous permettre de vous entraîner à produire des scripts de qualité.\n\nPour ce qui est du fond (pertinence des analyses statistiques réalisées et de leurs interprétations), une autre grille critériée sera fournie ici avant la fin du semestre."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "TP de Biométrie Semestre 5",
    "section": "Licence",
    "text": "Licence\nCe livre est ligne est sous licence Creative Commons (CC BY-NC-ND 4.0)\n\n\n\n\n\nVous êtes autorisé à partager, copier, distribuer et communiquer ce matériel par tous moyens et sous tous formats, tant que les conditions suivantes sont respectées :\n\n\n Attribution : vous devez créditer ce travail (donc citer son auteur), fournir un lien vers ce livre en ligne, intégrer un lien vers la licence Creative Commons et indiquer si des modifications du contenu original ont été effectuées. Vous devez indiquer ces informations par tous les moyens raisonnables, sans toutefois suggérer que l’auteur vous soutient ou soutient la façon dont vous avez utilisé son travail.\n\n\n Pas d’Utilisation Commerciale : vous n’êtes pas autorisé à faire un usage commercial de cet ouvrage, ni de tout ou partie du matériel le composant. Cela comprend évidemment la diffusion sur des plateformes de partage telles que studocu.com qui tirent profit d’œuvres dont elles ne sont pas propriétaires, souvent à l’insu des auteurs.\n\n\n Pas de modifications : dans le cas où vous effectuez un remix, que vous transformez, ou créez à partir du matériel composant l’ouvrage original, vous n’êtes pas autorisé à distribuer ou mettre à disposition l’ouvrage modifié.\n\n\n Pas de restrictions complémentaires : vous n’êtes pas autorisé à appliquer des conditions légales ou des mesures techniques qui restreindraient légalement autrui à utiliser cet ouvrage dans les conditions décrites par la licence."
  }
]